2025-01-07 18:12:01,994 INFO     Loading settings and stats
2025-01-07 18:12:01,994 INFO     Using prompt: Read the following Metal-Organic Framework (MOF) synthesis description and extract this information: temperature (highest reaction temp, use 25°C if not specified), time (longest duration at highest temp), one main solvent (no mixtures or ratios), one chemical additive ('None' if no additive present). Important: Do not use JSON or curly braces in your output, they are already provided for you and you do not need to generate them. Only output the extracted information and terminate strings with ", temperature: 0.0
2025-01-07 18:12:01,997 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:12:01,997 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_1_20250107_181157.yml'
2025-01-07 18:12:01,997 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_1_20250107_181157.yaml'
2025-01-07 18:12:01,997 WARNING  Could not load 'stats_llama_32_lora_run_1_epoch_1_20250107_181157.yml', creating it (empty)
2025-01-07 18:12:01,998 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:12:02,003 INFO     Found 778 paragraphs with labels
2025-01-07 18:12:02,003 DEBUG    First few valid IDs: ['KUHNEI_clean', 'NORPIV_clean', 'GUHMIH_clean', 'SAMVIO_clean', 'GETXAG_clean']
2025-01-07 18:12:02,003 INFO     Loading evaluation set instead of regular dataset.
2025-01-07 18:12:02,003 DEBUG    Already evaluated: 0 items
2025-01-07 18:12:02,003 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs
2025-01-07 18:12:02,003 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs'
2025-01-07 18:12:02,008 INFO     Dataset loaded with 77 items
2025-01-07 18:12:02,008 INFO     Found 77 paragraphs that have labels in dataset
2025-01-07 18:12:02,008 DEBUG    First few overlapping IDs: ['LUMZEA_clean', 'DOKHOB_clean', 'GOBTAT_clean', 'UZAFIL_clean', 'DEGPEL_charged']
2025-01-07 18:12:02,008 INFO     Processing model: LLaMa 3.2 1B Instruct
2025-01-07 18:12:02,009 INFO       0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:02,009 INFO     Loading Model [LLaMa 3.2 1B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama32_1b/instruction_lora/run_1_1736269137/epoch_1
2025-01-07 18:12:02,618 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-07 18:12:08,550 INFO       1%|▏         | 1/77 [00:06<08:17,  6.54s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:09,777 INFO       3%|▎         | 2/77 [00:07<04:16,  3.41s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:11,044 INFO       4%|▍         | 3/77 [00:09<03:00,  2.43s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:12,243 INFO       5%|▌         | 4/77 [00:10<02:22,  1.95s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:13,476 INFO       6%|▋         | 5/77 [00:11<02:01,  1.69s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:14,683 INFO       8%|▊         | 6/77 [00:12<01:48,  1.53s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:15,905 INFO       9%|▉         | 7/77 [00:13<01:39,  1.43s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:17,132 INFO      10%|█         | 8/77 [00:15<01:34,  1.36s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:18,323 INFO      12%|█▏        | 9/77 [00:16<01:29,  1.31s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:19,512 INFO      13%|█▎        | 10/77 [00:17<01:25,  1.27s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:20,720 INFO      14%|█▍        | 11/77 [00:18<01:22,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:21,942 INFO      16%|█▌        | 12/77 [00:19<01:20,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:23,171 INFO      17%|█▋        | 13/77 [00:21<01:19,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:24,397 INFO      18%|█▊        | 14/77 [00:22<01:17,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:25,601 INFO      19%|█▉        | 15/77 [00:23<01:15,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:26,851 INFO      21%|██        | 16/77 [00:24<01:15,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:28,090 INFO      22%|██▏       | 17/77 [00:26<01:14,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:29,308 INFO      23%|██▎       | 18/77 [00:27<01:12,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:30,534 INFO      25%|██▍       | 19/77 [00:28<01:11,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:31,820 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_1_20250107_181157.yml`
2025-01-07 18:12:31,832 INFO      26%|██▌       | 20/77 [00:29<01:11,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:33,060 INFO      27%|██▋       | 21/77 [00:31<01:09,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:34,256 INFO      29%|██▊       | 22/77 [00:32<01:07,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:35,471 INFO      30%|██▉       | 23/77 [00:33<01:06,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:36,687 INFO      31%|███       | 24/77 [00:34<01:04,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:37,912 INFO      32%|███▏      | 25/77 [00:35<01:03,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:39,149 INFO      34%|███▍      | 26/77 [00:37<01:02,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:40,371 INFO      35%|███▌      | 27/77 [00:38<01:01,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:41,640 INFO      36%|███▋      | 28/77 [00:39<01:00,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:42,868 INFO      38%|███▊      | 29/77 [00:40<00:59,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:44,105 INFO      39%|███▉      | 30/77 [00:42<00:58,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:45,303 INFO      40%|████      | 31/77 [00:43<00:56,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:46,545 INFO      42%|████▏     | 32/77 [00:44<00:55,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:47,755 INFO      43%|████▎     | 33/77 [00:45<00:53,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:48,988 INFO      44%|████▍     | 34/77 [00:46<00:52,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:50,194 INFO      45%|████▌     | 35/77 [00:48<00:51,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:51,445 INFO      47%|████▋     | 36/77 [00:49<00:50,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:52,661 INFO      48%|████▊     | 37/77 [00:50<00:49,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:53,960 INFO      49%|████▉     | 38/77 [00:51<00:48,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:55,188 INFO      51%|█████     | 39/77 [00:53<00:47,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:56,425 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_1_20250107_181157.yml`
2025-01-07 18:12:56,447 INFO      52%|█████▏    | 40/77 [00:54<00:46,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:57,656 INFO      53%|█████▎    | 41/77 [00:55<00:44,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:12:58,910 INFO      55%|█████▍    | 42/77 [00:56<00:43,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:00,126 INFO      56%|█████▌    | 43/77 [00:58<00:41,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:01,336 INFO      57%|█████▋    | 44/77 [00:59<00:40,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:02,545 INFO      58%|█████▊    | 45/77 [01:00<00:39,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:03,757 INFO      60%|█████▉    | 46/77 [01:01<00:37,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:05,003 INFO      61%|██████    | 47/77 [01:02<00:36,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:06,232 INFO      62%|██████▏   | 48/77 [01:04<00:35,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:07,433 INFO      64%|██████▎   | 49/77 [01:05<00:34,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:08,620 INFO      65%|██████▍   | 50/77 [01:06<00:32,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:09,842 INFO      66%|██████▌   | 51/77 [01:07<00:31,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:11,070 INFO      68%|██████▊   | 52/77 [01:09<00:30,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:12,298 INFO      69%|██████▉   | 53/77 [01:10<00:29,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:13,518 INFO      70%|███████   | 54/77 [01:11<00:28,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:14,749 INFO      71%|███████▏  | 55/77 [01:12<00:26,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:15,986 INFO      73%|███████▎  | 56/77 [01:13<00:25,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:17,180 INFO      74%|███████▍  | 57/77 [01:15<00:24,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:18,369 INFO      75%|███████▌  | 58/77 [01:16<00:22,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:19,593 INFO      77%|███████▋  | 59/77 [01:17<00:21,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:20,824 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_1_20250107_181157.yml`
2025-01-07 18:13:20,867 INFO      78%|███████▊  | 60/77 [01:18<00:20,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:22,091 INFO      79%|███████▉  | 61/77 [01:20<00:19,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:23,305 INFO      81%|████████  | 62/77 [01:21<00:18,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:24,521 INFO      82%|████████▏ | 63/77 [01:22<00:17,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:25,742 INFO      83%|████████▎ | 64/77 [01:23<00:15,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:26,968 INFO      84%|████████▍ | 65/77 [01:24<00:14,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:28,174 INFO      86%|████████▌ | 66/77 [01:26<00:13,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:29,449 INFO      87%|████████▋ | 67/77 [01:27<00:12,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:30,686 INFO      88%|████████▊ | 68/77 [01:28<00:11,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:31,870 INFO      90%|████████▉ | 69/77 [01:29<00:09,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:33,099 INFO      91%|█████████ | 70/77 [01:31<00:08,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:34,300 INFO      92%|█████████▏| 71/77 [01:32<00:07,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:35,556 INFO      94%|█████████▎| 72/77 [01:33<00:06,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:36,771 INFO      95%|█████████▍| 73/77 [01:34<00:04,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:38,001 INFO      96%|█████████▌| 74/77 [01:35<00:03,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:39,217 INFO      97%|█████████▋| 75/77 [01:37<00:02,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:40,460 INFO      99%|█████████▊| 76/77 [01:38<00:01,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:41,681 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_1_20250107_181157.yml`
2025-01-07 18:13:41,725 INFO     Processing model: LLaMa 3.2 3B Instruct
2025-01-07 18:13:41,725 INFO     Skipping model [LLaMa 3.2 3B Instruct]
  0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
model kwargs: {'torch_dtype': torch.float16, 'load_in_8bit': False, 'trust_remote_code': True, 'device_map': 'auto', 'do_sample': False}
  1%|▏         | 1/77 [00:06<08:17,  6.54s/it, LLaMa 3.2 1B Instruct]
  3%|▎         | 2/77 [00:07<04:16,  3.41s/it, LLaMa 3.2 1B Instruct]
  4%|▍         | 3/77 [00:09<03:00,  2.43s/it, LLaMa 3.2 1B Instruct]
  5%|▌         | 4/77 [00:10<02:22,  1.95s/it, LLaMa 3.2 1B Instruct]
  6%|▋         | 5/77 [00:11<02:01,  1.69s/it, LLaMa 3.2 1B Instruct]
  8%|▊         | 6/77 [00:12<01:48,  1.53s/it, LLaMa 3.2 1B Instruct]
  9%|▉         | 7/77 [00:13<01:39,  1.43s/it, LLaMa 3.2 1B Instruct]
 10%|█         | 8/77 [00:15<01:34,  1.36s/it, LLaMa 3.2 1B Instruct]
 12%|█▏        | 9/77 [00:16<01:29,  1.31s/it, LLaMa 3.2 1B Instruct]
 13%|█▎        | 10/77 [00:17<01:25,  1.27s/it, LLaMa 3.2 1B Instruct]
 14%|█▍        | 11/77 [00:18<01:22,  1.25s/it, LLaMa 3.2 1B Instruct]
 16%|█▌        | 12/77 [00:19<01:20,  1.24s/it, LLaMa 3.2 1B Instruct]
 17%|█▋        | 13/77 [00:21<01:19,  1.24s/it, LLaMa 3.2 1B Instruct]
 18%|█▊        | 14/77 [00:22<01:17,  1.23s/it, LLaMa 3.2 1B Instruct]
 19%|█▉        | 15/77 [00:23<01:15,  1.23s/it, LLaMa 3.2 1B Instruct]
 21%|██        | 16/77 [00:24<01:15,  1.23s/it, LLaMa 3.2 1B Instruct]
 22%|██▏       | 17/77 [00:26<01:14,  1.23s/it, LLaMa 3.2 1B Instruct]
 23%|██▎       | 18/77 [00:27<01:12,  1.23s/it, LLaMa 3.2 1B Instruct]
 25%|██▍       | 19/77 [00:28<01:11,  1.23s/it, LLaMa 3.2 1B Instruct]
 26%|██▌       | 20/77 [00:29<01:11,  1.25s/it, LLaMa 3.2 1B Instruct]
 27%|██▋       | 21/77 [00:31<01:09,  1.24s/it, LLaMa 3.2 1B Instruct]
 29%|██▊       | 22/77 [00:32<01:07,  1.23s/it, LLaMa 3.2 1B Instruct]
 30%|██▉       | 23/77 [00:33<01:06,  1.22s/it, LLaMa 3.2 1B Instruct]
 31%|███       | 24/77 [00:34<01:04,  1.22s/it, LLaMa 3.2 1B Instruct]
 32%|███▏      | 25/77 [00:35<01:03,  1.22s/it, LLaMa 3.2 1B Instruct]
 34%|███▍      | 26/77 [00:37<01:02,  1.23s/it, LLaMa 3.2 1B Instruct]
 35%|███▌      | 27/77 [00:38<01:01,  1.23s/it, LLaMa 3.2 1B Instruct]
 36%|███▋      | 28/77 [00:39<01:00,  1.24s/it, LLaMa 3.2 1B Instruct]
 38%|███▊      | 29/77 [00:40<00:59,  1.24s/it, LLaMa 3.2 1B Instruct]
 39%|███▉      | 30/77 [00:42<00:58,  1.24s/it, LLaMa 3.2 1B Instruct]
 40%|████      | 31/77 [00:43<00:56,  1.22s/it, LLaMa 3.2 1B Instruct]
 42%|████▏     | 32/77 [00:44<00:55,  1.23s/it, LLaMa 3.2 1B Instruct]
 43%|████▎     | 33/77 [00:45<00:53,  1.22s/it, LLaMa 3.2 1B Instruct]
 44%|████▍     | 34/77 [00:46<00:52,  1.23s/it, LLaMa 3.2 1B Instruct]
 45%|████▌     | 35/77 [00:48<00:51,  1.22s/it, LLaMa 3.2 1B Instruct]
 47%|████▋     | 36/77 [00:49<00:50,  1.23s/it, LLaMa 3.2 1B Instruct]
 48%|████▊     | 37/77 [00:50<00:49,  1.23s/it, LLaMa 3.2 1B Instruct]
 49%|████▉     | 38/77 [00:51<00:48,  1.25s/it, LLaMa 3.2 1B Instruct]
 51%|█████     | 39/77 [00:53<00:47,  1.24s/it, LLaMa 3.2 1B Instruct]
 52%|█████▏    | 40/77 [00:54<00:46,  1.25s/it, LLaMa 3.2 1B Instruct]
 53%|█████▎    | 41/77 [00:55<00:44,  1.24s/it, LLaMa 3.2 1B Instruct]
 55%|█████▍    | 42/77 [00:56<00:43,  1.24s/it, LLaMa 3.2 1B Instruct]
 56%|█████▌    | 43/77 [00:58<00:41,  1.23s/it, LLaMa 3.2 1B Instruct]
 57%|█████▋    | 44/77 [00:59<00:40,  1.23s/it, LLaMa 3.2 1B Instruct]
 58%|█████▊    | 45/77 [01:00<00:39,  1.22s/it, LLaMa 3.2 1B Instruct]
 60%|█████▉    | 46/77 [01:01<00:37,  1.22s/it, LLaMa 3.2 1B Instruct]
 61%|██████    | 47/77 [01:02<00:36,  1.23s/it, LLaMa 3.2 1B Instruct]
 62%|██████▏   | 48/77 [01:04<00:35,  1.23s/it, LLaMa 3.2 1B Instruct]
 64%|██████▎   | 49/77 [01:05<00:34,  1.22s/it, LLaMa 3.2 1B Instruct]
 65%|██████▍   | 50/77 [01:06<00:32,  1.21s/it, LLaMa 3.2 1B Instruct]
 66%|██████▌   | 51/77 [01:07<00:31,  1.21s/it, LLaMa 3.2 1B Instruct]
 68%|██████▊   | 52/77 [01:09<00:30,  1.22s/it, LLaMa 3.2 1B Instruct]
 69%|██████▉   | 53/77 [01:10<00:29,  1.22s/it, LLaMa 3.2 1B Instruct]
 70%|███████   | 54/77 [01:11<00:28,  1.22s/it, LLaMa 3.2 1B Instruct]
 71%|███████▏  | 55/77 [01:12<00:26,  1.22s/it, LLaMa 3.2 1B Instruct]
 73%|███████▎  | 56/77 [01:13<00:25,  1.23s/it, LLaMa 3.2 1B Instruct]
 74%|███████▍  | 57/77 [01:15<00:24,  1.22s/it, LLaMa 3.2 1B Instruct]
 75%|███████▌  | 58/77 [01:16<00:22,  1.21s/it, LLaMa 3.2 1B Instruct]
 77%|███████▋  | 59/77 [01:17<00:21,  1.21s/it, LLaMa 3.2 1B Instruct]
 78%|███████▊  | 60/77 [01:18<00:20,  1.23s/it, LLaMa 3.2 1B Instruct]
 79%|███████▉  | 61/77 [01:20<00:19,  1.23s/it, LLaMa 3.2 1B Instruct]
 81%|████████  | 62/77 [01:21<00:18,  1.22s/it, LLaMa 3.2 1B Instruct]
 82%|████████▏ | 63/77 [01:22<00:17,  1.22s/it, LLaMa 3.2 1B Instruct]
 83%|████████▎ | 64/77 [01:23<00:15,  1.22s/it, LLaMa 3.2 1B Instruct]
 84%|████████▍ | 65/77 [01:24<00:14,  1.22s/it, LLaMa 3.2 1B Instruct]
 86%|████████▌ | 66/77 [01:26<00:13,  1.22s/it, LLaMa 3.2 1B Instruct]
 87%|████████▋ | 67/77 [01:27<00:12,  1.23s/it, LLaMa 3.2 1B Instruct]
 88%|████████▊ | 68/77 [01:28<00:11,  1.24s/it, LLaMa 3.2 1B Instruct]
 90%|████████▉ | 69/77 [01:29<00:09,  1.22s/it, LLaMa 3.2 1B Instruct]
 91%|█████████ | 70/77 [01:31<00:08,  1.22s/it, LLaMa 3.2 1B Instruct]
 92%|█████████▏| 71/77 [01:32<00:07,  1.22s/it, LLaMa 3.2 1B Instruct]
 94%|█████████▎| 72/77 [01:33<00:06,  1.23s/it, LLaMa 3.2 1B Instruct]
 95%|█████████▍| 73/77 [01:34<00:04,  1.22s/it, LLaMa 3.2 1B Instruct]
 96%|█████████▌| 74/77 [01:35<00:03,  1.23s/it, LLaMa 3.2 1B Instruct]
 97%|█████████▋| 75/77 [01:37<00:02,  1.22s/it, LLaMa 3.2 1B Instruct]
 99%|█████████▊| 76/77 [01:38<00:01,  1.23s/it, LLaMa 3.2 1B Instruct]
