2025-01-06 14:50:49,955 INFO     Loading settings and stats
2025-01-06 14:50:49,956 INFO     Using prompt: , temperature: 0.1
2025-01-06 14:50:49,959 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-06 14:50:49,959 WARNING  [Errno 2] No such file or directory: 'stats_llama_31_instruction_full_ft_FULL_DATA_epoch_0_20250106_145046.yml'
2025-01-06 14:50:49,959 WARNING  [Errno 2] No such file or directory: 'stats_llama_31_instruction_full_ft_FULL_DATA_epoch_0_20250106_145046.yaml'
2025-01-06 14:50:49,959 WARNING  Could not load 'stats_llama_31_instruction_full_ft_FULL_DATA_epoch_0_20250106_145046.yml', creating it (empty)
2025-01-06 14:50:49,961 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-06 14:50:49,967 INFO     Found 778 paragraphs with labels
2025-01-06 14:50:49,967 DEBUG    First few valid IDs: ['DACSAE_clean', 'FECYOD_clean', 'NAHHOW_clean', 'NEVJOP_clean', 'QOZPIG_clean']
2025-01-06 14:50:49,967 DEBUG    Already evaluated: 0 items
2025-01-06 14:50:49,967 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/synthesis_paragraphs
2025-01-06 14:50:49,967 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/synthesis_paragraphs'
2025-01-06 14:50:50,010 INFO     Dataset loaded with 905 items
2025-01-06 14:50:50,010 INFO     Found 778 paragraphs that have labels in dataset
2025-01-06 14:50:50,010 DEBUG    First few overlapping IDs: ['DACSAE_clean', 'FECYOD_clean', 'NAHHOW_clean', 'NEVJOP_clean', 'QOZPIG_clean']
2025-01-06 14:50:50,010 INFO     Processing model: LLaMa 3.1 8B
2025-01-06 14:50:50,010 INFO     Skipping model [LLaMa 3.1 8B]
2025-01-06 14:50:50,011 INFO     Processing model: LLaMa 3.1 8B Instruct
2025-01-06 14:50:50,012 INFO       0%|          | 0/905 [00:00<?, ?it/s, LLaMa 3.1 8B Instruct]
2025-01-06 14:50:50,012 INFO     Loading Model [LLaMa 3.1 8B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama31_8b/instruction_full_ft/run_1_1736114112/epoch_0
2025-01-06 14:50:50,665 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
  0%|          | 0/905 [00:00<?, ?it/s, LLaMa 3.1 8B Instruct]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:12,  4.03s/it][A
Loading checkpoint shards:  50%|█████     | 2/4 [00:08<00:08,  4.04s/it][A
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:12<00:04,  4.02s/it][A
Loading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  2.83s/it][ALoading checkpoint shards: 100%|██████████| 4/4 [00:13<00:00,  3.27s/it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
╭───────────────── Traceback (most recent call last) ──────────────────╮
│ /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/master_thesis/legacy_c │
│ ode/src/mthesis/main.py:294 in evaluate                              │
│                                                                      │
│   291 │   │   │   │   "model_name": model_name,                      │
│   292 │   │   │   }                                                  │
│   293 │   │   │                                                      │
│ ❱ 294 │   │   │   entry["answer"] = model(batch["text"])  # forward  │
│   295 │   │   │   stats.append(entry)                                │
│   296 │   │   │   if count >= 20:                                    │
│   297 │   │   │   │   try:                                           │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │               batch = {                                          │ │
│ │                       │   'paragraph_id': 'LELROL_clean',        │ │
│ │                       │   'text': ' Anhydrous MnCl2 (398 mg,     │ │
│ │                       3.16 mmol) and H4L1·6.5H2O (225 mg, 0.27   │ │
│ │                       mmol), were d'+973                         │ │
│ │                       }                                          │ │
│ │               count = 1                                          │ │
│ │             dataset = <mthesis.dataloader.MOFDataset object at   │ │
│ │                       0x148499b18290>                            │ │
│ │         dataset_ids = {                                          │ │
│ │                       │   'DACSAE_clean',                        │ │
│ │                       │   'FECYOD_clean',                        │ │
│ │                       │   'NAHHOW_clean',                        │ │
│ │                       │   'NEVJOP_clean',                        │ │
│ │                       │   'JUTCIM_clean',                        │ │
│ │                       │   'QOZPIG_clean',                        │ │
│ │                       │   'KUZZAI_clean',                        │ │
│ │                       │   'SUKMAO_clean',                        │ │
│ │                       │   'FEJDEE_clean',                        │ │
│ │                       │   'LACJAC_clean',                        │ │
│ │                       │   ... +895                               │ │
│ │                       }                                          │ │
│ │        dataset_path = '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29… │ │
│ │         description = ''                                         │ │
│ │              device = device(type='cuda')                        │ │
│ │                diff = 0                                          │ │
│ │               entry = {                                          │ │
│ │                       │   'paragraph_id': 'LELROL_clean',        │ │
│ │                       │   'model_name': 'LLaMa 3.1 8B Instruct'  │ │
│ │                       }                                          │ │
│ │           evaluated = frozenset()                                │ │
│ │      evaluation_set = False                                      │ │
│ │               first = False                                      │ │
│ │           labels_df = │    Unnamed: 0      filename DISORDER     │ │
│ │                       ... other6 other7  other8                  │ │
│ │                       0             0  OFODET_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       1             1  XAVKIR_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       2             3  LATPIG_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       3             4  MOYYIJ_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       4             5  OFOCUI_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       ..          ...           ...      ...     │ │
│ │                       ...    ...    ...     ...                  │ │
│ │                       773         963  LEVDIB_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       774         964  LIKDOA_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       775         967  MUNDAC_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       776         968  MUNDOQ_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                       777         970  OYIVIC_clean      NaN     │ │
│ │                       ...    NaN    NaN     NaN                  │ │
│ │                                                                  │ │
│ │                       [778 rows x 36 columns]                    │ │
│ │             log_dir = 'logs/llama_31_instruction_full_ft_FULL_D… │ │
│ │               model = JsonformerModel(                           │ │
│ │                         (model): LlamaForCausalLM(               │ │
│ │                       │   (model): LlamaModel(                   │ │
│ │                       │     (embed_tokens): Embedding(128256,    │ │
│ │                       4096)                                      │ │
│ │                       │     (layers): ModuleList(                │ │
│ │                       │   │   (0-31): 32 x LlamaDecoderLayer(    │ │
│ │                       │   │     (self_attn):                     │ │
│ │                       LlamaFlashAttention2(                      │ │
│ │                       │   │   │   (q_proj):                      │ │
│ │                       Linear(in_features=4096,                   │ │
│ │                       out_features=4096, bias=False)             │ │
│ │                       │   │   │   (k_proj):                      │ │
│ │                       Linear(in_features=4096,                   │ │
│ │                       out_features=1024, bias=False)             │ │
│ │                       │   │   │   (v_proj):                      │ │
│ │                       Linear(in_features=4096,                   │ │
│ │                       out_features=1024, bias=False)             │ │
│ │                       │   │   │   (o_proj):                      │ │
│ │                       Linear(in_features=4096,                   │ │
│ │                       out_features=4096, bias=False)             │ │
│ │                       │   │   │   (rotary_emb):                  │ │
│ │                       LlamaRotaryEmbedding()                     │ │
│ │                       │   │     )                                │ │
│ │                       │   │     (mlp): LlamaMLP(                 │ │
│ │                       │   │   │   (gate_proj):                   │ │
│ │                       Linear(in_features=4096,                   │ │
│ │                       out_features=14336, bias=False)            │ │
│ │                       │   │   │   (up_proj):                     │ │
│ │                       Linear(in_features=4096,                   │ │
│ │                       out_features=14336, bias=False)            │ │
│ │                       │   │   │   (down_proj):                   │ │
│ │                       Linear(in_features=14336,                  │ │
│ │                       out_features=4096, bias=False)             │ │
│ │                       │   │   │   (act_fn): SiLU()               │ │
│ │                       │   │     )                                │ │
│ │                       │   │     (input_layernorm):               │ │
│ │                       LlamaRMSNorm((4096,), eps=1e-05)           │ │
│ │                       │   │     (post_attention_layernorm):      │ │
│ │                       LlamaRMSNorm((4096,), eps=1e-05)           │ │
│ │                       │   │   )                                  │ │
│ │                       │     )                                    │ │
│ │                       │     (norm): LlamaRMSNorm((4096,),        │ │
│ │                       eps=1e-05)                                 │ │
│ │                       │     (rotary_emb): LlamaRotaryEmbedding() │ │
│ │                       │   )                                      │ │
│ │                       │   (lm_head): Linear(in_features=4096,    │ │
│ │                       out_features=128256, bias=False)           │ │
│ │                         )                                        │ │
│ │                       )                                          │ │
│ │          model_name = 'LLaMa 3.1 8B Instruct'                    │ │
│ │          model_path = '/home/iti/zn2950/home/haicore_ws/tunes/l… │ │
│ │      model_settings = {                                          │ │
│ │                       │   'model_name': 'LLaMa 3.1 8B Instruct', │ │
│ │                       │   'model_path':                          │ │
│ │                       '/home/iti/zn2950/home/haicore_ws/tunes/l… │ │
│ │                       │   'model_type': 'text-generation'        │ │
│ │                       }                                          │ │
│ │          only_model = 'LLaMa 3.1 8B Instruct'                    │ │
│ │             overlap = {                                          │ │
│ │                       │   'DACSAE_clean',                        │ │
│ │                       │   'FECYOD_clean',                        │ │
│ │                       │   'NAHHOW_clean',                        │ │
│ │                       │   'NEVJOP_clean',                        │ │
│ │                       │   'QOZPIG_clean',                        │ │
│ │                       │   'JUTCIM_clean',                        │ │
│ │                       │   'SUKMAO_clean',                        │ │
│ │                       │   'KUZZAI_clean',                        │ │
│ │                       │   'LACJAC_clean',                        │ │
│ │                       │   'RUGKOV_clean',                        │ │
│ │                       │   ... +768                               │ │
│ │                       }                                          │ │
│ │        paragraph_id = 'LELROL_clean'                             │ │
│ │        progress_bar = <tqdm.std.tqdm object at 0x148497dd2e10>   │ │
│ │              prompt = ''                                         │ │
│ │            settings = {                                          │ │
│ │                       │   'dataset_path':                        │ │
│ │                       '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29… │ │
│ │                       │   'eval_dataset_path':                   │ │
│ │                       '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29… │ │
│ │                       │   'csv_path':                            │ │
│ │                       '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29… │ │
│ │                       │   'models': [                            │ │
│ │                       │   │   {                                  │ │
│ │                       │   │   │   'model_name': 'LLaMa 3.1 8B',  │ │
│ │                       │   │   │   'model_path':                  │ │
│ │                       '/home/iti/zn2950/home/haicore_ws/tunes/l… │ │
│ │                       │   │   │   'model_type':                  │ │
│ │                       'text-generation'                          │ │
│ │                       │   │   },                                 │ │
│ │                       │   │   {                                  │ │
│ │                       │   │   │   'model_name': 'LLaMa 3.1 8B    │ │
│ │                       Instruct',                                 │ │
│ │                       │   │   │   'model_path':                  │ │
│ │                       '/home/iti/zn2950/home/haicore_ws/tunes/l… │ │
│ │                       │   │   │   'model_type':                  │ │
│ │                       'text-generation'                          │ │
│ │                       │   │   }                                  │ │
│ │                       │   ],                                     │ │
│ │                       │   'extract_config': {                    │ │
│ │                       │   │   'additive': {                      │ │
│ │                       │   │   │   'type': 'string',              │ │
│ │                       │   │   │   'convert_funcs': ['ans2cid'],  │ │
│ │                       │   │   │   'dataset_cols': [              │ │
│ │                       │   │   │   │   'additive1'                │ │
│ │                       │   │   │   ]                              │ │
│ │                       │   │   },                                 │ │
│ │                       │   │   'solvent': {                       │ │
│ │                       │   │   │   'type': 'string',              │ │
│ │                       │   │   │   'convert_funcs': ['ans2cid'],  │ │
│ │                       │   │   │   'dataset_cols': ['solvent1']   │ │
│ │                       │   │   },                                 │ │
│ │                       │   │   'temperature': {                   │ │
│ │                       │   │   │   'type': 'number',              │ │
│ │                       │   │   │   'unit': 'C',                   │ │
│ │                       │   │   │   'convert_funcs': [             │ │
│ │                       │   │   │   │   'ans2temperature'          │ │
│ │                       │   │   │   ],                             │ │
│ │                       │   │   │   'dataset_cols': [              │ │
│ │                       │   │   │   │   'temperature_Celsius'      │ │
│ │                       │   │   │   ]                              │ │
│ │                       │   │   },                                 │ │
│ │                       │   │   'time': {                          │ │
│ │                       │   │   │   'type': 'number',              │ │
│ │                       │   │   │   'unit': 'h',                   │ │
│ │                       │   │   │   'convert_funcs': [             │ │
│ │                       │   │   │   │   'ans2time'                 │ │
│ │                       │   │   │   ],                             │ │
│ │                       │   │   │   'dataset_cols': ['time_h']     │ │
│ │                       │   │   }                                  │ │
│ │                       │   }                                      │ │
│ │                       }                                          │ │
│ │               stats = []                                         │ │
│ │          stats_path = 'stats_llama_31_instruction_full_ft_FULL_… │ │
│ │         temperature = 0.1                                        │ │
│ │ valid_paragraph_ids = {                                          │ │
│ │                       │   'DACSAE_clean',                        │ │
│ │                       │   'FECYOD_clean',                        │ │
│ │                       │   'NAHHOW_clean',                        │ │
│ │                       │   'NEVJOP_clean',                        │ │
│ │                       │   'QOZPIG_clean',                        │ │
│ │                       │   'JUTCIM_clean',                        │ │
│ │                       │   'SUKMAO_clean',                        │ │
│ │                       │   'KUZZAI_clean',                        │ │
│ │                       │   'LACJAC_clean',                        │ │
│ │                       │   'RUGKOV_clean',                        │ │
│ │                       │   ... +768                               │ │
│ │                       }                                          │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/torch/nn/modules/module.py:1553 in _wrapped_call_impl   │
│                                                                      │
│   1550 │   │   if self._compiled_call_impl is not None:              │
│   1551 │   │   │   return self._compiled_call_impl(*args, **kwargs)  │
│   1552 │   │   else:                                                 │
│ ❱ 1553 │   │   │   return self._call_impl(*args, **kwargs)           │
│   1554 │                                                             │
│   1555 │   def _call_impl(self, *args, **kwargs):                    │
│   1556 │   │   forward_call = (self._slow_forward if torch._C._get_t │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │   args = (                                                       │ │
│ │          │   ' Anhydrous MnCl2 (398 mg, 3.16 mmol) and           │ │
│ │          H4L1·6.5H2O (225 mg, 0.27 mmol), were d'+973,           │ │
│ │          )                                                       │ │
│ │ kwargs = {}                                                      │ │
│ │   self = JsonformerModel(                                        │ │
│ │            (model): LlamaForCausalLM(                            │ │
│ │          │   (model): LlamaModel(                                │ │
│ │          │     (embed_tokens): Embedding(128256, 4096)           │ │
│ │          │     (layers): ModuleList(                             │ │
│ │          │   │   (0-31): 32 x LlamaDecoderLayer(                 │ │
│ │          │   │     (self_attn): LlamaFlashAttention2(            │ │
│ │          │   │   │   (q_proj): Linear(in_features=4096,          │ │
│ │          out_features=4096, bias=False)                          │ │
│ │          │   │   │   (k_proj): Linear(in_features=4096,          │ │
│ │          out_features=1024, bias=False)                          │ │
│ │          │   │   │   (v_proj): Linear(in_features=4096,          │ │
│ │          out_features=1024, bias=False)                          │ │
│ │          │   │   │   (o_proj): Linear(in_features=4096,          │ │
│ │          out_features=4096, bias=False)                          │ │
│ │          │   │   │   (rotary_emb): LlamaRotaryEmbedding()        │ │
│ │          │   │     )                                             │ │
│ │          │   │     (mlp): LlamaMLP(                              │ │
│ │          │   │   │   (gate_proj): Linear(in_features=4096,       │ │
│ │          out_features=14336, bias=False)                         │ │
│ │          │   │   │   (up_proj): Linear(in_features=4096,         │ │
│ │          out_features=14336, bias=False)                         │ │
│ │          │   │   │   (down_proj): Linear(in_features=14336,      │ │
│ │          out_features=4096, bias=False)                          │ │
│ │          │   │   │   (act_fn): SiLU()                            │ │
│ │          │   │     )                                             │ │
│ │          │   │     (input_layernorm): LlamaRMSNorm((4096,),      │ │
│ │          eps=1e-05)                                              │ │
│ │          │   │     (post_attention_layernorm):                   │ │
│ │          LlamaRMSNorm((4096,), eps=1e-05)                        │ │
│ │          │   │   )                                               │ │
│ │          │     )                                                 │ │
│ │          │     (norm): LlamaRMSNorm((4096,), eps=1e-05)          │ │
│ │          │     (rotary_emb): LlamaRotaryEmbedding()              │ │
│ │          │   )                                                   │ │
│ │          │   (lm_head): Linear(in_features=4096,                 │ │
│ │          out_features=128256, bias=False)                        │ │
│ │            )                                                     │ │
│ │          )                                                       │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/torch/nn/modules/module.py:1562 in _call_impl           │
│                                                                      │
│   1559 │   │   if not (self._backward_hooks or self._backward_pre_ho │
│   1560 │   │   │   │   or _global_backward_pre_hooks or _global_back │
│   1561 │   │   │   │   or _global_forward_hooks or _global_forward_p │
│ ❱ 1562 │   │   │   return forward_call(*args, **kwargs)              │
│   1563 │   │                                                         │
│   1564 │   │   try:                                                  │
│   1565 │   │   │   result = None                                     │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │         args = (                                                 │ │
│ │                │   ' Anhydrous MnCl2 (398 mg, 3.16 mmol) and     │ │
│ │                H4L1·6.5H2O (225 mg, 0.27 mmol), were d'+973,     │ │
│ │                )                                                 │ │
│ │ forward_call = <bound method JsonformerModel.forward of          │ │
│ │                JsonformerModel(                                  │ │
│ │                  (model): LlamaForCausalLM(                      │ │
│ │                │   (model): LlamaModel(                          │ │
│ │                │     (embed_tokens): Embedding(128256, 4096)     │ │
│ │                │     (layers): ModuleList(                       │ │
│ │                │   │   (0-31): 32 x LlamaDecoderLayer(           │ │
│ │                │   │     (self_attn): LlamaFlashAttention2(      │ │
│ │                │   │   │   (q_proj): Linear(in_features=4096,    │ │
│ │                out_features=4096, bias=False)                    │ │
│ │                │   │   │   (k_proj): Linear(in_features=4096,    │ │
│ │                out_features=1024, bias=False)                    │ │
│ │                │   │   │   (v_proj): Linear(in_features=4096,    │ │
│ │                out_features=1024, bias=False)                    │ │
│ │                │   │   │   (o_proj): Linear(in_features=4096,    │ │
│ │                out_features=4096, bias=False)                    │ │
│ │                │   │   │   (rotary_emb): LlamaRotaryEmbedding()  │ │
│ │                │   │     )                                       │ │
│ │                │   │     (mlp): LlamaMLP(                        │ │
│ │                │   │   │   (gate_proj): Linear(in_features=4096, │ │
│ │                out_features=14336, bias=False)                   │ │
│ │                │   │   │   (up_proj): Linear(in_features=4096,   │ │
│ │                out_features=14336, bias=False)                   │ │
│ │                │   │   │   (down_proj):                          │ │
│ │                Linear(in_features=14336, out_features=4096,      │ │
│ │                bias=False)                                       │ │
│ │                │   │   │   (act_fn): SiLU()                      │ │
│ │                │   │     )                                       │ │
│ │                │   │     (input_layernorm):                      │ │
│ │                LlamaRMSNorm((4096,), eps=1e-05)                  │ │
│ │                │   │     (post_attention_layernorm):             │ │
│ │                LlamaRMSNorm((4096,), eps=1e-05)                  │ │
│ │                │   │   )                                         │ │
│ │                │     )                                           │ │
│ │                │     (norm): LlamaRMSNorm((4096,), eps=1e-05)    │ │
│ │                │     (rotary_emb): LlamaRotaryEmbedding()        │ │
│ │                │   )                                             │ │
│ │                │   (lm_head): Linear(in_features=4096,           │ │
│ │                out_features=128256, bias=False)                  │ │
│ │                  )                                               │ │
│ │                )>                                                │ │
│ │       kwargs = {}                                                │ │
│ │         self = JsonformerModel(                                  │ │
│ │                  (model): LlamaForCausalLM(                      │ │
│ │                │   (model): LlamaModel(                          │ │
│ │                │     (embed_tokens): Embedding(128256, 4096)     │ │
│ │                │     (layers): ModuleList(                       │ │
│ │                │   │   (0-31): 32 x LlamaDecoderLayer(           │ │
│ │                │   │     (self_attn): LlamaFlashAttention2(      │ │
│ │                │   │   │   (q_proj): Linear(in_features=4096,    │ │
│ │                out_features=4096, bias=False)                    │ │
│ │                │   │   │   (k_proj): Linear(in_features=4096,    │ │
│ │                out_features=1024, bias=False)                    │ │
│ │                │   │   │   (v_proj): Linear(in_features=4096,    │ │
│ │                out_features=1024, bias=False)                    │ │
│ │                │   │   │   (o_proj): Linear(in_features=4096,    │ │
│ │                out_features=4096, bias=False)                    │ │
│ │                │   │   │   (rotary_emb): LlamaRotaryEmbedding()  │ │
│ │                │   │     )                                       │ │
│ │                │   │     (mlp): LlamaMLP(                        │ │
│ │                │   │   │   (gate_proj): Linear(in_features=4096, │ │
│ │                out_features=14336, bias=False)                   │ │
│ │                │   │   │   (up_proj): Linear(in_features=4096,   │ │
│ │                out_features=14336, bias=False)                   │ │
│ │                │   │   │   (down_proj):                          │ │
│ │                Linear(in_features=14336, out_features=4096,      │ │
│ │                bias=False)                                       │ │
│ │                │   │   │   (act_fn): SiLU()                      │ │
│ │                │   │     )                                       │ │
│ │                │   │     (input_layernorm):                      │ │
│ │                LlamaRMSNorm((4096,), eps=1e-05)                  │ │
│ │                │   │     (post_attention_layernorm):             │ │
│ │                LlamaRMSNorm((4096,), eps=1e-05)                  │ │
│ │                │   │   )                                         │ │
│ │                │     )                                           │ │
│ │                │     (norm): LlamaRMSNorm((4096,), eps=1e-05)    │ │
│ │                │     (rotary_emb): LlamaRotaryEmbedding()        │ │
│ │                │   )                                             │ │
│ │                │   (lm_head): Linear(in_features=4096,           │ │
│ │                out_features=128256, bias=False)                  │ │
│ │                  )                                               │ │
│ │                )                                                 │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/master_thesis/legacy_c │
│ ode/src/mthesis/models.py:88 in forward                              │
│                                                                      │
│    85 │   │   │   │   jsonformer = Jsonformer(self.model, self.token │
│    86 │   │   │   else:                                              │
│    87 │   │   │   │   jsonformer = Jsonformer(self.model, self.token │
│ ❱  88 │   │   return jsonformer()                                    │
│    89 │                                                              │
│    90 │   def forward2(self, text: str):                             │
│    91 │   │   input_tokens = self.tokenizer.encode(text, return_tens │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │ full_prompt = '\n Anhydrous MnCl2 (398 mg, 3.16 mmol) and        │ │
│ │               H4L1·6.5H2O (225 mg, 0.27 mmol), were '+974        │ │
│ │  jsonformer = <jsonformer.main.Jsonformer object at              │ │
│ │               0x14849782a750>                                    │ │
│ │        self = JsonformerModel(                                   │ │
│ │                 (model): LlamaForCausalLM(                       │ │
│ │               │   (model): LlamaModel(                           │ │
│ │               │     (embed_tokens): Embedding(128256, 4096)      │ │
│ │               │     (layers): ModuleList(                        │ │
│ │               │   │   (0-31): 32 x LlamaDecoderLayer(            │ │
│ │               │   │     (self_attn): LlamaFlashAttention2(       │ │
│ │               │   │   │   (q_proj): Linear(in_features=4096,     │ │
│ │               out_features=4096, bias=False)                     │ │
│ │               │   │   │   (k_proj): Linear(in_features=4096,     │ │
│ │               out_features=1024, bias=False)                     │ │
│ │               │   │   │   (v_proj): Linear(in_features=4096,     │ │
│ │               out_features=1024, bias=False)                     │ │
│ │               │   │   │   (o_proj): Linear(in_features=4096,     │ │
│ │               out_features=4096, bias=False)                     │ │
│ │               │   │   │   (rotary_emb): LlamaRotaryEmbedding()   │ │
│ │               │   │     )                                        │ │
│ │               │   │     (mlp): LlamaMLP(                         │ │
│ │               │   │   │   (gate_proj): Linear(in_features=4096,  │ │
│ │               out_features=14336, bias=False)                    │ │
│ │               │   │   │   (up_proj): Linear(in_features=4096,    │ │
│ │               out_features=14336, bias=False)                    │ │
│ │               │   │   │   (down_proj): Linear(in_features=14336, │ │
│ │               out_features=4096, bias=False)                     │ │
│ │               │   │   │   (act_fn): SiLU()                       │ │
│ │               │   │     )                                        │ │
│ │               │   │     (input_layernorm): LlamaRMSNorm((4096,), │ │
│ │               eps=1e-05)                                         │ │
│ │               │   │     (post_attention_layernorm):              │ │
│ │               LlamaRMSNorm((4096,), eps=1e-05)                   │ │
│ │               │   │   )                                          │ │
│ │               │     )                                            │ │
│ │               │     (norm): LlamaRMSNorm((4096,), eps=1e-05)     │ │
│ │               │     (rotary_emb): LlamaRotaryEmbedding()         │ │
│ │               │   )                                              │ │
│ │               │   (lm_head): Linear(in_features=4096,            │ │
│ │               out_features=128256, bias=False)                   │ │
│ │                 )                                                │ │
│ │               )                                                  │ │
│ │        text = ' Anhydrous MnCl2 (398 mg, 3.16 mmol) and          │ │
│ │               H4L1·6.5H2O (225 mg, 0.27 mmol), were d'+973       │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/jsonformer/main.py:242 in __call__                      │
│                                                                      │
│   239 │                                                              │
│   240 │   def __call__(self) -> Dict[str, Any]:                      │
│   241 │   │   self.value = {}                                        │
│ ❱ 242 │   │   generated_data = self.generate_object(                 │
│   243 │   │   │   self.json_schema["properties"], self.value         │
│   244 │   │   )                                                      │
│   245 │   │   return generated_data                                  │
│                                                                      │
│ ╭─────────────────────────── locals ───────────────────────────╮     │
│ │ self = <jsonformer.main.Jsonformer object at 0x14849782a750> │     │
│ ╰──────────────────────────────────────────────────────────────╯     │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/jsonformer/main.py:147 in generate_object               │
│                                                                      │
│   144 │   ) -> Dict[str, Any]:                                       │
│   145 │   │   for key, schema in properties.items():                 │
│   146 │   │   │   self.debug("[generate_object] generating value for │
│ ❱ 147 │   │   │   obj[key] = self.generate_value(schema, obj, key)   │
│   148 │   │   return obj                                             │
│   149 │                                                              │
│   150 │   def generate_value(                                        │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │        key = 'temperature'                                       │ │
│ │        obj = {                                                   │ │
│ │              │   'additive': 'None',                             │ │
│ │              │   'solvent': 'DMF:MeOH',                          │ │
│ │              │   'temperature': '|GENERATION|'                   │ │
│ │              }                                                   │ │
│ │ properties = {                                                   │ │
│ │              │   'additive': {'type': 'string'},                 │ │
│ │              │   'solvent': {'type': 'string'},                  │ │
│ │              │   'temperature': {'type': 'number'},              │ │
│ │              │   'temperature_unit': {'type': 'string'},         │ │
│ │              │   'time': {'type': 'number'},                     │ │
│ │              │   'time_unit': {'type': 'string'}                 │ │
│ │              }                                                   │ │
│ │     schema = {'type': 'number'}                                  │ │
│ │       self = <jsonformer.main.Jsonformer object at               │ │
│ │              0x14849782a750>                                     │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/jsonformer/main.py:162 in generate_value                │
│                                                                      │
│   159 │   │   │   │   obj[key] = self.generation_marker              │
│   160 │   │   │   else:                                              │
│   161 │   │   │   │   obj.append(self.generation_marker)             │
│ ❱ 162 │   │   │   return self.generate_number()                      │
│   163 │   │   elif schema_type == "boolean":                         │
│   164 │   │   │   if key:                                            │
│   165 │   │   │   │   obj[key] = self.generation_marker              │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │         key = 'temperature'                                      │ │
│ │         obj = {                                                  │ │
│ │               │   'additive': 'None',                            │ │
│ │               │   'solvent': 'DMF:MeOH',                         │ │
│ │               │   'temperature': '|GENERATION|'                  │ │
│ │               }                                                  │ │
│ │      schema = {'type': 'number'}                                 │ │
│ │ schema_type = 'number'                                           │ │
│ │        self = <jsonformer.main.Jsonformer object at              │ │
│ │               0x14849782a750>                                    │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/jsonformer/main.py:61 in generate_number                │
│                                                                      │
│    58 │   │   input_tokens = self.tokenizer.encode(prompt, return_te │
│    59 │   │   │   self.model.device                                  │
│    60 │   │   )                                                      │
│ ❱  61 │   │   response = self.model.generate(                        │
│    62 │   │   │   input_tokens,                                      │
│    63 │   │   │   max_new_tokens=self.max_number_tokens,             │
│    64 │   │   │   num_return_sequences=1,                            │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │ input_tokens = tensor([[128000,    198,   1556,   8671,     67,  │ │
│ │                27620,  57831,   5176,     17,                    │ │
│ │                │   │   │   320,  19838,  14060,     11,    220,  │ │
│ │                18,     13,    845,   9653,                       │ │
│ │                │   │   │   337,      8,    323,    473,     19,  │ │
│ │                43,     16,  14260,     21,                       │ │
│ │                │   │   │    13,     20,     39,     17,     46,  │ │
│ │                320,  11057,  14060,     11,                      │ │
│ │                │   │   │   220,     15,     13,   1544,   9653,  │ │
│ │                337,    705,   1051,  56767,                      │ │
│ │                │   │   │   304,    264,  21655,    315,  20804,  │ │
│ │                37,    320,   1313,     13,                       │ │
│ │                │   │   │    20,  65170,      8,    323,   2206,  │ │
│ │                47861,    320,   1313,     13,                    │ │
│ │                │   │   │    20,  65170,      8,   4871,    264,  │ │
│ │                220,   1135,  65170,   9168,                      │ │
│ │                │   │   │   348,    532,  19584,    449,    264,  │ │
│ │                350,    830,  12490,  32393,                      │ │
│ │                │   │     22733,   2107,     13,    578,   6425,  │ │
│ │                574,  32813,    369,    220,                      │ │
│ │                │   │   │    23,   2919,    520,    220,   2031,  │ │
│ │                37386,     34,     13,  20902,                    │ │
│ │                │   │     65251,   1499,   3418,     81,    599,  │ │
│ │                543,    483,  88751,    988,                      │ │
│ │                │   │   │   315,   4661,   1933,   1752,  48473,  │ │
│ │                1051,  54568,   6041,    505,                     │ │
│ │                │   │   │   279,   2132,   1938,    315,  24494,  │ │
│ │                13,    578,  39887,    266,                       │ │
│ │                │   │   │   519,     11,   8649,    264,   9099,  │ │
│ │                3392,    315,   2536,  48689,                     │ │
│ │                │   │   │   599,    543,    483,  16946,     11,  │ │
│ │                574,   1654,   7719,    323,                      │ │
│ │                │   │   │   279,  64568,    483,  36841,  20227,  │ │
│ │                574,   6288,  23217,   1139,                      │ │
│ │                │   │   │   264,   5124,   2963,     74,  61319,  │ │
│ │                449,    264,    282,   1018,                      │ │
│ │                │   │   │   369,    279,  17876,  28786,    323,  │ │
│ │                76038,   7677,   1234,  81073,                    │ │
│ │                │   │      6962,     13,   4427,   9861,   2536,  │ │
│ │                48689,    599,    543,    483,                    │ │
│ │                │   │     14933,    953,  19020,   1051,  19180,  │ │
│ │                555,   9482,   2518,    279,                      │ │
│ │                │   │     64568,    483,  36841,  20227,   5361,  │ │
│ │                3115,    304,    264,    220,                     │ │
│ │                │   │   │    16,     25,     16,  21655,    315,  │ │
│ │                20804,     37,     25,   7979,                    │ │
│ │                │   │     47861,    323,  18054,    279,  39887,  │ │
│ │                266,    519,    449,    264,                      │ │
│ │                │   │     58325,    324,  24547,   6672,   8272,  │ │
│ │                555,   2033,  97139,    287,                      │ │
│ │                │   │   │   315,    279,  16946,    449,    279,  │ │
│ │                1890,  69996,  21655,    323,                     │ │
│ │                │   │     84878,  76038,     13,   3804,  72457,  │ │
│ │                25402,  46479,    304,   9467,                    │ │
│ │                │   │     24012,    320,    605,  34363,     17,  │ │
│ │                8611,     81,      8,    520,                     │ │
│ │                │   │   │   436,    739,     13,  58487,    220,  │ │
│ │                13384,  14060,    320,   5313,                    │ │
│ │                │   │   │     4,   3196,    389,    473,     19,  │ │
│ │                43,     16,  14260,     21,                       │ │
│ │                │   │   │    13,     20,     39,     17,     46,  │ │
│ │                8,    315,   4661,   1933,                        │ │
│ │                │   │      1752,  64568,    483,   2027,     13,  │ │
│ │                720,   5207,   1121,    304,                      │ │
│ │                │   │   │   279,   2768,   4823,  11036,   3645,  │ │
│ │                512,   5018,   1337,    794,                      │ │
│ │                │   │   │   330,   1735,    498,    330,  13495,  │ │
│ │                794,   5324,    723,   3486,                      │ │
│ │                │   │   │   794,   5324,   1337,    794,    330,  │ │
│ │                928,  14682,    330,  39298,                      │ │
│ │                │   │   │   688,    794,   5324,   1337,    794,  │ │
│ │                330,    928,  14682,    330,                      │ │
│ │                │   │     35658,    794,   5324,   1337,    794,  │ │
│ │                330,   4174,  14682,    330,                      │ │
│ │                │   │     35658,  15176,    794,   5324,   1337,  │ │
│ │                794,    330,    928,  14682,                      │ │
│ │                │   │   │   330,   1712,    794,   5324,   1337,  │ │
│ │                794,    330,   4174,  14682,                      │ │
│ │                │   │   │   330,   1712,  15176,    794,   5324,  │ │
│ │                1337,    794,    330,    928,                     │ │
│ │                │   │     32075,    534,   2122,     25,   5324,  │ │
│ │                723,   3486,    794,    330,                      │ │
│ │                │   │      4155,    498,    330,  39298,    688,  │ │
│ │                794,    330,   8561,     37,                      │ │
│ │                │   │   │    25,   7979,  47861,    498,    330,  │ │
│ │                35658,    794,    220]],                          │ │
│ │                │      device='cuda:0')                           │ │
│ │   iterations = 0                                                 │ │
│ │       prompt = '\n Anhydrous MnCl2 (398 mg, 3.16 mmol) and       │ │
│ │                H4L1·6.5H2O (225 mg, 0.27 mmol), were '+1327      │ │
│ │         self = <jsonformer.main.Jsonformer object at             │ │
│ │                0x14849782a750>                                   │ │
│ │  temperature = None                                              │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/torch/utils/_contextlib.py:116 in decorate_context      │
│                                                                      │
│   113 │   @functools.wraps(func)                                     │
│   114 │   def decorate_context(*args, **kwargs):                     │
│   115 │   │   with ctx_factory():                                    │
│ ❱ 116 │   │   │   return func(*args, **kwargs)                       │
│   117 │                                                              │
│   118 │   return decorate_context                                    │
│   119                                                                │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │        args = (                                                  │ │
│ │               │   LlamaForCausalLM(                              │ │
│ │                 (model): LlamaModel(                             │ │
│ │               │   (embed_tokens): Embedding(128256, 4096)        │ │
│ │               │   (layers): ModuleList(                          │ │
│ │               │     (0-31): 32 x LlamaDecoderLayer(              │ │
│ │               │   │   (self_attn): LlamaFlashAttention2(         │ │
│ │               │   │     (q_proj): Linear(in_features=4096,       │ │
│ │               out_features=4096, bias=False)                     │ │
│ │               │   │     (k_proj): Linear(in_features=4096,       │ │
│ │               out_features=1024, bias=False)                     │ │
│ │               │   │     (v_proj): Linear(in_features=4096,       │ │
│ │               out_features=1024, bias=False)                     │ │
│ │               │   │     (o_proj): Linear(in_features=4096,       │ │
│ │               out_features=4096, bias=False)                     │ │
│ │               │   │     (rotary_emb): LlamaRotaryEmbedding()     │ │
│ │               │   │   )                                          │ │
│ │               │   │   (mlp): LlamaMLP(                           │ │
│ │               │   │     (gate_proj): Linear(in_features=4096,    │ │
│ │               out_features=14336, bias=False)                    │ │
│ │               │   │     (up_proj): Linear(in_features=4096,      │ │
│ │               out_features=14336, bias=False)                    │ │
│ │               │   │     (down_proj): Linear(in_features=14336,   │ │
│ │               out_features=4096, bias=False)                     │ │
│ │               │   │     (act_fn): SiLU()                         │ │
│ │               │   │   )                                          │ │
│ │               │   │   (input_layernorm): LlamaRMSNorm((4096,),   │ │
│ │               eps=1e-05)                                         │ │
│ │               │   │   (post_attention_layernorm):                │ │
│ │               LlamaRMSNorm((4096,), eps=1e-05)                   │ │
│ │               │     )                                            │ │
│ │               │   )                                              │ │
│ │               │   (norm): LlamaRMSNorm((4096,), eps=1e-05)       │ │
│ │               │   (rotary_emb): LlamaRotaryEmbedding()           │ │
│ │                 )                                                │ │
│ │                 (lm_head): Linear(in_features=4096,              │ │
│ │               out_features=128256, bias=False)                   │ │
│ │               ),                                                 │ │
│ │               │   tensor([[128000,    198,   1556,   8671,       │ │
│ │               67,  27620,  57831,   5176,     17,                │ │
│ │               │   │   │   320,  19838,  14060,     11,    220,   │ │
│ │               18,     13,    845,   9653,                        │ │
│ │               │   │   │   337,      8,    323,    473,     19,   │ │
│ │               43,     16,  14260,     21,                        │ │
│ │               │   │   │    13,     20,     39,     17,     46,   │ │
│ │               320,  11057,  14060,     11,                       │ │
│ │               │   │   │   220,     15,     13,   1544,   9653,   │ │
│ │               337,    705,   1051,  56767,                       │ │
│ │               │   │   │   304,    264,  21655,    315,  20804,   │ │
│ │               37,    320,   1313,     13,                        │ │
│ │               │   │   │    20,  65170,      8,    323,   2206,   │ │
│ │               47861,    320,   1313,     13,                     │ │
│ │               │   │   │    20,  65170,      8,   4871,    264,   │ │
│ │               220,   1135,  65170,   9168,                       │ │
│ │               │   │   │   348,    532,  19584,    449,    264,   │ │
│ │               350,    830,  12490,  32393,                       │ │
│ │               │   │     22733,   2107,     13,    578,   6425,   │ │
│ │               574,  32813,    369,    220,                       │ │
│ │               │   │   │    23,   2919,    520,    220,   2031,   │ │
│ │               37386,     34,     13,  20902,                     │ │
│ │               │   │     65251,   1499,   3418,     81,    599,   │ │
│ │               543,    483,  88751,    988,                       │ │
│ │               │   │   │   315,   4661,   1933,   1752,  48473,   │ │
│ │               1051,  54568,   6041,    505,                      │ │
│ │               │   │   │   279,   2132,   1938,    315,  24494,   │ │
│ │               13,    578,  39887,    266,                        │ │
│ │               │   │   │   519,     11,   8649,    264,   9099,   │ │
│ │               3392,    315,   2536,  48689,                      │ │
│ │               │   │   │   599,    543,    483,  16946,     11,   │ │
│ │               574,   1654,   7719,    323,                       │ │
│ │               │   │   │   279,  64568,    483,  36841,  20227,   │ │
│ │               574,   6288,  23217,   1139,                       │ │
│ │               │   │   │   264,   5124,   2963,     74,  61319,   │ │
│ │               449,    264,    282,   1018,                       │ │
│ │               │   │   │   369,    279,  17876,  28786,    323,   │ │
│ │               76038,   7677,   1234,  81073,                     │ │
│ │               │   │      6962,     13,   4427,   9861,   2536,   │ │
│ │               48689,    599,    543,    483,                     │ │
│ │               │   │     14933,    953,  19020,   1051,  19180,   │ │
│ │               555,   9482,   2518,    279,                       │ │
│ │               │   │     64568,    483,  36841,  20227,   5361,   │ │
│ │               3115,    304,    264,    220,                      │ │
│ │               │   │   │    16,     25,     16,  21655,    315,   │ │
│ │               20804,     37,     25,   7979,                     │ │
│ │               │   │     47861,    323,  18054,    279,  39887,   │ │
│ │               266,    519,    449,    264,                       │ │
│ │               │   │     58325,    324,  24547,   6672,   8272,   │ │
│ │               555,   2033,  97139,    287,                       │ │
│ │               │   │   │   315,    279,  16946,    449,    279,   │ │
│ │               1890,  69996,  21655,    323,                      │ │
│ │               │   │     84878,  76038,     13,   3804,  72457,   │ │
│ │               25402,  46479,    304,   9467,                     │ │
│ │               │   │     24012,    320,    605,  34363,     17,   │ │
│ │               8611,     81,      8,    520,                      │ │
│ │               │   │   │   436,    739,     13,  58487,    220,   │ │
│ │               13384,  14060,    320,   5313,                     │ │
│ │               │   │   │     4,   3196,    389,    473,     19,   │ │
│ │               43,     16,  14260,     21,                        │ │
│ │               │   │   │    13,     20,     39,     17,     46,   │ │
│ │               8,    315,   4661,   1933,                         │ │
│ │               │   │      1752,  64568,    483,   2027,     13,   │ │
│ │               720,   5207,   1121,    304,                       │ │
│ │               │   │   │   279,   2768,   4823,  11036,   3645,   │ │
│ │               512,   5018,   1337,    794,                       │ │
│ │               │   │   │   330,   1735,    498,    330,  13495,   │ │
│ │               794,   5324,    723,   3486,                       │ │
│ │               │   │   │   794,   5324,   1337,    794,    330,   │ │
│ │               928,  14682,    330,  39298,                       │ │
│ │               │   │   │   688,    794,   5324,   1337,    794,   │ │
│ │               330,    928,  14682,    330,                       │ │
│ │               │   │     35658,    794,   5324,   1337,    794,   │ │
│ │               330,   4174,  14682,    330,                       │ │
│ │               │   │     35658,  15176,    794,   5324,   1337,   │ │
│ │               794,    330,    928,  14682,                       │ │
│ │               │   │   │   330,   1712,    794,   5324,   1337,   │ │
│ │               794,    330,   4174,  14682,                       │ │
│ │               │   │   │   330,   1712,  15176,    794,   5324,   │ │
│ │               1337,    794,    330,    928,                      │ │
│ │               │   │     32075,    534,   2122,     25,   5324,   │ │
│ │               723,   3486,    794,    330,                       │ │
│ │               │   │      4155,    498,    330,  39298,    688,   │ │
│ │               794,    330,   8561,     37,                       │ │
│ │               │   │   │    25,   7979,  47861,    498,    330,   │ │
│ │               35658,    794,    220]],                           │ │
│ │               │      device='cuda:0')                            │ │
│ │               )                                                  │ │
│ │ ctx_factory = <bound method _DecoratorContextManager.clone of    │ │
│ │               <torch.autograd.grad_mode.no_grad object at        │ │
│ │               0x1484ad78f490>>                                   │ │
│ │        func = <function GenerationMixin.generate at              │ │
│ │               0x1484ad78bba0>                                    │ │
│ │      kwargs = {                                                  │ │
│ │               │   'max_new_tokens': 6,                           │ │
│ │               │   'num_return_sequences': 1,                     │ │
│ │               │   'logits_processor': [                          │ │
│ │               │   │                                              │ │
│ │               <jsonformer.logits_processors.OutputNumbersTokens  │ │
│ │               object at 0x14849782a690>                          │ │
│ │               │   ],                                             │ │
│ │               │   'stopping_criteria': [                         │ │
│ │               │   │                                              │ │
│ │               <jsonformer.logits_processors.NumberStoppingCrite… │ │
│ │               object at 0x148499999c10>                          │ │
│ │               │   ],                                             │ │
│ │               │   'temperature': 0.1,                            │ │
│ │               │   'pad_token_id': 128009                         │ │
│ │               }                                                  │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/transformers/generation/utils.py:2252 in generate       │
│                                                                      │
│   2249 │   │   │   )                                                 │
│   2250 │   │   │                                                     │
│   2251 │   │   │   # 12. run sample (it degenerates to greedy search │
│ ❱ 2252 │   │   │   result = self._sample(                            │
│   2253 │   │   │   │   input_ids,                                    │
│   2254 │   │   │   │   logits_processor=prepared_logits_processor,   │
│   2255 │   │   │   │   stopping_criteria=prepared_stopping_criteria, │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │        accepts_attention_mask = True                             │ │
│ │               assistant_model = None                             │ │
│ │           assistant_tokenizer = None                             │ │
│ │                    batch_size = 1                                │ │
│ │                    cache_name = 'past_key_values'                │ │
│ │                        device = device(type='cuda', index=0)     │ │
│ │             generation_config = <repr-error 'Object of type      │ │
│ │                                 Tensor is not JSON               │ │
│ │                                 serializable'>                   │ │
│ │               generation_mode = <GenerationMode.SAMPLE:          │ │
│ │                                 'sample'>                        │ │
│ │        has_default_max_length = True                             │ │
│ │        has_default_min_length = True                             │ │
│ │                     input_ids = tensor([[128000,    198,   1556, │ │
│ │                                 8671,     67,  27620,  57831,    │ │
│ │                                 5176,     17,                    │ │
│ │                                 │   │   │   320,  19838,  14060, │ │
│ │                                 11,    220,     18,     13,      │ │
│ │                                 845,   9653,                     │ │
│ │                                 │   │   │   337,      8,    323, │ │
│ │                                 473,     19,     43,     16,     │ │
│ │                                 14260,     21,                   │ │
│ │                                 │   │   │    13,     20,     39, │ │
│ │                                 17,     46,    320,  11057,      │ │
│ │                                 14060,     11,                   │ │
│ │                                 │   │   │   220,     15,     13, │ │
│ │                                 1544,   9653,    337,    705,    │ │
│ │                                 1051,  56767,                    │ │
│ │                                 │   │   │   304,    264,  21655, │ │
│ │                                 315,  20804,     37,    320,     │ │
│ │                                 1313,     13,                    │ │
│ │                                 │   │   │    20,  65170,      8, │ │
│ │                                 323,   2206,  47861,    320,     │ │
│ │                                 1313,     13,                    │ │
│ │                                 │   │   │    20,  65170,      8, │ │
│ │                                 4871,    264,    220,   1135,    │ │
│ │                                 65170,   9168,                   │ │
│ │                                 │   │   │   348,    532,  19584, │ │
│ │                                 449,    264,    350,    830,     │ │
│ │                                 12490,  32393,                   │ │
│ │                                 │   │     22733,   2107,     13, │ │
│ │                                 578,   6425,    574,  32813,     │ │
│ │                                 369,    220,                     │ │
│ │                                 │   │   │    23,   2919,    520, │ │
│ │                                 220,   2031,  37386,     34,     │ │
│ │                                 13,  20902,                      │ │
│ │                                 │   │     65251,   1499,   3418, │ │
│ │                                 81,    599,    543,    483,      │ │
│ │                                 88751,    988,                   │ │
│ │                                 │   │   │   315,   4661,   1933, │ │
│ │                                 1752,  48473,   1051,  54568,    │ │
│ │                                 6041,    505,                    │ │
│ │                                 │   │   │   279,   2132,   1938, │ │
│ │                                 315,  24494,     13,    578,     │ │
│ │                                 39887,    266,                   │ │
│ │                                 │   │   │   519,     11,   8649, │ │
│ │                                 264,   9099,   3392,    315,     │ │
│ │                                 2536,  48689,                    │ │
│ │                                 │   │   │   599,    543,    483, │ │
│ │                                 16946,     11,    574,   1654,   │ │
│ │                                 7719,    323,                    │ │
│ │                                 │   │   │   279,  64568,    483, │ │
│ │                                 36841,  20227,    574,   6288,   │ │
│ │                                 23217,   1139,                   │ │
│ │                                 │   │   │   264,   5124,   2963, │ │
│ │                                 74,  61319,    449,    264,      │ │
│ │                                 282,   1018,                     │ │
│ │                                 │   │   │   369,    279,  17876, │ │
│ │                                 28786,    323,  76038,   7677,   │ │
│ │                                 1234,  81073,                    │ │
│ │                                 │   │      6962,     13,   4427, │ │
│ │                                 9861,   2536,  48689,    599,    │ │
│ │                                 543,    483,                     │ │
│ │                                 │   │     14933,    953,  19020, │ │
│ │                                 1051,  19180,    555,   9482,    │ │
│ │                                 2518,    279,                    │ │
│ │                                 │   │     64568,    483,  36841, │ │
│ │                                 20227,   5361,   3115,    304,   │ │
│ │                                 264,    220,                     │ │
│ │                                 │   │   │    16,     25,     16, │ │
│ │                                 21655,    315,  20804,     37,   │ │
│ │                                 25,   7979,                      │ │
│ │                                 │   │     47861,    323,  18054, │ │
│ │                                 279,  39887,    266,    519,     │ │
│ │                                 449,    264,                     │ │
│ │                                 │   │     58325,    324,  24547, │ │
│ │                                 6672,   8272,    555,   2033,    │ │
│ │                                 97139,    287,                   │ │
│ │                                 │   │   │   315,    279,  16946, │ │
│ │                                 449,    279,   1890,  69996,     │ │
│ │                                 21655,    323,                   │ │
│ │                                 │   │     84878,  76038,     13, │ │
│ │                                 3804,  72457,  25402,  46479,    │ │
│ │                                 304,   9467,                     │ │
│ │                                 │   │     24012,    320,    605, │ │
│ │                                 34363,     17,   8611,     81,   │ │
│ │                                 8,    520,                       │ │
│ │                                 │   │   │   436,    739,     13, │ │
│ │                                 58487,    220,  13384,  14060,   │ │
│ │                                 320,   5313,                     │ │
│ │                                 │   │   │     4,   3196,    389, │ │
│ │                                 473,     19,     43,     16,     │ │
│ │                                 14260,     21,                   │ │
│ │                                 │   │   │    13,     20,     39, │ │
│ │                                 17,     46,      8,    315,      │ │
│ │                                 4661,   1933,                    │ │
│ │                                 │   │      1752,  64568,    483, │ │
│ │                                 2027,     13,    720,   5207,    │ │
│ │                                 1121,    304,                    │ │
│ │                                 │   │   │   279,   2768,   4823, │ │
│ │                                 11036,   3645,    512,   5018,   │ │
│ │                                 1337,    794,                    │ │
│ │                                 │   │   │   330,   1735,    498, │ │
│ │                                 330,  13495,    794,   5324,     │ │
│ │                                 723,   3486,                     │ │
│ │                                 │   │   │   794,   5324,   1337, │ │
│ │                                 794,    330,    928,  14682,     │ │
│ │                                 330,  39298,                     │ │
│ │                                 │   │   │   688,    794,   5324, │ │
│ │                                 1337,    794,    330,    928,    │ │
│ │                                 14682,    330,                   │ │
│ │                                 │   │     35658,    794,   5324, │ │
│ │                                 1337,    794,    330,   4174,    │ │
│ │                                 14682,    330,                   │ │
│ │                                 │   │     35658,  15176,    794, │ │
│ │                                 5324,   1337,    794,    330,    │ │
│ │                                 928,  14682,                     │ │
│ │                                 │   │   │   330,   1712,    794, │ │
│ │                                 5324,   1337,    794,    330,    │ │
│ │                                 4174,  14682,                    │ │
│ │                                 │   │   │   330,   1712,  15176, │ │
│ │                                 794,   5324,   1337,    794,     │ │
│ │                                 330,    928,                     │ │
│ │                                 │   │     32075,    534,   2122, │ │
│ │                                 25,   5324,    723,   3486,      │ │
│ │                                 794,    330,                     │ │
│ │                                 │   │      4155,    498,    330, │ │
│ │                                 39298,    688,    794,    330,   │ │
│ │                                 8561,     37,                    │ │
│ │                                 │   │   │    25,   7979,  47861, │ │
│ │                                 498,    330,  35658,    794,     │ │
│ │                                 220]],                           │ │
│ │                                 │      device='cuda:0')          │ │
│ │              input_ids_length = 386                              │ │
│ │                        inputs = tensor([[128000,    198,   1556, │ │
│ │                                 8671,     67,  27620,  57831,    │ │
│ │                                 5176,     17,                    │ │
│ │                                 │   │   │   320,  19838,  14060, │ │
│ │                                 11,    220,     18,     13,      │ │
│ │                                 845,   9653,                     │ │
│ │                                 │   │   │   337,      8,    323, │ │
│ │                                 473,     19,     43,     16,     │ │
│ │                                 14260,     21,                   │ │
│ │                                 │   │   │    13,     20,     39, │ │
│ │                                 17,     46,    320,  11057,      │ │
│ │                                 14060,     11,                   │ │
│ │                                 │   │   │   220,     15,     13, │ │
│ │                                 1544,   9653,    337,    705,    │ │
│ │                                 1051,  56767,                    │ │
│ │                                 │   │   │   304,    264,  21655, │ │
│ │                                 315,  20804,     37,    320,     │ │
│ │                                 1313,     13,                    │ │
│ │                                 │   │   │    20,  65170,      8, │ │
│ │                                 323,   2206,  47861,    320,     │ │
│ │                                 1313,     13,                    │ │
│ │                                 │   │   │    20,  65170,      8, │ │
│ │                                 4871,    264,    220,   1135,    │ │
│ │                                 65170,   9168,                   │ │
│ │                                 │   │   │   348,    532,  19584, │ │
│ │                                 449,    264,    350,    830,     │ │
│ │                                 12490,  32393,                   │ │
│ │                                 │   │     22733,   2107,     13, │ │
│ │                                 578,   6425,    574,  32813,     │ │
│ │                                 369,    220,                     │ │
│ │                                 │   │   │    23,   2919,    520, │ │
│ │                                 220,   2031,  37386,     34,     │ │
│ │                                 13,  20902,                      │ │
│ │                                 │   │     65251,   1499,   3418, │ │
│ │                                 81,    599,    543,    483,      │ │
│ │                                 88751,    988,                   │ │
│ │                                 │   │   │   315,   4661,   1933, │ │
│ │                                 1752,  48473,   1051,  54568,    │ │
│ │                                 6041,    505,                    │ │
│ │                                 │   │   │   279,   2132,   1938, │ │
│ │                                 315,  24494,     13,    578,     │ │
│ │                                 39887,    266,                   │ │
│ │                                 │   │   │   519,     11,   8649, │ │
│ │                                 264,   9099,   3392,    315,     │ │
│ │                                 2536,  48689,                    │ │
│ │                                 │   │   │   599,    543,    483, │ │
│ │                                 16946,     11,    574,   1654,   │ │
│ │                                 7719,    323,                    │ │
│ │                                 │   │   │   279,  64568,    483, │ │
│ │                                 36841,  20227,    574,   6288,   │ │
│ │                                 23217,   1139,                   │ │
│ │                                 │   │   │   264,   5124,   2963, │ │
│ │                                 74,  61319,    449,    264,      │ │
│ │                                 282,   1018,                     │ │
│ │                                 │   │   │   369,    279,  17876, │ │
│ │                                 28786,    323,  76038,   7677,   │ │
│ │                                 1234,  81073,                    │ │
│ │                                 │   │      6962,     13,   4427, │ │
│ │                                 9861,   2536,  48689,    599,    │ │
│ │                                 543,    483,                     │ │
│ │                                 │   │     14933,    953,  19020, │ │
│ │                                 1051,  19180,    555,   9482,    │ │
│ │                                 2518,    279,                    │ │
│ │                                 │   │     64568,    483,  36841, │ │
│ │                                 20227,   5361,   3115,    304,   │ │
│ │                                 264,    220,                     │ │
│ │                                 │   │   │    16,     25,     16, │ │
│ │                                 21655,    315,  20804,     37,   │ │
│ │                                 25,   7979,                      │ │
│ │                                 │   │     47861,    323,  18054, │ │
│ │                                 279,  39887,    266,    519,     │ │
│ │                                 449,    264,                     │ │
│ │                                 │   │     58325,    324,  24547, │ │
│ │                                 6672,   8272,    555,   2033,    │ │
│ │                                 97139,    287,                   │ │
│ │                                 │   │   │   315,    279,  16946, │ │
│ │                                 449,    279,   1890,  69996,     │ │
│ │                                 21655,    323,                   │ │
│ │                                 │   │     84878,  76038,     13, │ │
│ │                                 3804,  72457,  25402,  46479,    │ │
│ │                                 304,   9467,                     │ │
│ │                                 │   │     24012,    320,    605, │ │
│ │                                 34363,     17,   8611,     81,   │ │
│ │                                 8,    520,                       │ │
│ │                                 │   │   │   436,    739,     13, │ │
│ │                                 58487,    220,  13384,  14060,   │ │
│ │                                 320,   5313,                     │ │
│ │                                 │   │   │     4,   3196,    389, │ │
│ │                                 473,     19,     43,     16,     │ │
│ │                                 14260,     21,                   │ │
│ │                                 │   │   │    13,     20,     39, │ │
│ │                                 17,     46,      8,    315,      │ │
│ │                                 4661,   1933,                    │ │
│ │                                 │   │      1752,  64568,    483, │ │
│ │                                 2027,     13,    720,   5207,    │ │
│ │                                 1121,    304,                    │ │
│ │                                 │   │   │   279,   2768,   4823, │ │
│ │                                 11036,   3645,    512,   5018,   │ │
│ │                                 1337,    794,                    │ │
│ │                                 │   │   │   330,   1735,    498, │ │
│ │                                 330,  13495,    794,   5324,     │ │
│ │                                 723,   3486,                     │ │
│ │                                 │   │   │   794,   5324,   1337, │ │
│ │                                 794,    330,    928,  14682,     │ │
│ │                                 330,  39298,                     │ │
│ │                                 │   │   │   688,    794,   5324, │ │
│ │                                 1337,    794,    330,    928,    │ │
│ │                                 14682,    330,                   │ │
│ │                                 │   │     35658,    794,   5324, │ │
│ │                                 1337,    794,    330,   4174,    │ │
│ │                                 14682,    330,                   │ │
│ │                                 │   │     35658,  15176,    794, │ │
│ │                                 5324,   1337,    794,    330,    │ │
│ │                                 928,  14682,                     │ │
│ │                                 │   │   │   330,   1712,    794, │ │
│ │                                 5324,   1337,    794,    330,    │ │
│ │                                 4174,  14682,                    │ │
│ │                                 │   │   │   330,   1712,  15176, │ │
│ │                                 794,   5324,   1337,    794,     │ │
│ │                                 330,    928,                     │ │
│ │                                 │   │     32075,    534,   2122, │ │
│ │                                 25,   5324,    723,   3486,      │ │
│ │                                 794,    330,                     │ │
│ │                                 │   │      4155,    498,    330, │ │
│ │                                 39298,    688,    794,    330,   │ │
│ │                                 8561,     37,                    │ │
│ │                                 │   │   │    25,   7979,  47861, │ │
│ │                                 498,    330,  35658,    794,     │ │
│ │                                 220]],                           │ │
│ │                                 │      device='cuda:0')          │ │
│ │                 inputs_tensor = tensor([[128000,    198,   1556, │ │
│ │                                 8671,     67,  27620,  57831,    │ │
│ │                                 5176,     17,                    │ │
│ │                                 │   │   │   320,  19838,  14060, │ │
│ │                                 11,    220,     18,     13,      │ │
│ │                                 845,   9653,                     │ │
│ │                                 │   │   │   337,      8,    323, │ │
│ │                                 473,     19,     43,     16,     │ │
│ │                                 14260,     21,                   │ │
│ │                                 │   │   │    13,     20,     39, │ │
│ │                                 17,     46,    320,  11057,      │ │
│ │                                 14060,     11,                   │ │
│ │                                 │   │   │   220,     15,     13, │ │
│ │                                 1544,   9653,    337,    705,    │ │
│ │                                 1051,  56767,                    │ │
│ │                                 │   │   │   304,    264,  21655, │ │
│ │                                 315,  20804,     37,    320,     │ │
│ │                                 1313,     13,                    │ │
│ │                                 │   │   │    20,  65170,      8, │ │
│ │                                 323,   2206,  47861,    320,     │ │
│ │                                 1313,     13,                    │ │
│ │                                 │   │   │    20,  65170,      8, │ │
│ │                                 4871,    264,    220,   1135,    │ │
│ │                                 65170,   9168,                   │ │
│ │                                 │   │   │   348,    532,  19584, │ │
│ │                                 449,    264,    350,    830,     │ │
│ │                                 12490,  32393,                   │ │
│ │                                 │   │     22733,   2107,     13, │ │
│ │                                 578,   6425,    574,  32813,     │ │
│ │                                 369,    220,                     │ │
│ │                                 │   │   │    23,   2919,    520, │ │
│ │                                 220,   2031,  37386,     34,     │ │
│ │                                 13,  20902,                      │ │
│ │                                 │   │     65251,   1499,   3418, │ │
│ │                                 81,    599,    543,    483,      │ │
│ │                                 88751,    988,                   │ │
│ │                                 │   │   │   315,   4661,   1933, │ │
│ │                                 1752,  48473,   1051,  54568,    │ │
│ │                                 6041,    505,                    │ │
│ │                                 │   │   │   279,   2132,   1938, │ │
│ │                                 315,  24494,     13,    578,     │ │
│ │                                 39887,    266,                   │ │
│ │                                 │   │   │   519,     11,   8649, │ │
│ │                                 264,   9099,   3392,    315,     │ │
│ │                                 2536,  48689,                    │ │
│ │                                 │   │   │   599,    543,    483, │ │
│ │                                 16946,     11,    574,   1654,   │ │
│ │                                 7719,    323,                    │ │
│ │                                 │   │   │   279,  64568,    483, │ │
│ │                                 36841,  20227,    574,   6288,   │ │
│ │                                 23217,   1139,                   │ │
│ │                                 │   │   │   264,   5124,   2963, │ │
│ │                                 74,  61319,    449,    264,      │ │
│ │                                 282,   1018,                     │ │
│ │                                 │   │   │   369,    279,  17876, │ │
│ │                                 28786,    323,  76038,   7677,   │ │
│ │                                 1234,  81073,                    │ │
│ │                                 │   │      6962,     13,   4427, │ │
│ │                                 9861,   2536,  48689,    599,    │ │
│ │                                 543,    483,                     │ │
│ │                                 │   │     14933,    953,  19020, │ │
│ │                                 1051,  19180,    555,   9482,    │ │
│ │                                 2518,    279,                    │ │
│ │                                 │   │     64568,    483,  36841, │ │
│ │                                 20227,   5361,   3115,    304,   │ │
│ │                                 264,    220,                     │ │
│ │                                 │   │   │    16,     25,     16, │ │
│ │                                 21655,    315,  20804,     37,   │ │
│ │                                 25,   7979,                      │ │
│ │                                 │   │     47861,    323,  18054, │ │
│ │                                 279,  39887,    266,    519,     │ │
│ │                                 449,    264,                     │ │
│ │                                 │   │     58325,    324,  24547, │ │
│ │                                 6672,   8272,    555,   2033,    │ │
│ │                                 97139,    287,                   │ │
│ │                                 │   │   │   315,    279,  16946, │ │
│ │                                 449,    279,   1890,  69996,     │ │
│ │                                 21655,    323,                   │ │
│ │                                 │   │     84878,  76038,     13, │ │
│ │                                 3804,  72457,  25402,  46479,    │ │
│ │                                 304,   9467,                     │ │
│ │                                 │   │     24012,    320,    605, │ │
│ │                                 34363,     17,   8611,     81,   │ │
│ │                                 8,    520,                       │ │
│ │                                 │   │   │   436,    739,     13, │ │
│ │                                 58487,    220,  13384,  14060,   │ │
│ │                                 320,   5313,                     │ │
│ │                                 │   │   │     4,   3196,    389, │ │
│ │                                 473,     19,     43,     16,     │ │
│ │                                 14260,     21,                   │ │
│ │                                 │   │   │    13,     20,     39, │ │
│ │                                 17,     46,      8,    315,      │ │
│ │                                 4661,   1933,                    │ │
│ │                                 │   │      1752,  64568,    483, │ │
│ │                                 2027,     13,    720,   5207,    │ │
│ │                                 1121,    304,                    │ │
│ │                                 │   │   │   279,   2768,   4823, │ │
│ │                                 11036,   3645,    512,   5018,   │ │
│ │                                 1337,    794,                    │ │
│ │                                 │   │   │   330,   1735,    498, │ │
│ │                                 330,  13495,    794,   5324,     │ │
│ │                                 723,   3486,                     │ │
│ │                                 │   │   │   794,   5324,   1337, │ │
│ │                                 794,    330,    928,  14682,     │ │
│ │                                 330,  39298,                     │ │
│ │                                 │   │   │   688,    794,   5324, │ │
│ │                                 1337,    794,    330,    928,    │ │
│ │                                 14682,    330,                   │ │
│ │                                 │   │     35658,    794,   5324, │ │
│ │                                 1337,    794,    330,   4174,    │ │
│ │                                 14682,    330,                   │ │
│ │                                 │   │     35658,  15176,    794, │ │
│ │                                 5324,   1337,    794,    330,    │ │
│ │                                 928,  14682,                     │ │
│ │                                 │   │   │   330,   1712,    794, │ │
│ │                                 5324,   1337,    794,    330,    │ │
│ │                                 4174,  14682,                    │ │
│ │                                 │   │   │   330,   1712,  15176, │ │
│ │                                 794,   5324,   1337,    794,     │ │
│ │                                 330,    928,                     │ │
│ │                                 │   │     32075,    534,   2122, │ │
│ │                                 25,   5324,    723,   3486,      │ │
│ │                                 794,    330,                     │ │
│ │                                 │   │      4155,    498,    330, │ │
│ │                                 39298,    688,    794,    330,   │ │
│ │                                 8561,     37,                    │ │
│ │                                 │   │   │    25,   7979,  47861, │ │
│ │                                 498,    330,  35658,    794,     │ │
│ │                                 220]],                           │ │
│ │                                 │      device='cuda:0')          │ │
│ │                        kwargs = {                                │ │
│ │                                 │   'max_new_tokens': 6,         │ │
│ │                                 │   'num_return_sequences': 1,   │ │
│ │                                 │   'temperature': 0.1,          │ │
│ │                                 │   'pad_token_id': 128009       │ │
│ │                                 }                                │ │
│ │     kwargs_has_attention_mask = False                            │ │
│ │              logits_processor = [                                │ │
│ │                                 │                                │ │
│ │                                 <jsonformer.logits_processors.O… │ │
│ │                                 object at 0x14849782a690>        │ │
│ │                                 ]                                │ │
│ │              max_cache_length = 392                              │ │
│ │              model_input_name = 'input_ids'                      │ │
│ │                  model_kwargs = {                                │ │
│ │                                 │   'attention_mask':            │ │
│ │                                 tensor([[1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, │ │
│ │                                 1, 1, 1, 1, 1,                   │ │
│ │                                 │   │    1, 1]],                 │ │
│ │                                 device='cuda:0'),                │ │
│ │                                 │   'num_logits_to_keep': 1,     │ │
│ │                                 │   'past_key_values':           │ │
│ │                                 DynamicCache(),                  │ │
│ │                                 │   'use_cache': True            │ │
│ │                                 }                                │ │
│ │  negative_prompt_attention_mask None                             │ │
│ │                               =                                  │ │
│ │           negative_prompt_ids = None                             │ │
│ │      prefix_allowed_tokens_fn = None                             │ │
│ │     prepared_logits_processor = [                                │ │
│ │                                 │                                │ │
│ │                                 <jsonformer.logits_processors.O… │ │
│ │                                 object at 0x14849782a690>,       │ │
│ │                                 │                                │ │
│ │                                 <transformers.generation.logits… │ │
│ │                                 object at 0x1484977c8f10>,       │ │
│ │                                 │                                │ │
│ │                                 <transformers.generation.logits… │ │
│ │                                 object at 0x148497807bd0>,       │ │
│ │                                 │                                │ │
│ │                                 <transformers.generation.logits… │ │
│ │                                 object at 0x148497876b90>        │ │
│ │                                 ]                                │ │
│ │    prepared_stopping_criteria = [                                │ │
│ │                                 │                                │ │
│ │                                 <transformers.generation.stoppi… │ │
│ │                                 object at 0x148497876e10>,       │ │
│ │                                 │                                │ │
│ │                                 <transformers.generation.stoppi… │ │
│ │                                 object at 0x14849782bdd0>,       │ │
│ │                                 │                                │ │
│ │                                 <jsonformer.logits_processors.N… │ │
│ │                                 object at 0x148499999c10>        │ │
│ │                                 ]                                │ │
│ │       requires_attention_mask = True                             │ │
│ │                          self = LlamaForCausalLM(                │ │
│ │                                   (model): LlamaModel(           │ │
│ │                                 │   (embed_tokens):              │ │
│ │                                 Embedding(128256, 4096)          │ │
│ │                                 │   (layers): ModuleList(        │ │
│ │                                 │     (0-31): 32 x               │ │
│ │                                 LlamaDecoderLayer(               │ │
│ │                                 │   │   (self_attn):             │ │
│ │                                 LlamaFlashAttention2(            │ │
│ │                                 │   │     (q_proj):              │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=4096, bias=False)   │ │
│ │                                 │   │     (k_proj):              │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=1024, bias=False)   │ │
│ │                                 │   │     (v_proj):              │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=1024, bias=False)   │ │
│ │                                 │   │     (o_proj):              │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=4096, bias=False)   │ │
│ │                                 │   │     (rotary_emb):          │ │
│ │                                 LlamaRotaryEmbedding()           │ │
│ │                                 │   │   )                        │ │
│ │                                 │   │   (mlp): LlamaMLP(         │ │
│ │                                 │   │     (gate_proj):           │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=14336, bias=False)  │ │
│ │                                 │   │     (up_proj):             │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=14336, bias=False)  │ │
│ │                                 │   │     (down_proj):           │ │
│ │                                 Linear(in_features=14336,        │ │
│ │                                 out_features=4096, bias=False)   │ │
│ │                                 │   │     (act_fn): SiLU()       │ │
│ │                                 │   │   )                        │ │
│ │                                 │   │   (input_layernorm):       │ │
│ │                                 LlamaRMSNorm((4096,), eps=1e-05) │ │
│ │                                 │   │                            │ │
│ │                                 (post_attention_layernorm):      │ │
│ │                                 LlamaRMSNorm((4096,), eps=1e-05) │ │
│ │                                 │     )                          │ │
│ │                                 │   )                            │ │
│ │                                 │   (norm):                      │ │
│ │                                 LlamaRMSNorm((4096,), eps=1e-05) │ │
│ │                                 │   (rotary_emb):                │ │
│ │                                 LlamaRotaryEmbedding()           │ │
│ │                                   )                              │ │
│ │                                   (lm_head):                     │ │
│ │                                 Linear(in_features=4096,         │ │
│ │                                 out_features=128256, bias=False) │ │
│ │                                 )                                │ │
│ │             stopping_criteria = [                                │ │
│ │                                 │                                │ │
│ │                                 <jsonformer.logits_processors.N… │ │
│ │                                 object at 0x148499999c10>        │ │
│ │                                 ]                                │ │
│ │                      streamer = None                             │ │
│ │                   synced_gpus = False                            │ │
│ │                     tokenizer = None                             │ │
│ │            user_defined_cache = None                             │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/transformers/generation/utils.py:3271 in _sample        │
│                                                                      │
│   3268 │   │   │   next_token_logits = next_token_logits.to(input_id │
│   3269 │   │   │                                                     │
│   3270 │   │   │   # pre-process distribution                        │
│ ❱ 3271 │   │   │   next_token_scores = logits_processor(input_ids, n │
│   3272 │   │   │                                                     │
│   3273 │   │   │   # Store scores, attentions and hidden_states when │
│   3274 │   │   │   if return_dict_in_generate:                       │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │                batch_size = 1                                    │ │
│ │          cross_attentions = None                                 │ │
│ │                   cur_len = 386                                  │ │
│ │        decoder_attentions = None                                 │ │
│ │     decoder_hidden_states = None                                 │ │
│ │                 do_sample = True                                 │ │
│ │         generation_config = <repr-error 'Object of type Tensor   │ │
│ │                             is not JSON serializable'>           │ │
│ │ has_eos_stopping_criteria = True                                 │ │
│ │                 input_ids = tensor([[128000,    198,   1556,     │ │
│ │                             8671,     67,  27620,  57831,        │ │
│ │                             5176,     17,                        │ │
│ │                             │   │   │   320,  19838,  14060,     │ │
│ │                             11,    220,     18,     13,    845,  │ │
│ │                             9653,                                │ │
│ │                             │   │   │   337,      8,    323,     │ │
│ │                             473,     19,     43,     16,  14260, │ │
│ │                             21,                                  │ │
│ │                             │   │   │    13,     20,     39,     │ │
│ │                             17,     46,    320,  11057,  14060,  │ │
│ │                             11,                                  │ │
│ │                             │   │   │   220,     15,     13,     │ │
│ │                             1544,   9653,    337,    705,        │ │
│ │                             1051,  56767,                        │ │
│ │                             │   │   │   304,    264,  21655,     │ │
│ │                             315,  20804,     37,    320,   1313, │ │
│ │                             13,                                  │ │
│ │                             │   │   │    20,  65170,      8,     │ │
│ │                             323,   2206,  47861,    320,   1313, │ │
│ │                             13,                                  │ │
│ │                             │   │   │    20,  65170,      8,     │ │
│ │                             4871,    264,    220,   1135,        │ │
│ │                             65170,   9168,                       │ │
│ │                             │   │   │   348,    532,  19584,     │ │
│ │                             449,    264,    350,    830,  12490, │ │
│ │                             32393,                               │ │
│ │                             │   │     22733,   2107,     13,     │ │
│ │                             578,   6425,    574,  32813,    369, │ │
│ │                             220,                                 │ │
│ │                             │   │   │    23,   2919,    520,     │ │
│ │                             220,   2031,  37386,     34,     13, │ │
│ │                             20902,                               │ │
│ │                             │   │     65251,   1499,   3418,     │ │
│ │                             81,    599,    543,    483,  88751,  │ │
│ │                             988,                                 │ │
│ │                             │   │   │   315,   4661,   1933,     │ │
│ │                             1752,  48473,   1051,  54568,        │ │
│ │                             6041,    505,                        │ │
│ │                             │   │   │   279,   2132,   1938,     │ │
│ │                             315,  24494,     13,    578,  39887, │ │
│ │                             266,                                 │ │
│ │                             │   │   │   519,     11,   8649,     │ │
│ │                             264,   9099,   3392,    315,   2536, │ │
│ │                             48689,                               │ │
│ │                             │   │   │   599,    543,    483,     │ │
│ │                             16946,     11,    574,   1654,       │ │
│ │                             7719,    323,                        │ │
│ │                             │   │   │   279,  64568,    483,     │ │
│ │                             36841,  20227,    574,   6288,       │ │
│ │                             23217,   1139,                       │ │
│ │                             │   │   │   264,   5124,   2963,     │ │
│ │                             74,  61319,    449,    264,    282,  │ │
│ │                             1018,                                │ │
│ │                             │   │   │   369,    279,  17876,     │ │
│ │                             28786,    323,  76038,   7677,       │ │
│ │                             1234,  81073,                        │ │
│ │                             │   │      6962,     13,   4427,     │ │
│ │                             9861,   2536,  48689,    599,        │ │
│ │                             543,    483,                         │ │
│ │                             │   │     14933,    953,  19020,     │ │
│ │                             1051,  19180,    555,   9482,        │ │
│ │                             2518,    279,                        │ │
│ │                             │   │     64568,    483,  36841,     │ │
│ │                             20227,   5361,   3115,    304,       │ │
│ │                             264,    220,                         │ │
│ │                             │   │   │    16,     25,     16,     │ │
│ │                             21655,    315,  20804,     37,       │ │
│ │                             25,   7979,                          │ │
│ │                             │   │     47861,    323,  18054,     │ │
│ │                             279,  39887,    266,    519,    449, │ │
│ │                             264,                                 │ │
│ │                             │   │     58325,    324,  24547,     │ │
│ │                             6672,   8272,    555,   2033,        │ │
│ │                             97139,    287,                       │ │
│ │                             │   │   │   315,    279,  16946,     │ │
│ │                             449,    279,   1890,  69996,  21655, │ │
│ │                             323,                                 │ │
│ │                             │   │     84878,  76038,     13,     │ │
│ │                             3804,  72457,  25402,  46479,        │ │
│ │                             304,   9467,                         │ │
│ │                             │   │     24012,    320,    605,     │ │
│ │                             34363,     17,   8611,     81,       │ │
│ │                             8,    520,                           │ │
│ │                             │   │   │   436,    739,     13,     │ │
│ │                             58487,    220,  13384,  14060,       │ │
│ │                             320,   5313,                         │ │
│ │                             │   │   │     4,   3196,    389,     │ │
│ │                             473,     19,     43,     16,  14260, │ │
│ │                             21,                                  │ │
│ │                             │   │   │    13,     20,     39,     │ │
│ │                             17,     46,      8,    315,   4661,  │ │
│ │                             1933,                                │ │
│ │                             │   │      1752,  64568,    483,     │ │
│ │                             2027,     13,    720,   5207,        │ │
│ │                             1121,    304,                        │ │
│ │                             │   │   │   279,   2768,   4823,     │ │
│ │                             11036,   3645,    512,   5018,       │ │
│ │                             1337,    794,                        │ │
│ │                             │   │   │   330,   1735,    498,     │ │
│ │                             330,  13495,    794,   5324,    723, │ │
│ │                             3486,                                │ │
│ │                             │   │   │   794,   5324,   1337,     │ │
│ │                             794,    330,    928,  14682,    330, │ │
│ │                             39298,                               │ │
│ │                             │   │   │   688,    794,   5324,     │ │
│ │                             1337,    794,    330,    928,        │ │
│ │                             14682,    330,                       │ │
│ │                             │   │     35658,    794,   5324,     │ │
│ │                             1337,    794,    330,   4174,        │ │
│ │                             14682,    330,                       │ │
│ │                             │   │     35658,  15176,    794,     │ │
│ │                             5324,   1337,    794,    330,        │ │
│ │                             928,  14682,                         │ │
│ │                             │   │   │   330,   1712,    794,     │ │
│ │                             5324,   1337,    794,    330,        │ │
│ │                             4174,  14682,                        │ │
│ │                             │   │   │   330,   1712,  15176,     │ │
│ │                             794,   5324,   1337,    794,    330, │ │
│ │                             928,                                 │ │
│ │                             │   │     32075,    534,   2122,     │ │
│ │                             25,   5324,    723,   3486,    794,  │ │
│ │                             330,                                 │ │
│ │                             │   │      4155,    498,    330,     │ │
│ │                             39298,    688,    794,    330,       │ │
│ │                             8561,     37,                        │ │
│ │                             │   │   │    25,   7979,  47861,     │ │
│ │                             498,    330,  35658,    794,         │ │
│ │                             220]],                               │ │
│ │                             │      device='cuda:0')              │ │
│ │                is_prefill = False                                │ │
│ │          logits_processor = [                                    │ │
│ │                             │                                    │ │
│ │                             <jsonformer.logits_processors.Outpu… │ │
│ │                             object at 0x14849782a690>,           │ │
│ │                             │                                    │ │
│ │                             <transformers.generation.logits_pro… │ │
│ │                             object at 0x1484977c8f10>,           │ │
│ │                             │                                    │ │
│ │                             <transformers.generation.logits_pro… │ │
│ │                             object at 0x148497807bd0>,           │ │
│ │                             │                                    │ │
│ │                             <transformers.generation.logits_pro… │ │
│ │                             object at 0x148497876b90>            │ │
│ │                             ]                                    │ │
│ │                max_length = 392                                  │ │
│ │             model_forward = <bound method                        │ │
│ │                             Module._wrapped_call_impl of         │ │
│ │                             LlamaForCausalLM(                    │ │
│ │                               (model): LlamaModel(               │ │
│ │                             │   (embed_tokens):                  │ │
│ │                             Embedding(128256, 4096)              │ │
│ │                             │   (layers): ModuleList(            │ │
│ │                             │     (0-31): 32 x                   │ │
│ │                             LlamaDecoderLayer(                   │ │
│ │                             │   │   (self_attn):                 │ │
│ │                             LlamaFlashAttention2(                │ │
│ │                             │   │     (q_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=4096, bias=False)       │ │
│ │                             │   │     (k_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=1024, bias=False)       │ │
│ │                             │   │     (v_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=1024, bias=False)       │ │
│ │                             │   │     (o_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=4096, bias=False)       │ │
│ │                             │   │     (rotary_emb):              │ │
│ │                             LlamaRotaryEmbedding()               │ │
│ │                             │   │   )                            │ │
│ │                             │   │   (mlp): LlamaMLP(             │ │
│ │                             │   │     (gate_proj):               │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=14336, bias=False)      │ │
│ │                             │   │     (up_proj):                 │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=14336, bias=False)      │ │
│ │                             │   │     (down_proj):               │ │
│ │                             Linear(in_features=14336,            │ │
│ │                             out_features=4096, bias=False)       │ │
│ │                             │   │     (act_fn): SiLU()           │ │
│ │                             │   │   )                            │ │
│ │                             │   │   (input_layernorm):           │ │
│ │                             LlamaRMSNorm((4096,), eps=1e-05)     │ │
│ │                             │   │   (post_attention_layernorm):  │ │
│ │                             LlamaRMSNorm((4096,), eps=1e-05)     │ │
│ │                             │     )                              │ │
│ │                             │   )                                │ │
│ │                             │   (norm): LlamaRMSNorm((4096,),    │ │
│ │                             eps=1e-05)                           │ │
│ │                             │   (rotary_emb):                    │ │
│ │                             LlamaRotaryEmbedding()               │ │
│ │                               )                                  │ │
│ │                               (lm_head):                         │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=128256, bias=False)     │ │
│ │                             )>                                   │ │
│ │              model_inputs = {                                    │ │
│ │                             │   'cache_position': tensor([  0,   │ │
│ │                             1,   2,   3,   4,   5,   6,   7,     │ │
│ │                             8,   9,  10,  11,  12,  13,          │ │
│ │                             │   │    14,  15,  16,  17,  18,     │ │
│ │                             19,  20,  21,  22,  23,  24,  25,    │ │
│ │                             26,  27,                             │ │
│ │                             │   │    28,  29,  30,  31,  32,     │ │
│ │                             33,  34,  35,  36,  37,  38,  39,    │ │
│ │                             40,  41,                             │ │
│ │                             │   │    42,  43,  44,  45,  46,     │ │
│ │                             47,  48,  49,  50,  51,  52,  53,    │ │
│ │                             54,  55,                             │ │
│ │                             │   │    56,  57,  58,  59,  60,     │ │
│ │                             61,  62,  63,  64,  65,  66,  67,    │ │
│ │                             68,  69,                             │ │
│ │                             │   │    70,  71,  72,  73,  74,     │ │
│ │                             75,  76,  77,  78,  79,  80,  81,    │ │
│ │                             82,  83,                             │ │
│ │                             │   │    84,  85,  86,  87,  88,     │ │
│ │                             89,  90,  91,  92,  93,  94,  95,    │ │
│ │                             96,  97,                             │ │
│ │                             │   │    98,  99, 100, 101, 102,     │ │
│ │                             103, 104, 105, 106, 107, 108, 109,   │ │
│ │                             110, 111,                            │ │
│ │                             │   │   112, 113, 114, 115, 116,     │ │
│ │                             117, 118, 119, 120, 121, 122, 123,   │ │
│ │                             124, 125,                            │ │
│ │                             │   │   126, 127, 128, 129, 130,     │ │
│ │                             131, 132, 133, 134, 135, 136, 137,   │ │
│ │                             138, 139,                            │ │
│ │                             │   │   140, 141, 142, 143, 144,     │ │
│ │                             145, 146, 147, 148, 149, 150, 151,   │ │
│ │                             152, 153,                            │ │
│ │                             │   │   154, 155, 156, 157, 158,     │ │
│ │                             159, 160, 161, 162, 163, 164, 165,   │ │
│ │                             166, 167,                            │ │
│ │                             │   │   168, 169, 170, 171, 172,     │ │
│ │                             173, 174, 175, 176, 177, 178, 179,   │ │
│ │                             180, 181,                            │ │
│ │                             │   │   182, 183, 184, 185, 186,     │ │
│ │                             187, 188, 189, 190, 191, 192, 193,   │ │
│ │                             194, 195,                            │ │
│ │                             │   │   196, 197, 198, 199, 200,     │ │
│ │                             201, 202, 203, 204, 205, 206, 207,   │ │
│ │                             208, 209,                            │ │
│ │                             │   │   210, 211, 212, 213, 214,     │ │
│ │                             215, 216, 217, 218, 219, 220, 221,   │ │
│ │                             222, 223,                            │ │
│ │                             │   │   224, 225, 226, 227, 228,     │ │
│ │                             229, 230, 231, 232, 233, 234, 235,   │ │
│ │                             236, 237,                            │ │
│ │                             │   │   238, 239, 240, 241, 242,     │ │
│ │                             243, 244, 245, 246, 247, 248, 249,   │ │
│ │                             250, 251,                            │ │
│ │                             │   │   252, 253, 254, 255, 256,     │ │
│ │                             257, 258, 259, 260, 261, 262, 263,   │ │
│ │                             264, 265,                            │ │
│ │                             │   │   266, 267, 268, 269, 270,     │ │
│ │                             271, 272, 273, 274, 275, 276, 277,   │ │
│ │                             278, 279,                            │ │
│ │                             │   │   280, 281, 282, 283, 284,     │ │
│ │                             285, 286, 287, 288, 289, 290, 291,   │ │
│ │                             292, 293,                            │ │
│ │                             │   │   294, 295, 296, 297, 298,     │ │
│ │                             299, 300, 301, 302, 303, 304, 305,   │ │
│ │                             306, 307,                            │ │
│ │                             │   │   308, 309, 310, 311, 312,     │ │
│ │                             313, 314, 315, 316, 317, 318, 319,   │ │
│ │                             320, 321,                            │ │
│ │                             │   │   322, 323, 324, 325, 326,     │ │
│ │                             327, 328, 329, 330, 331, 332, 333,   │ │
│ │                             334, 335,                            │ │
│ │                             │   │   336, 337, 338, 339, 340,     │ │
│ │                             341, 342, 343, 344, 345, 346, 347,   │ │
│ │                             348, 349,                            │ │
│ │                             │   │   350, 351, 352, 353, 354,     │ │
│ │                             355, 356, 357, 358, 359, 360, 361,   │ │
│ │                             362, 363,                            │ │
│ │                             │   │   364, 365, 366, 367, 368,     │ │
│ │                             369, 370, 371, 372, 373, 374, 375,   │ │
│ │                             376, 377,                            │ │
│ │                             │   │   378, 379, 380, 381, 382,     │ │
│ │                             383, 384, 385], device='cuda:0'),    │ │
│ │                             │   'past_key_values':               │ │
│ │                             DynamicCache(),                      │ │
│ │                             │   'input_ids': tensor([[128000,    │ │
│ │                             198,   1556,   8671,     67,  27620, │ │
│ │                             57831,   5176,     17,               │ │
│ │                             │   │   │   320,  19838,  14060,     │ │
│ │                             11,    220,     18,     13,    845,  │ │
│ │                             9653,                                │ │
│ │                             │   │   │   337,      8,    323,     │ │
│ │                             473,     19,     43,     16,  14260, │ │
│ │                             21,                                  │ │
│ │                             │   │   │    13,     20,     39,     │ │
│ │                             17,     46,    320,  11057,  14060,  │ │
│ │                             11,                                  │ │
│ │                             │   │   │   220,     15,     13,     │ │
│ │                             1544,   9653,    337,    705,        │ │
│ │                             1051,  56767,                        │ │
│ │                             │   │   │   304,    264,  21655,     │ │
│ │                             315,  20804,     37,    320,   1313, │ │
│ │                             13,                                  │ │
│ │                             │   │   │    20,  65170,      8,     │ │
│ │                             323,   2206,  47861,    320,   1313, │ │
│ │                             13,                                  │ │
│ │                             │   │   │    20,  65170,      8,     │ │
│ │                             4871,    264,    220,   1135,        │ │
│ │                             65170,   9168,                       │ │
│ │                             │   │   │   348,    532,  19584,     │ │
│ │                             449,    264,    350,    830,  12490, │ │
│ │                             32393,                               │ │
│ │                             │   │     22733,   2107,     13,     │ │
│ │                             578,   6425,    574,  32813,    369, │ │
│ │                             220,                                 │ │
│ │                             │   │   │    23,   2919,    520,     │ │
│ │                             220,   2031,  37386,     34,     13, │ │
│ │                             20902,                               │ │
│ │                             │   │     65251,   1499,   3418,     │ │
│ │                             81,    599,    543,    483,  88751,  │ │
│ │                             988,                                 │ │
│ │                             │   │   │   315,   4661,   1933,     │ │
│ │                             1752,  48473,   1051,  54568,        │ │
│ │                             6041,    505,                        │ │
│ │                             │   │   │   279,   2132,   1938,     │ │
│ │                             315,  24494,     13,    578,  39887, │ │
│ │                             266,                                 │ │
│ │                             │   │   │   519,     11,   8649,     │ │
│ │                             264,   9099,   3392,    315,   2536, │ │
│ │                             48689,                               │ │
│ │                             │   │   │   599,    543,    483,     │ │
│ │                             16946,     11,    574,   1654,       │ │
│ │                             7719,    323,                        │ │
│ │                             │   │   │   279,  64568,    483,     │ │
│ │                             36841,  20227,    574,   6288,       │ │
│ │                             23217,   1139,                       │ │
│ │                             │   │   │   264,   5124,   2963,     │ │
│ │                             74,  61319,    449,    264,    282,  │ │
│ │                             1018,                                │ │
│ │                             │   │   │   369,    279,  17876,     │ │
│ │                             28786,    323,  76038,   7677,       │ │
│ │                             1234,  81073,                        │ │
│ │                             │   │      6962,     13,   4427,     │ │
│ │                             9861,   2536,  48689,    599,        │ │
│ │                             543,    483,                         │ │
│ │                             │   │     14933,    953,  19020,     │ │
│ │                             1051,  19180,    555,   9482,        │ │
│ │                             2518,    279,                        │ │
│ │                             │   │     64568,    483,  36841,     │ │
│ │                             20227,   5361,   3115,    304,       │ │
│ │                             264,    220,                         │ │
│ │                             │   │   │    16,     25,     16,     │ │
│ │                             21655,    315,  20804,     37,       │ │
│ │                             25,   7979,                          │ │
│ │                             │   │     47861,    323,  18054,     │ │
│ │                             279,  39887,    266,    519,    449, │ │
│ │                             264,                                 │ │
│ │                             │   │     58325,    324,  24547,     │ │
│ │                             6672,   8272,    555,   2033,        │ │
│ │                             97139,    287,                       │ │
│ │                             │   │   │   315,    279,  16946,     │ │
│ │                             449,    279,   1890,  69996,  21655, │ │
│ │                             323,                                 │ │
│ │                             │   │     84878,  76038,     13,     │ │
│ │                             3804,  72457,  25402,  46479,        │ │
│ │                             304,   9467,                         │ │
│ │                             │   │     24012,    320,    605,     │ │
│ │                             34363,     17,   8611,     81,       │ │
│ │                             8,    520,                           │ │
│ │                             │   │   │   436,    739,     13,     │ │
│ │                             58487,    220,  13384,  14060,       │ │
│ │                             320,   5313,                         │ │
│ │                             │   │   │     4,   3196,    389,     │ │
│ │                             473,     19,     43,     16,  14260, │ │
│ │                             21,                                  │ │
│ │                             │   │   │    13,     20,     39,     │ │
│ │                             17,     46,      8,    315,   4661,  │ │
│ │                             1933,                                │ │
│ │                             │   │      1752,  64568,    483,     │ │
│ │                             2027,     13,    720,   5207,        │ │
│ │                             1121,    304,                        │ │
│ │                             │   │   │   279,   2768,   4823,     │ │
│ │                             11036,   3645,    512,   5018,       │ │
│ │                             1337,    794,                        │ │
│ │                             │   │   │   330,   1735,    498,     │ │
│ │                             330,  13495,    794,   5324,    723, │ │
│ │                             3486,                                │ │
│ │                             │   │   │   794,   5324,   1337,     │ │
│ │                             794,    330,    928,  14682,    330, │ │
│ │                             39298,                               │ │
│ │                             │   │   │   688,    794,   5324,     │ │
│ │                             1337,    794,    330,    928,        │ │
│ │                             14682,    330,                       │ │
│ │                             │   │     35658,    794,   5324,     │ │
│ │                             1337,    794,    330,   4174,        │ │
│ │                             14682,    330,                       │ │
│ │                             │   │     35658,  15176,    794,     │ │
│ │                             5324,   1337,    794,    330,        │ │
│ │                             928,  14682,                         │ │
│ │                             │   │   │   330,   1712,    794,     │ │
│ │                             5324,   1337,    794,    330,        │ │
│ │                             4174,  14682,                        │ │
│ │                             │   │   │   330,   1712,  15176,     │ │
│ │                             794,   5324,   1337,    794,    330, │ │
│ │                             928,                                 │ │
│ │                             │   │     32075,    534,   2122,     │ │
│ │                             25,   5324,    723,   3486,    794,  │ │
│ │                             330,                                 │ │
│ │                             │   │      4155,    498,    330,     │ │
│ │                             39298,    688,    794,    330,       │ │
│ │                             8561,     37,                        │ │
│ │                             │   │   │    25,   7979,  47861,     │ │
│ │                             498,    330,  35658,    794,         │ │
│ │                             220]],                               │ │
│ │                             │      device='cuda:0'),             │ │
│ │                             │   'inputs_embeds': None,           │ │
│ │                             │   'position_ids': tensor([[  0,    │ │
│ │                             1,   2,   3,   4,   5,   6,   7,     │ │
│ │                             8,   9,  10,  11,  12,  13,          │ │
│ │                             │   │     14,  15,  16,  17,  18,    │ │
│ │                             19,  20,  21,  22,  23,  24,  25,    │ │
│ │                             26,  27,                             │ │
│ │                             │   │     28,  29,  30,  31,  32,    │ │
│ │                             33,  34,  35,  36,  37,  38,  39,    │ │
│ │                             40,  41,                             │ │
│ │                             │   │     42,  43,  44,  45,  46,    │ │
│ │                             47,  48,  49,  50,  51,  52,  53,    │ │
│ │                             54,  55,                             │ │
│ │                             │   │     56,  57,  58,  59,  60,    │ │
│ │                             61,  62,  63,  64,  65,  66,  67,    │ │
│ │                             68,  69,                             │ │
│ │                             │   │     70,  71,  72,  73,  74,    │ │
│ │                             75,  76,  77,  78,  79,  80,  81,    │ │
│ │                             82,  83,                             │ │
│ │                             │   │     84,  85,  86,  87,  88,    │ │
│ │                             89,  90,  91,  92,  93,  94,  95,    │ │
│ │                             96,  97,                             │ │
│ │                             │   │     98,  99, 100, 101, 102,    │ │
│ │                             103, 104, 105, 106, 107, 108, 109,   │ │
│ │                             110, 111,                            │ │
│ │                             │   │    112, 113, 114, 115, 116,    │ │
│ │                             117, 118, 119, 120, 121, 122, 123,   │ │
│ │                             124, 125,                            │ │
│ │                             │   │    126, 127, 128, 129, 130,    │ │
│ │                             131, 132, 133, 134, 135, 136, 137,   │ │
│ │                             138, 139,                            │ │
│ │                             │   │    140, 141, 142, 143, 144,    │ │
│ │                             145, 146, 147, 148, 149, 150, 151,   │ │
│ │                             152, 153,                            │ │
│ │                             │   │    154, 155, 156, 157, 158,    │ │
│ │                             159, 160, 161, 162, 163, 164, 165,   │ │
│ │                             166, 167,                            │ │
│ │                             │   │    168, 169, 170, 171, 172,    │ │
│ │                             173, 174, 175, 176, 177, 178, 179,   │ │
│ │                             180, 181,                            │ │
│ │                             │   │    182, 183, 184, 185, 186,    │ │
│ │                             187, 188, 189, 190, 191, 192, 193,   │ │
│ │                             194, 195,                            │ │
│ │                             │   │    196, 197, 198, 199, 200,    │ │
│ │                             201, 202, 203, 204, 205, 206, 207,   │ │
│ │                             208, 209,                            │ │
│ │                             │   │    210, 211, 212, 213, 214,    │ │
│ │                             215, 216, 217, 218, 219, 220, 221,   │ │
│ │                             222, 223,                            │ │
│ │                             │   │    224, 225, 226, 227, 228,    │ │
│ │                             229, 230, 231, 232, 233, 234, 235,   │ │
│ │                             236, 237,                            │ │
│ │                             │   │    238, 239, 240, 241, 242,    │ │
│ │                             243, 244, 245, 246, 247, 248, 249,   │ │
│ │                             250, 251,                            │ │
│ │                             │   │    252, 253, 254, 255, 256,    │ │
│ │                             257, 258, 259, 260, 261, 262, 263,   │ │
│ │                             264, 265,                            │ │
│ │                             │   │    266, 267, 268, 269, 270,    │ │
│ │                             271, 272, 273, 274, 275, 276, 277,   │ │
│ │                             278, 279,                            │ │
│ │                             │   │    280, 281, 282, 283, 284,    │ │
│ │                             285, 286, 287, 288, 289, 290, 291,   │ │
│ │                             292, 293,                            │ │
│ │                             │   │    294, 295, 296, 297, 298,    │ │
│ │                             299, 300, 301, 302, 303, 304, 305,   │ │
│ │                             306, 307,                            │ │
│ │                             │   │    308, 309, 310, 311, 312,    │ │
│ │                             313, 314, 315, 316, 317, 318, 319,   │ │
│ │                             320, 321,                            │ │
│ │                             │   │    322, 323, 324, 325, 326,    │ │
│ │                             327, 328, 329, 330, 331, 332, 333,   │ │
│ │                             334, 335,                            │ │
│ │                             │   │    336, 337, 338, 339, 340,    │ │
│ │                             341, 342, 343, 344, 345, 346, 347,   │ │
│ │                             348, 349,                            │ │
│ │                             │   │    350, 351, 352, 353, 354,    │ │
│ │                             355, 356, 357, 358, 359, 360, 361,   │ │
│ │                             362, 363,                            │ │
│ │                             │   │    364, 365, 366, 367, 368,    │ │
│ │                             369, 370, 371, 372, 373, 374, 375,   │ │
│ │                             376, 377,                            │ │
│ │                             │   │    378, 379, 380, 381, 382,    │ │
│ │                             383, 384, 385]], device='cuda:0'),   │ │
│ │                             │   'attention_mask': tensor([[1, 1, │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1]], device='cuda:0'),   │ │
│ │                             │   'num_logits_to_keep': 1,         │ │
│ │                             │   'use_cache': True                │ │
│ │                             }                                    │ │
│ │              model_kwargs = {                                    │ │
│ │                             │   'attention_mask': tensor([[1, 1, │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  │ │
│ │                             1, 1, 1,                             │ │
│ │                             │   │    1, 1, 1]],                  │ │
│ │                             device='cuda:0'),                    │ │
│ │                             │   'num_logits_to_keep': 1,         │ │
│ │                             │   'past_key_values':               │ │
│ │                             DynamicCache(),                      │ │
│ │                             │   'use_cache': True,               │ │
│ │                             │   'cache_position': tensor([386],  │ │
│ │                             device='cuda:0')                     │ │
│ │                             }                                    │ │
│ │         next_token_logits = tensor([[ 7.1914,  7.8555,  8.5234,  │ │
│ │                             ..., -2.7402, -2.7402, -2.7402]],    │ │
│ │                             │      device='cuda:0')              │ │
│ │         output_attentions = False                                │ │
│ │      output_hidden_states = False                                │ │
│ │             output_logits = None                                 │ │
│ │             output_scores = False                                │ │
│ │                   outputs = CausalLMOutputWithPast(              │ │
│ │                             │   loss=None,                       │ │
│ │                             │   logits=tensor([[[ 7.1914,        │ │
│ │                             7.8555,  8.5234,  ..., -2.7402,      │ │
│ │                             -2.7402, -2.7402]]],                 │ │
│ │                             │      device='cuda:0',              │ │
│ │                             dtype=torch.float16),                │ │
│ │                             │   past_key_values=DynamicCache(),  │ │
│ │                             │   hidden_states=None,              │ │
│ │                             │   attentions=None                  │ │
│ │                             )                                    │ │
│ │              pad_token_id = tensor(128009, device='cuda:0')      │ │
│ │                raw_logits = None                                 │ │
│ │   return_dict_in_generate = False                                │ │
│ │                    scores = None                                 │ │
│ │                      self = LlamaForCausalLM(                    │ │
│ │                               (model): LlamaModel(               │ │
│ │                             │   (embed_tokens):                  │ │
│ │                             Embedding(128256, 4096)              │ │
│ │                             │   (layers): ModuleList(            │ │
│ │                             │     (0-31): 32 x                   │ │
│ │                             LlamaDecoderLayer(                   │ │
│ │                             │   │   (self_attn):                 │ │
│ │                             LlamaFlashAttention2(                │ │
│ │                             │   │     (q_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=4096, bias=False)       │ │
│ │                             │   │     (k_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=1024, bias=False)       │ │
│ │                             │   │     (v_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=1024, bias=False)       │ │
│ │                             │   │     (o_proj):                  │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=4096, bias=False)       │ │
│ │                             │   │     (rotary_emb):              │ │
│ │                             LlamaRotaryEmbedding()               │ │
│ │                             │   │   )                            │ │
│ │                             │   │   (mlp): LlamaMLP(             │ │
│ │                             │   │     (gate_proj):               │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=14336, bias=False)      │ │
│ │                             │   │     (up_proj):                 │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=14336, bias=False)      │ │
│ │                             │   │     (down_proj):               │ │
│ │                             Linear(in_features=14336,            │ │
│ │                             out_features=4096, bias=False)       │ │
│ │                             │   │     (act_fn): SiLU()           │ │
│ │                             │   │   )                            │ │
│ │                             │   │   (input_layernorm):           │ │
│ │                             LlamaRMSNorm((4096,), eps=1e-05)     │ │
│ │                             │   │   (post_attention_layernorm):  │ │
│ │                             LlamaRMSNorm((4096,), eps=1e-05)     │ │
│ │                             │     )                              │ │
│ │                             │   )                                │ │
│ │                             │   (norm): LlamaRMSNorm((4096,),    │ │
│ │                             eps=1e-05)                           │ │
│ │                             │   (rotary_emb):                    │ │
│ │                             LlamaRotaryEmbedding()               │ │
│ │                               )                                  │ │
│ │                               (lm_head):                         │ │
│ │                             Linear(in_features=4096,             │ │
│ │                             out_features=128256, bias=False)     │ │
│ │                             )                                    │ │
│ │         stopping_criteria = [                                    │ │
│ │                             │                                    │ │
│ │                             <transformers.generation.stopping_c… │ │
│ │                             object at 0x148497876e10>,           │ │
│ │                             │                                    │ │
│ │                             <transformers.generation.stopping_c… │ │
│ │                             object at 0x14849782bdd0>,           │ │
│ │                             │                                    │ │
│ │                             <jsonformer.logits_processors.Numbe… │ │
│ │                             object at 0x148499999c10>            │ │
│ │                             ]                                    │ │
│ │                  streamer = None                                 │ │
│ │               synced_gpus = False                                │ │
│ │        this_peer_finished = False                                │ │
│ │      unfinished_sequences = tensor([1], device='cuda:0')         │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/transformers/generation/logits_process.py:104 in        │
│ __call__                                                             │
│                                                                      │
│    101 │   │   │   │   │   )                                         │
│    102 │   │   │   │   scores = processor(input_ids, scores, **kwarg │
│    103 │   │   │   else:                                             │
│ ❱  104 │   │   │   │   scores = processor(input_ids, scores)         │
│    105 │   │                                                         │
│    106 │   │   return scores                                         │
│    107                                                               │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │ function_args = mappingproxy({                                   │ │
│ │                 │   '_': <Parameter "_">,                        │ │
│ │                 │   'scores': <Parameter "scores">               │ │
│ │                 })                                               │ │
│ │     input_ids = tensor([[128000,    198,   1556,   8671,     67, │ │
│ │                 27620,  57831,   5176,     17,                   │ │
│ │                 │   │   │   320,  19838,  14060,     11,    220, │ │
│ │                 18,     13,    845,   9653,                      │ │
│ │                 │   │   │   337,      8,    323,    473,     19, │ │
│ │                 43,     16,  14260,     21,                      │ │
│ │                 │   │   │    13,     20,     39,     17,     46, │ │
│ │                 320,  11057,  14060,     11,                     │ │
│ │                 │   │   │   220,     15,     13,   1544,   9653, │ │
│ │                 337,    705,   1051,  56767,                     │ │
│ │                 │   │   │   304,    264,  21655,    315,  20804, │ │
│ │                 37,    320,   1313,     13,                      │ │
│ │                 │   │   │    20,  65170,      8,    323,   2206, │ │
│ │                 47861,    320,   1313,     13,                   │ │
│ │                 │   │   │    20,  65170,      8,   4871,    264, │ │
│ │                 220,   1135,  65170,   9168,                     │ │
│ │                 │   │   │   348,    532,  19584,    449,    264, │ │
│ │                 350,    830,  12490,  32393,                     │ │
│ │                 │   │     22733,   2107,     13,    578,   6425, │ │
│ │                 574,  32813,    369,    220,                     │ │
│ │                 │   │   │    23,   2919,    520,    220,   2031, │ │
│ │                 37386,     34,     13,  20902,                   │ │
│ │                 │   │     65251,   1499,   3418,     81,    599, │ │
│ │                 543,    483,  88751,    988,                     │ │
│ │                 │   │   │   315,   4661,   1933,   1752,  48473, │ │
│ │                 1051,  54568,   6041,    505,                    │ │
│ │                 │   │   │   279,   2132,   1938,    315,  24494, │ │
│ │                 13,    578,  39887,    266,                      │ │
│ │                 │   │   │   519,     11,   8649,    264,   9099, │ │
│ │                 3392,    315,   2536,  48689,                    │ │
│ │                 │   │   │   599,    543,    483,  16946,     11, │ │
│ │                 574,   1654,   7719,    323,                     │ │
│ │                 │   │   │   279,  64568,    483,  36841,  20227, │ │
│ │                 574,   6288,  23217,   1139,                     │ │
│ │                 │   │   │   264,   5124,   2963,     74,  61319, │ │
│ │                 449,    264,    282,   1018,                     │ │
│ │                 │   │   │   369,    279,  17876,  28786,    323, │ │
│ │                 76038,   7677,   1234,  81073,                   │ │
│ │                 │   │      6962,     13,   4427,   9861,   2536, │ │
│ │                 48689,    599,    543,    483,                   │ │
│ │                 │   │     14933,    953,  19020,   1051,  19180, │ │
│ │                 555,   9482,   2518,    279,                     │ │
│ │                 │   │     64568,    483,  36841,  20227,   5361, │ │
│ │                 3115,    304,    264,    220,                    │ │
│ │                 │   │   │    16,     25,     16,  21655,    315, │ │
│ │                 20804,     37,     25,   7979,                   │ │
│ │                 │   │     47861,    323,  18054,    279,  39887, │ │
│ │                 266,    519,    449,    264,                     │ │
│ │                 │   │     58325,    324,  24547,   6672,   8272, │ │
│ │                 555,   2033,  97139,    287,                     │ │
│ │                 │   │   │   315,    279,  16946,    449,    279, │ │
│ │                 1890,  69996,  21655,    323,                    │ │
│ │                 │   │     84878,  76038,     13,   3804,  72457, │ │
│ │                 25402,  46479,    304,   9467,                   │ │
│ │                 │   │     24012,    320,    605,  34363,     17, │ │
│ │                 8611,     81,      8,    520,                    │ │
│ │                 │   │   │   436,    739,     13,  58487,    220, │ │
│ │                 13384,  14060,    320,   5313,                   │ │
│ │                 │   │   │     4,   3196,    389,    473,     19, │ │
│ │                 43,     16,  14260,     21,                      │ │
│ │                 │   │   │    13,     20,     39,     17,     46, │ │
│ │                 8,    315,   4661,   1933,                       │ │
│ │                 │   │      1752,  64568,    483,   2027,     13, │ │
│ │                 720,   5207,   1121,    304,                     │ │
│ │                 │   │   │   279,   2768,   4823,  11036,   3645, │ │
│ │                 512,   5018,   1337,    794,                     │ │
│ │                 │   │   │   330,   1735,    498,    330,  13495, │ │
│ │                 794,   5324,    723,   3486,                     │ │
│ │                 │   │   │   794,   5324,   1337,    794,    330, │ │
│ │                 928,  14682,    330,  39298,                     │ │
│ │                 │   │   │   688,    794,   5324,   1337,    794, │ │
│ │                 330,    928,  14682,    330,                     │ │
│ │                 │   │     35658,    794,   5324,   1337,    794, │ │
│ │                 330,   4174,  14682,    330,                     │ │
│ │                 │   │     35658,  15176,    794,   5324,   1337, │ │
│ │                 794,    330,    928,  14682,                     │ │
│ │                 │   │   │   330,   1712,    794,   5324,   1337, │ │
│ │                 794,    330,   4174,  14682,                     │ │
│ │                 │   │   │   330,   1712,  15176,    794,   5324, │ │
│ │                 1337,    794,    330,    928,                    │ │
│ │                 │   │     32075,    534,   2122,     25,   5324, │ │
│ │                 723,   3486,    794,    330,                     │ │
│ │                 │   │      4155,    498,    330,  39298,    688, │ │
│ │                 794,    330,   8561,     37,                     │ │
│ │                 │   │   │    25,   7979,  47861,    498,    330, │ │
│ │                 35658,    794,    220]],                         │ │
│ │                 │      device='cuda:0')                          │ │
│ │        kwargs = {}                                               │ │
│ │     processor = <jsonformer.logits_processors.OutputNumbersToke… │ │
│ │                 object at 0x14849782a690>                        │ │
│ │        scores = tensor([[ 7.1914,  7.8555,  8.5234,  ...,        │ │
│ │                 -2.7402, -2.7402, -2.7402]],                     │ │
│ │                 │      device='cuda:0')                          │ │
│ │          self = [                                                │ │
│ │                 │                                                │ │
│ │                 <jsonformer.logits_processors.OutputNumbersToke… │ │
│ │                 object at 0x14849782a690>,                       │ │
│ │                 │                                                │ │
│ │                 <transformers.generation.logits_process.Tempera… │ │
│ │                 object at 0x1484977c8f10>,                       │ │
│ │                 │                                                │ │
│ │                 <transformers.generation.logits_process.TopKLog… │ │
│ │                 object at 0x148497807bd0>,                       │ │
│ │                 │                                                │ │
│ │                 <transformers.generation.logits_process.TopPLog… │ │
│ │                 object at 0x148497876b90>                        │ │
│ │                 ]                                                │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
│                                                                      │
│ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s │
│ ite-packages/jsonformer/logits_processors.py:81 in __call__          │
│                                                                      │
│   78 │   │   │   │   self.allowed_mask[token_id] = True              │
│   79 │                                                               │
│   80 │   def __call__(self, _, scores):                              │
│ ❱ 81 │   │   mask = self.allowed_mask.expand_as(scores)              │
│   82 │   │   scores[~mask] = -float("inf")                           │
│   83 │   │                                                           │
│   84 │   │   return scores                                           │
│                                                                      │
│ ╭───────────────────────────── locals ─────────────────────────────╮ │
│ │      _ = tensor([[128000,    198,   1556,   8671,     67,        │ │
│ │          27620,  57831,   5176,     17,                          │ │
│ │          │   │   │   320,  19838,  14060,     11,    220,        │ │
│ │          18,     13,    845,   9653,                             │ │
│ │          │   │   │   337,      8,    323,    473,     19,        │ │
│ │          43,     16,  14260,     21,                             │ │
│ │          │   │   │    13,     20,     39,     17,     46,        │ │
│ │          320,  11057,  14060,     11,                            │ │
│ │          │   │   │   220,     15,     13,   1544,   9653,        │ │
│ │          337,    705,   1051,  56767,                            │ │
│ │          │   │   │   304,    264,  21655,    315,  20804,        │ │
│ │          37,    320,   1313,     13,                             │ │
│ │          │   │   │    20,  65170,      8,    323,   2206,        │ │
│ │          47861,    320,   1313,     13,                          │ │
│ │          │   │   │    20,  65170,      8,   4871,    264,        │ │
│ │          220,   1135,  65170,   9168,                            │ │
│ │          │   │   │   348,    532,  19584,    449,    264,        │ │
│ │          350,    830,  12490,  32393,                            │ │
│ │          │   │     22733,   2107,     13,    578,   6425,        │ │
│ │          574,  32813,    369,    220,                            │ │
│ │          │   │   │    23,   2919,    520,    220,   2031,        │ │
│ │          37386,     34,     13,  20902,                          │ │
│ │          │   │     65251,   1499,   3418,     81,    599,        │ │
│ │          543,    483,  88751,    988,                            │ │
│ │          │   │   │   315,   4661,   1933,   1752,  48473,        │ │
│ │          1051,  54568,   6041,    505,                           │ │
│ │          │   │   │   279,   2132,   1938,    315,  24494,        │ │
│ │          13,    578,  39887,    266,                             │ │
│ │          │   │   │   519,     11,   8649,    264,   9099,        │ │
│ │          3392,    315,   2536,  48689,                           │ │
│ │          │   │   │   599,    543,    483,  16946,     11,        │ │
│ │          574,   1654,   7719,    323,                            │ │
│ │          │   │   │   279,  64568,    483,  36841,  20227,        │ │
│ │          574,   6288,  23217,   1139,                            │ │
│ │          │   │   │   264,   5124,   2963,     74,  61319,        │ │
│ │          449,    264,    282,   1018,                            │ │
│ │          │   │   │   369,    279,  17876,  28786,    323,        │ │
│ │          76038,   7677,   1234,  81073,                          │ │
│ │          │   │      6962,     13,   4427,   9861,   2536,        │ │
│ │          48689,    599,    543,    483,                          │ │
│ │          │   │     14933,    953,  19020,   1051,  19180,        │ │
│ │          555,   9482,   2518,    279,                            │ │
│ │          │   │     64568,    483,  36841,  20227,   5361,        │ │
│ │          3115,    304,    264,    220,                           │ │
│ │          │   │   │    16,     25,     16,  21655,    315,        │ │
│ │          20804,     37,     25,   7979,                          │ │
│ │          │   │     47861,    323,  18054,    279,  39887,        │ │
│ │          266,    519,    449,    264,                            │ │
│ │          │   │     58325,    324,  24547,   6672,   8272,        │ │
│ │          555,   2033,  97139,    287,                            │ │
│ │          │   │   │   315,    279,  16946,    449,    279,        │ │
│ │          1890,  69996,  21655,    323,                           │ │
│ │          │   │     84878,  76038,     13,   3804,  72457,        │ │
│ │          25402,  46479,    304,   9467,                          │ │
│ │          │   │     24012,    320,    605,  34363,     17,        │ │
│ │          8611,     81,      8,    520,                           │ │
│ │          │   │   │   436,    739,     13,  58487,    220,        │ │
│ │          13384,  14060,    320,   5313,                          │ │
│ │          │   │   │     4,   3196,    389,    473,     19,        │ │
│ │          43,     16,  14260,     21,                             │ │
│ │          │   │   │    13,     20,     39,     17,     46,        │ │
│ │          8,    315,   4661,   1933,                              │ │
│ │          │   │      1752,  64568,    483,   2027,     13,        │ │
│ │          720,   5207,   1121,    304,                            │ │
│ │          │   │   │   279,   2768,   4823,  11036,   3645,        │ │
│ │          512,   5018,   1337,    794,                            │ │
│ │          │   │   │   330,   1735,    498,    330,  13495,        │ │
│ │          794,   5324,    723,   3486,                            │ │
│ │          │   │   │   794,   5324,   1337,    794,    330,        │ │
│ │          928,  14682,    330,  39298,                            │ │
│ │          │   │   │   688,    794,   5324,   1337,    794,        │ │
│ │          330,    928,  14682,    330,                            │ │
│ │          │   │     35658,    794,   5324,   1337,    794,        │ │
│ │          330,   4174,  14682,    330,                            │ │
│ │          │   │     35658,  15176,    794,   5324,   1337,        │ │
│ │          794,    330,    928,  14682,                            │ │
│ │          │   │   │   330,   1712,    794,   5324,   1337,        │ │
│ │          794,    330,   4174,  14682,                            │ │
│ │          │   │   │   330,   1712,  15176,    794,   5324,        │ │
│ │          1337,    794,    330,    928,                           │ │
│ │          │   │     32075,    534,   2122,     25,   5324,        │ │
│ │          723,   3486,    794,    330,                            │ │
│ │          │   │      4155,    498,    330,  39298,    688,        │ │
│ │          794,    330,   8561,     37,                            │ │
│ │          │   │   │    25,   7979,  47861,    498,    330,        │ │
│ │          35658,    794,    220]],                                │ │
│ │          │      device='cuda:0')                                 │ │
│ │ scores = tensor([[ 7.1914,  7.8555,  8.5234,  ..., -2.7402,      │ │
│ │          -2.7402, -2.7402]],                                     │ │
│ │          │      device='cuda:0')                                 │ │
│ │   self = <jsonformer.logits_processors.OutputNumbersTokens       │ │
│ │          object at 0x14849782a690>                               │ │
│ ╰──────────────────────────────────────────────────────────────────╯ │
╰──────────────────────────────────────────────────────────────────────╯
RuntimeError: The expanded size of the tensor (128256) must match the 
existing size (128309) at non-singleton dimension 1.  Target sizes: [1, 
128256].  Tensor sizes: [128309]
======================128309=======================================
