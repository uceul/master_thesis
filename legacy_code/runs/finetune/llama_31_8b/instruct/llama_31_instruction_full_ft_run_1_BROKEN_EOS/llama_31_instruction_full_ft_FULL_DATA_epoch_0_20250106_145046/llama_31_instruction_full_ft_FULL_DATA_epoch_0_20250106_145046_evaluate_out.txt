2025-01-06 14:50:49,955 INFO     Loading settings and stats
2025-01-06 14:50:49,956 INFO     Using prompt: , temperature: 0.1
2025-01-06 14:50:49,959 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-06 14:50:49,959 WARNING  [Errno 2] No such file or directory: 'stats_llama_31_instruction_full_ft_FULL_DATA_epoch_0_20250106_145046.yml'
2025-01-06 14:50:49,959 WARNING  [Errno 2] No such file or directory: 'stats_llama_31_instruction_full_ft_FULL_DATA_epoch_0_20250106_145046.yaml'
2025-01-06 14:50:49,959 WARNING  Could not load 'stats_llama_31_instruction_full_ft_FULL_DATA_epoch_0_20250106_145046.yml', creating it (empty)
2025-01-06 14:50:49,961 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-06 14:50:49,967 INFO     Found 778 paragraphs with labels
2025-01-06 14:50:49,967 DEBUG    First few valid IDs: ['DACSAE_clean', 'FECYOD_clean', 'NAHHOW_clean', 'NEVJOP_clean', 'QOZPIG_clean']
2025-01-06 14:50:49,967 DEBUG    Already evaluated: 0 items
2025-01-06 14:50:49,967 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/synthesis_paragraphs
2025-01-06 14:50:49,967 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/synthesis_paragraphs'
2025-01-06 14:50:50,010 INFO     Dataset loaded with 905 items
2025-01-06 14:50:50,010 INFO     Found 778 paragraphs that have labels in dataset
2025-01-06 14:50:50,010 DEBUG    First few overlapping IDs: ['DACSAE_clean', 'FECYOD_clean', 'NAHHOW_clean', 'NEVJOP_clean', 'QOZPIG_clean']
2025-01-06 14:50:50,010 INFO     Processing model: LLaMa 3.1 8B
2025-01-06 14:50:50,010 INFO     Skipping model [LLaMa 3.1 8B]
2025-01-06 14:50:50,011 INFO     Processing model: LLaMa 3.1 8B Instruct
2025-01-06 14:50:50,012 INFO       0%|          | 0/905 [00:00<?, ?it/s, LLaMa 3.1 8B Instruct]
2025-01-06 14:50:50,012 INFO     Loading Model [LLaMa 3.1 8B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama31_8b/instruction_full_ft/run_1_1736114112/epoch_0
2025-01-06 14:50:50,665 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
  0%|          | 0/905 [00:00<?, ?it/s, LLaMa 3.1 8B Instruct]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.03s/it][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.04s/it][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:12<00:04,  4.02s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  2.83s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.27s/it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Traceback (most recent call last) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/master_thesis/legacy_c â”‚
â”‚ ode/src/mthesis/main.py:294 in evaluate                              â”‚
â”‚                                                                      â”‚
â”‚   291 â”‚   â”‚   â”‚   â”‚   "model_name": model_name,                      â”‚
â”‚   292 â”‚   â”‚   â”‚   }                                                  â”‚
â”‚   293 â”‚   â”‚   â”‚                                                      â”‚
â”‚ â± 294 â”‚   â”‚   â”‚   entry["answer"] = model(batch["text"])  # forward  â”‚
â”‚   295 â”‚   â”‚   â”‚   stats.append(entry)                                â”‚
â”‚   296 â”‚   â”‚   â”‚   if count >= 20:                                    â”‚
â”‚   297 â”‚   â”‚   â”‚   â”‚   try:                                           â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚               batch = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'paragraph_id': 'LELROL_clean',        â”‚ â”‚
â”‚ â”‚                       â”‚   'text': ' Anhydrous MnCl2 (398 mg,     â”‚ â”‚
â”‚ â”‚                       3.16 mmol) and H4L1Â·6.5H2O (225 mg, 0.27   â”‚ â”‚
â”‚ â”‚                       mmol), were d'+973                         â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â”‚               count = 1                                          â”‚ â”‚
â”‚ â”‚             dataset = <mthesis.dataloader.MOFDataset object at   â”‚ â”‚
â”‚ â”‚                       0x148499b18290>                            â”‚ â”‚
â”‚ â”‚         dataset_ids = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'DACSAE_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'FECYOD_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'NAHHOW_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'NEVJOP_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'JUTCIM_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'QOZPIG_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'KUZZAI_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'SUKMAO_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'FEJDEE_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'LACJAC_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   ... +895                               â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â”‚        dataset_path = '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29â€¦ â”‚ â”‚
â”‚ â”‚         description = ''                                         â”‚ â”‚
â”‚ â”‚              device = device(type='cuda')                        â”‚ â”‚
â”‚ â”‚                diff = 0                                          â”‚ â”‚
â”‚ â”‚               entry = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'paragraph_id': 'LELROL_clean',        â”‚ â”‚
â”‚ â”‚                       â”‚   'model_name': 'LLaMa 3.1 8B Instruct'  â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â”‚           evaluated = frozenset()                                â”‚ â”‚
â”‚ â”‚      evaluation_set = False                                      â”‚ â”‚
â”‚ â”‚               first = False                                      â”‚ â”‚
â”‚ â”‚           labels_df = â”‚    Unnamed: 0      filename DISORDER     â”‚ â”‚
â”‚ â”‚                       ... other6 other7  other8                  â”‚ â”‚
â”‚ â”‚                       0             0  OFODET_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       1             1  XAVKIR_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       2             3  LATPIG_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       3             4  MOYYIJ_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       4             5  OFOCUI_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       ..          ...           ...      ...     â”‚ â”‚
â”‚ â”‚                       ...    ...    ...     ...                  â”‚ â”‚
â”‚ â”‚                       773         963  LEVDIB_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       774         964  LIKDOA_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       775         967  MUNDAC_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       776         968  MUNDOQ_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                       777         970  OYIVIC_clean      NaN     â”‚ â”‚
â”‚ â”‚                       ...    NaN    NaN     NaN                  â”‚ â”‚
â”‚ â”‚                                                                  â”‚ â”‚
â”‚ â”‚                       [778 rows x 36 columns]                    â”‚ â”‚
â”‚ â”‚             log_dir = 'logs/llama_31_instruction_full_ft_FULL_Dâ€¦ â”‚ â”‚
â”‚ â”‚               model = JsonformerModel(                           â”‚ â”‚
â”‚ â”‚                         (model): LlamaForCausalLM(               â”‚ â”‚
â”‚ â”‚                       â”‚   (model): LlamaModel(                   â”‚ â”‚
â”‚ â”‚                       â”‚     (embed_tokens): Embedding(128256,    â”‚ â”‚
â”‚ â”‚                       4096)                                      â”‚ â”‚
â”‚ â”‚                       â”‚     (layers): ModuleList(                â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   (0-31): 32 x LlamaDecoderLayer(    â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚     (self_attn):                     â”‚ â”‚
â”‚ â”‚                       LlamaFlashAttention2(                      â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (q_proj):                      â”‚ â”‚
â”‚ â”‚                       Linear(in_features=4096,                   â”‚ â”‚
â”‚ â”‚                       out_features=4096, bias=False)             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (k_proj):                      â”‚ â”‚
â”‚ â”‚                       Linear(in_features=4096,                   â”‚ â”‚
â”‚ â”‚                       out_features=1024, bias=False)             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (v_proj):                      â”‚ â”‚
â”‚ â”‚                       Linear(in_features=4096,                   â”‚ â”‚
â”‚ â”‚                       out_features=1024, bias=False)             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (o_proj):                      â”‚ â”‚
â”‚ â”‚                       Linear(in_features=4096,                   â”‚ â”‚
â”‚ â”‚                       out_features=4096, bias=False)             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (rotary_emb):                  â”‚ â”‚
â”‚ â”‚                       LlamaRotaryEmbedding()                     â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚     )                                â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚     (mlp): LlamaMLP(                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (gate_proj):                   â”‚ â”‚
â”‚ â”‚                       Linear(in_features=4096,                   â”‚ â”‚
â”‚ â”‚                       out_features=14336, bias=False)            â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (up_proj):                     â”‚ â”‚
â”‚ â”‚                       Linear(in_features=4096,                   â”‚ â”‚
â”‚ â”‚                       out_features=14336, bias=False)            â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (down_proj):                   â”‚ â”‚
â”‚ â”‚                       Linear(in_features=14336,                  â”‚ â”‚
â”‚ â”‚                       out_features=4096, bias=False)             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   (act_fn): SiLU()               â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚     )                                â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚     (input_layernorm):               â”‚ â”‚
â”‚ â”‚                       LlamaRMSNorm((4096,), eps=1e-05)           â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚     (post_attention_layernorm):      â”‚ â”‚
â”‚ â”‚                       LlamaRMSNorm((4096,), eps=1e-05)           â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   )                                  â”‚ â”‚
â”‚ â”‚                       â”‚     )                                    â”‚ â”‚
â”‚ â”‚                       â”‚     (norm): LlamaRMSNorm((4096,),        â”‚ â”‚
â”‚ â”‚                       eps=1e-05)                                 â”‚ â”‚
â”‚ â”‚                       â”‚     (rotary_emb): LlamaRotaryEmbedding() â”‚ â”‚
â”‚ â”‚                       â”‚   )                                      â”‚ â”‚
â”‚ â”‚                       â”‚   (lm_head): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                       out_features=128256, bias=False)           â”‚ â”‚
â”‚ â”‚                         )                                        â”‚ â”‚
â”‚ â”‚                       )                                          â”‚ â”‚
â”‚ â”‚          model_name = 'LLaMa 3.1 8B Instruct'                    â”‚ â”‚
â”‚ â”‚          model_path = '/home/iti/zn2950/home/haicore_ws/tunes/lâ€¦ â”‚ â”‚
â”‚ â”‚      model_settings = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'model_name': 'LLaMa 3.1 8B Instruct', â”‚ â”‚
â”‚ â”‚                       â”‚   'model_path':                          â”‚ â”‚
â”‚ â”‚                       '/home/iti/zn2950/home/haicore_ws/tunes/lâ€¦ â”‚ â”‚
â”‚ â”‚                       â”‚   'model_type': 'text-generation'        â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â”‚          only_model = 'LLaMa 3.1 8B Instruct'                    â”‚ â”‚
â”‚ â”‚             overlap = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'DACSAE_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'FECYOD_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'NAHHOW_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'NEVJOP_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'QOZPIG_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'JUTCIM_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'SUKMAO_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'KUZZAI_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'LACJAC_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'RUGKOV_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   ... +768                               â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â”‚        paragraph_id = 'LELROL_clean'                             â”‚ â”‚
â”‚ â”‚        progress_bar = <tqdm.std.tqdm object at 0x148497dd2e10>   â”‚ â”‚
â”‚ â”‚              prompt = ''                                         â”‚ â”‚
â”‚ â”‚            settings = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'dataset_path':                        â”‚ â”‚
â”‚ â”‚                       '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29â€¦ â”‚ â”‚
â”‚ â”‚                       â”‚   'eval_dataset_path':                   â”‚ â”‚
â”‚ â”‚                       '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29â€¦ â”‚ â”‚
â”‚ â”‚                       â”‚   'csv_path':                            â”‚ â”‚
â”‚ â”‚                       '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn29â€¦ â”‚ â”‚
â”‚ â”‚                       â”‚   'models': [                            â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   {                                  â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'model_name': 'LLaMa 3.1 8B',  â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'model_path':                  â”‚ â”‚
â”‚ â”‚                       '/home/iti/zn2950/home/haicore_ws/tunes/lâ€¦ â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'model_type':                  â”‚ â”‚
â”‚ â”‚                       'text-generation'                          â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   },                                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   {                                  â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'model_name': 'LLaMa 3.1 8B    â”‚ â”‚
â”‚ â”‚                       Instruct',                                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'model_path':                  â”‚ â”‚
â”‚ â”‚                       '/home/iti/zn2950/home/haicore_ws/tunes/lâ€¦ â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'model_type':                  â”‚ â”‚
â”‚ â”‚                       'text-generation'                          â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   }                                  â”‚ â”‚
â”‚ â”‚                       â”‚   ],                                     â”‚ â”‚
â”‚ â”‚                       â”‚   'extract_config': {                    â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   'additive': {                      â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'type': 'string',              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'convert_funcs': ['ans2cid'],  â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'dataset_cols': [              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   â”‚   'additive1'                â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   ]                              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   },                                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   'solvent': {                       â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'type': 'string',              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'convert_funcs': ['ans2cid'],  â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'dataset_cols': ['solvent1']   â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   },                                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   'temperature': {                   â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'type': 'number',              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'unit': 'C',                   â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'convert_funcs': [             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   â”‚   'ans2temperature'          â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   ],                             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'dataset_cols': [              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   â”‚   'temperature_Celsius'      â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   ]                              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   },                                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   'time': {                          â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'type': 'number',              â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'unit': 'h',                   â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'convert_funcs': [             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   â”‚   'ans2time'                 â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   ],                             â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   â”‚   'dataset_cols': ['time_h']     â”‚ â”‚
â”‚ â”‚                       â”‚   â”‚   }                                  â”‚ â”‚
â”‚ â”‚                       â”‚   }                                      â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â”‚               stats = []                                         â”‚ â”‚
â”‚ â”‚          stats_path = 'stats_llama_31_instruction_full_ft_FULL_â€¦ â”‚ â”‚
â”‚ â”‚         temperature = 0.1                                        â”‚ â”‚
â”‚ â”‚ valid_paragraph_ids = {                                          â”‚ â”‚
â”‚ â”‚                       â”‚   'DACSAE_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'FECYOD_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'NAHHOW_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'NEVJOP_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'QOZPIG_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'JUTCIM_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'SUKMAO_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'KUZZAI_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'LACJAC_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   'RUGKOV_clean',                        â”‚ â”‚
â”‚ â”‚                       â”‚   ... +768                               â”‚ â”‚
â”‚ â”‚                       }                                          â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/torch/nn/modules/module.py:1553 in _wrapped_call_impl   â”‚
â”‚                                                                      â”‚
â”‚   1550 â”‚   â”‚   if self._compiled_call_impl is not None:              â”‚
â”‚   1551 â”‚   â”‚   â”‚   return self._compiled_call_impl(*args, **kwargs)  â”‚
â”‚   1552 â”‚   â”‚   else:                                                 â”‚
â”‚ â± 1553 â”‚   â”‚   â”‚   return self._call_impl(*args, **kwargs)           â”‚
â”‚   1554 â”‚                                                             â”‚
â”‚   1555 â”‚   def _call_impl(self, *args, **kwargs):                    â”‚
â”‚   1556 â”‚   â”‚   forward_call = (self._slow_forward if torch._C._get_t â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚   args = (                                                       â”‚ â”‚
â”‚ â”‚          â”‚   ' Anhydrous MnCl2 (398 mg, 3.16 mmol) and           â”‚ â”‚
â”‚ â”‚          H4L1Â·6.5H2O (225 mg, 0.27 mmol), were d'+973,           â”‚ â”‚
â”‚ â”‚          )                                                       â”‚ â”‚
â”‚ â”‚ kwargs = {}                                                      â”‚ â”‚
â”‚ â”‚   self = JsonformerModel(                                        â”‚ â”‚
â”‚ â”‚            (model): LlamaForCausalLM(                            â”‚ â”‚
â”‚ â”‚          â”‚   (model): LlamaModel(                                â”‚ â”‚
â”‚ â”‚          â”‚     (embed_tokens): Embedding(128256, 4096)           â”‚ â”‚
â”‚ â”‚          â”‚     (layers): ModuleList(                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   (0-31): 32 x LlamaDecoderLayer(                 â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     (self_attn): LlamaFlashAttention2(            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (q_proj): Linear(in_features=4096,          â”‚ â”‚
â”‚ â”‚          out_features=4096, bias=False)                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (k_proj): Linear(in_features=4096,          â”‚ â”‚
â”‚ â”‚          out_features=1024, bias=False)                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (v_proj): Linear(in_features=4096,          â”‚ â”‚
â”‚ â”‚          out_features=1024, bias=False)                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (o_proj): Linear(in_features=4096,          â”‚ â”‚
â”‚ â”‚          out_features=4096, bias=False)                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (rotary_emb): LlamaRotaryEmbedding()        â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     )                                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     (mlp): LlamaMLP(                              â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (gate_proj): Linear(in_features=4096,       â”‚ â”‚
â”‚ â”‚          out_features=14336, bias=False)                         â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (up_proj): Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚          out_features=14336, bias=False)                         â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (down_proj): Linear(in_features=14336,      â”‚ â”‚
â”‚ â”‚          out_features=4096, bias=False)                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   (act_fn): SiLU()                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     )                                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     (input_layernorm): LlamaRMSNorm((4096,),      â”‚ â”‚
â”‚ â”‚          eps=1e-05)                                              â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     (post_attention_layernorm):                   â”‚ â”‚
â”‚ â”‚          LlamaRMSNorm((4096,), eps=1e-05)                        â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   )                                               â”‚ â”‚
â”‚ â”‚          â”‚     )                                                 â”‚ â”‚
â”‚ â”‚          â”‚     (norm): LlamaRMSNorm((4096,), eps=1e-05)          â”‚ â”‚
â”‚ â”‚          â”‚     (rotary_emb): LlamaRotaryEmbedding()              â”‚ â”‚
â”‚ â”‚          â”‚   )                                                   â”‚ â”‚
â”‚ â”‚          â”‚   (lm_head): Linear(in_features=4096,                 â”‚ â”‚
â”‚ â”‚          out_features=128256, bias=False)                        â”‚ â”‚
â”‚ â”‚            )                                                     â”‚ â”‚
â”‚ â”‚          )                                                       â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/torch/nn/modules/module.py:1562 in _call_impl           â”‚
â”‚                                                                      â”‚
â”‚   1559 â”‚   â”‚   if not (self._backward_hooks or self._backward_pre_ho â”‚
â”‚   1560 â”‚   â”‚   â”‚   â”‚   or _global_backward_pre_hooks or _global_back â”‚
â”‚   1561 â”‚   â”‚   â”‚   â”‚   or _global_forward_hooks or _global_forward_p â”‚
â”‚ â± 1562 â”‚   â”‚   â”‚   return forward_call(*args, **kwargs)              â”‚
â”‚   1563 â”‚   â”‚                                                         â”‚
â”‚   1564 â”‚   â”‚   try:                                                  â”‚
â”‚   1565 â”‚   â”‚   â”‚   result = None                                     â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚         args = (                                                 â”‚ â”‚
â”‚ â”‚                â”‚   ' Anhydrous MnCl2 (398 mg, 3.16 mmol) and     â”‚ â”‚
â”‚ â”‚                H4L1Â·6.5H2O (225 mg, 0.27 mmol), were d'+973,     â”‚ â”‚
â”‚ â”‚                )                                                 â”‚ â”‚
â”‚ â”‚ forward_call = <bound method JsonformerModel.forward of          â”‚ â”‚
â”‚ â”‚                JsonformerModel(                                  â”‚ â”‚
â”‚ â”‚                  (model): LlamaForCausalLM(                      â”‚ â”‚
â”‚ â”‚                â”‚   (model): LlamaModel(                          â”‚ â”‚
â”‚ â”‚                â”‚     (embed_tokens): Embedding(128256, 4096)     â”‚ â”‚
â”‚ â”‚                â”‚     (layers): ModuleList(                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   (0-31): 32 x LlamaDecoderLayer(           â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (self_attn): LlamaFlashAttention2(      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (q_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=4096, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (k_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=1024, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (v_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=1024, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (o_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=4096, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (rotary_emb): LlamaRotaryEmbedding()  â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     )                                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (mlp): LlamaMLP(                        â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (gate_proj): Linear(in_features=4096, â”‚ â”‚
â”‚ â”‚                out_features=14336, bias=False)                   â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (up_proj): Linear(in_features=4096,   â”‚ â”‚
â”‚ â”‚                out_features=14336, bias=False)                   â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (down_proj):                          â”‚ â”‚
â”‚ â”‚                Linear(in_features=14336, out_features=4096,      â”‚ â”‚
â”‚ â”‚                bias=False)                                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (act_fn): SiLU()                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     )                                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (input_layernorm):                      â”‚ â”‚
â”‚ â”‚                LlamaRMSNorm((4096,), eps=1e-05)                  â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (post_attention_layernorm):             â”‚ â”‚
â”‚ â”‚                LlamaRMSNorm((4096,), eps=1e-05)                  â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   )                                         â”‚ â”‚
â”‚ â”‚                â”‚     )                                           â”‚ â”‚
â”‚ â”‚                â”‚     (norm): LlamaRMSNorm((4096,), eps=1e-05)    â”‚ â”‚
â”‚ â”‚                â”‚     (rotary_emb): LlamaRotaryEmbedding()        â”‚ â”‚
â”‚ â”‚                â”‚   )                                             â”‚ â”‚
â”‚ â”‚                â”‚   (lm_head): Linear(in_features=4096,           â”‚ â”‚
â”‚ â”‚                out_features=128256, bias=False)                  â”‚ â”‚
â”‚ â”‚                  )                                               â”‚ â”‚
â”‚ â”‚                )>                                                â”‚ â”‚
â”‚ â”‚       kwargs = {}                                                â”‚ â”‚
â”‚ â”‚         self = JsonformerModel(                                  â”‚ â”‚
â”‚ â”‚                  (model): LlamaForCausalLM(                      â”‚ â”‚
â”‚ â”‚                â”‚   (model): LlamaModel(                          â”‚ â”‚
â”‚ â”‚                â”‚     (embed_tokens): Embedding(128256, 4096)     â”‚ â”‚
â”‚ â”‚                â”‚     (layers): ModuleList(                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   (0-31): 32 x LlamaDecoderLayer(           â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (self_attn): LlamaFlashAttention2(      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (q_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=4096, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (k_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=1024, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (v_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=1024, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (o_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚                out_features=4096, bias=False)                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (rotary_emb): LlamaRotaryEmbedding()  â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     )                                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (mlp): LlamaMLP(                        â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (gate_proj): Linear(in_features=4096, â”‚ â”‚
â”‚ â”‚                out_features=14336, bias=False)                   â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (up_proj): Linear(in_features=4096,   â”‚ â”‚
â”‚ â”‚                out_features=14336, bias=False)                   â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (down_proj):                          â”‚ â”‚
â”‚ â”‚                Linear(in_features=14336, out_features=4096,      â”‚ â”‚
â”‚ â”‚                bias=False)                                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   (act_fn): SiLU()                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     )                                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (input_layernorm):                      â”‚ â”‚
â”‚ â”‚                LlamaRMSNorm((4096,), eps=1e-05)                  â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     (post_attention_layernorm):             â”‚ â”‚
â”‚ â”‚                LlamaRMSNorm((4096,), eps=1e-05)                  â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   )                                         â”‚ â”‚
â”‚ â”‚                â”‚     )                                           â”‚ â”‚
â”‚ â”‚                â”‚     (norm): LlamaRMSNorm((4096,), eps=1e-05)    â”‚ â”‚
â”‚ â”‚                â”‚     (rotary_emb): LlamaRotaryEmbedding()        â”‚ â”‚
â”‚ â”‚                â”‚   )                                             â”‚ â”‚
â”‚ â”‚                â”‚   (lm_head): Linear(in_features=4096,           â”‚ â”‚
â”‚ â”‚                out_features=128256, bias=False)                  â”‚ â”‚
â”‚ â”‚                  )                                               â”‚ â”‚
â”‚ â”‚                )                                                 â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/master_thesis/legacy_c â”‚
â”‚ ode/src/mthesis/models.py:88 in forward                              â”‚
â”‚                                                                      â”‚
â”‚    85 â”‚   â”‚   â”‚   â”‚   jsonformer = Jsonformer(self.model, self.token â”‚
â”‚    86 â”‚   â”‚   â”‚   else:                                              â”‚
â”‚    87 â”‚   â”‚   â”‚   â”‚   jsonformer = Jsonformer(self.model, self.token â”‚
â”‚ â±  88 â”‚   â”‚   return jsonformer()                                    â”‚
â”‚    89 â”‚                                                              â”‚
â”‚    90 â”‚   def forward2(self, text: str):                             â”‚
â”‚    91 â”‚   â”‚   input_tokens = self.tokenizer.encode(text, return_tens â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚ full_prompt = '\n Anhydrous MnCl2 (398 mg, 3.16 mmol) and        â”‚ â”‚
â”‚ â”‚               H4L1Â·6.5H2O (225 mg, 0.27 mmol), were '+974        â”‚ â”‚
â”‚ â”‚  jsonformer = <jsonformer.main.Jsonformer object at              â”‚ â”‚
â”‚ â”‚               0x14849782a750>                                    â”‚ â”‚
â”‚ â”‚        self = JsonformerModel(                                   â”‚ â”‚
â”‚ â”‚                 (model): LlamaForCausalLM(                       â”‚ â”‚
â”‚ â”‚               â”‚   (model): LlamaModel(                           â”‚ â”‚
â”‚ â”‚               â”‚     (embed_tokens): Embedding(128256, 4096)      â”‚ â”‚
â”‚ â”‚               â”‚     (layers): ModuleList(                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   (0-31): 32 x LlamaDecoderLayer(            â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (self_attn): LlamaFlashAttention2(       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (q_proj): Linear(in_features=4096,     â”‚ â”‚
â”‚ â”‚               out_features=4096, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (k_proj): Linear(in_features=4096,     â”‚ â”‚
â”‚ â”‚               out_features=1024, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (v_proj): Linear(in_features=4096,     â”‚ â”‚
â”‚ â”‚               out_features=1024, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (o_proj): Linear(in_features=4096,     â”‚ â”‚
â”‚ â”‚               out_features=4096, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (rotary_emb): LlamaRotaryEmbedding()   â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     )                                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (mlp): LlamaMLP(                         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (gate_proj): Linear(in_features=4096,  â”‚ â”‚
â”‚ â”‚               out_features=14336, bias=False)                    â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (up_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚               out_features=14336, bias=False)                    â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (down_proj): Linear(in_features=14336, â”‚ â”‚
â”‚ â”‚               out_features=4096, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   (act_fn): SiLU()                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     )                                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (input_layernorm): LlamaRMSNorm((4096,), â”‚ â”‚
â”‚ â”‚               eps=1e-05)                                         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (post_attention_layernorm):              â”‚ â”‚
â”‚ â”‚               LlamaRMSNorm((4096,), eps=1e-05)                   â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   )                                          â”‚ â”‚
â”‚ â”‚               â”‚     )                                            â”‚ â”‚
â”‚ â”‚               â”‚     (norm): LlamaRMSNorm((4096,), eps=1e-05)     â”‚ â”‚
â”‚ â”‚               â”‚     (rotary_emb): LlamaRotaryEmbedding()         â”‚ â”‚
â”‚ â”‚               â”‚   )                                              â”‚ â”‚
â”‚ â”‚               â”‚   (lm_head): Linear(in_features=4096,            â”‚ â”‚
â”‚ â”‚               out_features=128256, bias=False)                   â”‚ â”‚
â”‚ â”‚                 )                                                â”‚ â”‚
â”‚ â”‚               )                                                  â”‚ â”‚
â”‚ â”‚        text = ' Anhydrous MnCl2 (398 mg, 3.16 mmol) and          â”‚ â”‚
â”‚ â”‚               H4L1Â·6.5H2O (225 mg, 0.27 mmol), were d'+973       â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/jsonformer/main.py:242 in __call__                      â”‚
â”‚                                                                      â”‚
â”‚   239 â”‚                                                              â”‚
â”‚   240 â”‚   def __call__(self) -> Dict[str, Any]:                      â”‚
â”‚   241 â”‚   â”‚   self.value = {}                                        â”‚
â”‚ â± 242 â”‚   â”‚   generated_data = self.generate_object(                 â”‚
â”‚   243 â”‚   â”‚   â”‚   self.json_schema["properties"], self.value         â”‚
â”‚   244 â”‚   â”‚   )                                                      â”‚
â”‚   245 â”‚   â”‚   return generated_data                                  â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®     â”‚
â”‚ â”‚ self = <jsonformer.main.Jsonformer object at 0x14849782a750> â”‚     â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯     â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/jsonformer/main.py:147 in generate_object               â”‚
â”‚                                                                      â”‚
â”‚   144 â”‚   ) -> Dict[str, Any]:                                       â”‚
â”‚   145 â”‚   â”‚   for key, schema in properties.items():                 â”‚
â”‚   146 â”‚   â”‚   â”‚   self.debug("[generate_object] generating value for â”‚
â”‚ â± 147 â”‚   â”‚   â”‚   obj[key] = self.generate_value(schema, obj, key)   â”‚
â”‚   148 â”‚   â”‚   return obj                                             â”‚
â”‚   149 â”‚                                                              â”‚
â”‚   150 â”‚   def generate_value(                                        â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚        key = 'temperature'                                       â”‚ â”‚
â”‚ â”‚        obj = {                                                   â”‚ â”‚
â”‚ â”‚              â”‚   'additive': 'None',                             â”‚ â”‚
â”‚ â”‚              â”‚   'solvent': 'DMF:MeOH',                          â”‚ â”‚
â”‚ â”‚              â”‚   'temperature': '|GENERATION|'                   â”‚ â”‚
â”‚ â”‚              }                                                   â”‚ â”‚
â”‚ â”‚ properties = {                                                   â”‚ â”‚
â”‚ â”‚              â”‚   'additive': {'type': 'string'},                 â”‚ â”‚
â”‚ â”‚              â”‚   'solvent': {'type': 'string'},                  â”‚ â”‚
â”‚ â”‚              â”‚   'temperature': {'type': 'number'},              â”‚ â”‚
â”‚ â”‚              â”‚   'temperature_unit': {'type': 'string'},         â”‚ â”‚
â”‚ â”‚              â”‚   'time': {'type': 'number'},                     â”‚ â”‚
â”‚ â”‚              â”‚   'time_unit': {'type': 'string'}                 â”‚ â”‚
â”‚ â”‚              }                                                   â”‚ â”‚
â”‚ â”‚     schema = {'type': 'number'}                                  â”‚ â”‚
â”‚ â”‚       self = <jsonformer.main.Jsonformer object at               â”‚ â”‚
â”‚ â”‚              0x14849782a750>                                     â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/jsonformer/main.py:162 in generate_value                â”‚
â”‚                                                                      â”‚
â”‚   159 â”‚   â”‚   â”‚   â”‚   obj[key] = self.generation_marker              â”‚
â”‚   160 â”‚   â”‚   â”‚   else:                                              â”‚
â”‚   161 â”‚   â”‚   â”‚   â”‚   obj.append(self.generation_marker)             â”‚
â”‚ â± 162 â”‚   â”‚   â”‚   return self.generate_number()                      â”‚
â”‚   163 â”‚   â”‚   elif schema_type == "boolean":                         â”‚
â”‚   164 â”‚   â”‚   â”‚   if key:                                            â”‚
â”‚   165 â”‚   â”‚   â”‚   â”‚   obj[key] = self.generation_marker              â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚         key = 'temperature'                                      â”‚ â”‚
â”‚ â”‚         obj = {                                                  â”‚ â”‚
â”‚ â”‚               â”‚   'additive': 'None',                            â”‚ â”‚
â”‚ â”‚               â”‚   'solvent': 'DMF:MeOH',                         â”‚ â”‚
â”‚ â”‚               â”‚   'temperature': '|GENERATION|'                  â”‚ â”‚
â”‚ â”‚               }                                                  â”‚ â”‚
â”‚ â”‚      schema = {'type': 'number'}                                 â”‚ â”‚
â”‚ â”‚ schema_type = 'number'                                           â”‚ â”‚
â”‚ â”‚        self = <jsonformer.main.Jsonformer object at              â”‚ â”‚
â”‚ â”‚               0x14849782a750>                                    â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/jsonformer/main.py:61 in generate_number                â”‚
â”‚                                                                      â”‚
â”‚    58 â”‚   â”‚   input_tokens = self.tokenizer.encode(prompt, return_te â”‚
â”‚    59 â”‚   â”‚   â”‚   self.model.device                                  â”‚
â”‚    60 â”‚   â”‚   )                                                      â”‚
â”‚ â±  61 â”‚   â”‚   response = self.model.generate(                        â”‚
â”‚    62 â”‚   â”‚   â”‚   input_tokens,                                      â”‚
â”‚    63 â”‚   â”‚   â”‚   max_new_tokens=self.max_number_tokens,             â”‚
â”‚    64 â”‚   â”‚   â”‚   num_return_sequences=1,                            â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚ input_tokens = tensor([[128000,    198,   1556,   8671,     67,  â”‚ â”‚
â”‚ â”‚                27620,  57831,   5176,     17,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   320,  19838,  14060,     11,    220,  â”‚ â”‚
â”‚ â”‚                18,     13,    845,   9653,                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   337,      8,    323,    473,     19,  â”‚ â”‚
â”‚ â”‚                43,     16,  14260,     21,                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    13,     20,     39,     17,     46,  â”‚ â”‚
â”‚ â”‚                320,  11057,  14060,     11,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   220,     15,     13,   1544,   9653,  â”‚ â”‚
â”‚ â”‚                337,    705,   1051,  56767,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   304,    264,  21655,    315,  20804,  â”‚ â”‚
â”‚ â”‚                37,    320,   1313,     13,                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    20,  65170,      8,    323,   2206,  â”‚ â”‚
â”‚ â”‚                47861,    320,   1313,     13,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    20,  65170,      8,   4871,    264,  â”‚ â”‚
â”‚ â”‚                220,   1135,  65170,   9168,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   348,    532,  19584,    449,    264,  â”‚ â”‚
â”‚ â”‚                350,    830,  12490,  32393,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     22733,   2107,     13,    578,   6425,  â”‚ â”‚
â”‚ â”‚                574,  32813,    369,    220,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    23,   2919,    520,    220,   2031,  â”‚ â”‚
â”‚ â”‚                37386,     34,     13,  20902,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     65251,   1499,   3418,     81,    599,  â”‚ â”‚
â”‚ â”‚                543,    483,  88751,    988,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   315,   4661,   1933,   1752,  48473,  â”‚ â”‚
â”‚ â”‚                1051,  54568,   6041,    505,                     â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   279,   2132,   1938,    315,  24494,  â”‚ â”‚
â”‚ â”‚                13,    578,  39887,    266,                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   519,     11,   8649,    264,   9099,  â”‚ â”‚
â”‚ â”‚                3392,    315,   2536,  48689,                     â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   599,    543,    483,  16946,     11,  â”‚ â”‚
â”‚ â”‚                574,   1654,   7719,    323,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   279,  64568,    483,  36841,  20227,  â”‚ â”‚
â”‚ â”‚                574,   6288,  23217,   1139,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   264,   5124,   2963,     74,  61319,  â”‚ â”‚
â”‚ â”‚                449,    264,    282,   1018,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   369,    279,  17876,  28786,    323,  â”‚ â”‚
â”‚ â”‚                76038,   7677,   1234,  81073,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚      6962,     13,   4427,   9861,   2536,  â”‚ â”‚
â”‚ â”‚                48689,    599,    543,    483,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     14933,    953,  19020,   1051,  19180,  â”‚ â”‚
â”‚ â”‚                555,   9482,   2518,    279,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     64568,    483,  36841,  20227,   5361,  â”‚ â”‚
â”‚ â”‚                3115,    304,    264,    220,                     â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    16,     25,     16,  21655,    315,  â”‚ â”‚
â”‚ â”‚                20804,     37,     25,   7979,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     47861,    323,  18054,    279,  39887,  â”‚ â”‚
â”‚ â”‚                266,    519,    449,    264,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     58325,    324,  24547,   6672,   8272,  â”‚ â”‚
â”‚ â”‚                555,   2033,  97139,    287,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   315,    279,  16946,    449,    279,  â”‚ â”‚
â”‚ â”‚                1890,  69996,  21655,    323,                     â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     84878,  76038,     13,   3804,  72457,  â”‚ â”‚
â”‚ â”‚                25402,  46479,    304,   9467,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     24012,    320,    605,  34363,     17,  â”‚ â”‚
â”‚ â”‚                8611,     81,      8,    520,                     â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   436,    739,     13,  58487,    220,  â”‚ â”‚
â”‚ â”‚                13384,  14060,    320,   5313,                    â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚     4,   3196,    389,    473,     19,  â”‚ â”‚
â”‚ â”‚                43,     16,  14260,     21,                       â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    13,     20,     39,     17,     46,  â”‚ â”‚
â”‚ â”‚                8,    315,   4661,   1933,                        â”‚ â”‚
â”‚ â”‚                â”‚   â”‚      1752,  64568,    483,   2027,     13,  â”‚ â”‚
â”‚ â”‚                720,   5207,   1121,    304,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   279,   2768,   4823,  11036,   3645,  â”‚ â”‚
â”‚ â”‚                512,   5018,   1337,    794,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   330,   1735,    498,    330,  13495,  â”‚ â”‚
â”‚ â”‚                794,   5324,    723,   3486,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   794,   5324,   1337,    794,    330,  â”‚ â”‚
â”‚ â”‚                928,  14682,    330,  39298,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   688,    794,   5324,   1337,    794,  â”‚ â”‚
â”‚ â”‚                330,    928,  14682,    330,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     35658,    794,   5324,   1337,    794,  â”‚ â”‚
â”‚ â”‚                330,   4174,  14682,    330,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     35658,  15176,    794,   5324,   1337,  â”‚ â”‚
â”‚ â”‚                794,    330,    928,  14682,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   330,   1712,    794,   5324,   1337,  â”‚ â”‚
â”‚ â”‚                794,    330,   4174,  14682,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚   330,   1712,  15176,    794,   5324,  â”‚ â”‚
â”‚ â”‚                1337,    794,    330,    928,                     â”‚ â”‚
â”‚ â”‚                â”‚   â”‚     32075,    534,   2122,     25,   5324,  â”‚ â”‚
â”‚ â”‚                723,   3486,    794,    330,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚      4155,    498,    330,  39298,    688,  â”‚ â”‚
â”‚ â”‚                794,    330,   8561,     37,                      â”‚ â”‚
â”‚ â”‚                â”‚   â”‚   â”‚    25,   7979,  47861,    498,    330,  â”‚ â”‚
â”‚ â”‚                35658,    794,    220]],                          â”‚ â”‚
â”‚ â”‚                â”‚      device='cuda:0')                           â”‚ â”‚
â”‚ â”‚   iterations = 0                                                 â”‚ â”‚
â”‚ â”‚       prompt = '\n Anhydrous MnCl2 (398 mg, 3.16 mmol) and       â”‚ â”‚
â”‚ â”‚                H4L1Â·6.5H2O (225 mg, 0.27 mmol), were '+1327      â”‚ â”‚
â”‚ â”‚         self = <jsonformer.main.Jsonformer object at             â”‚ â”‚
â”‚ â”‚                0x14849782a750>                                   â”‚ â”‚
â”‚ â”‚  temperature = None                                              â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/torch/utils/_contextlib.py:116 in decorate_context      â”‚
â”‚                                                                      â”‚
â”‚   113 â”‚   @functools.wraps(func)                                     â”‚
â”‚   114 â”‚   def decorate_context(*args, **kwargs):                     â”‚
â”‚   115 â”‚   â”‚   with ctx_factory():                                    â”‚
â”‚ â± 116 â”‚   â”‚   â”‚   return func(*args, **kwargs)                       â”‚
â”‚   117 â”‚                                                              â”‚
â”‚   118 â”‚   return decorate_context                                    â”‚
â”‚   119                                                                â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚        args = (                                                  â”‚ â”‚
â”‚ â”‚               â”‚   LlamaForCausalLM(                              â”‚ â”‚
â”‚ â”‚                 (model): LlamaModel(                             â”‚ â”‚
â”‚ â”‚               â”‚   (embed_tokens): Embedding(128256, 4096)        â”‚ â”‚
â”‚ â”‚               â”‚   (layers): ModuleList(                          â”‚ â”‚
â”‚ â”‚               â”‚     (0-31): 32 x LlamaDecoderLayer(              â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   (self_attn): LlamaFlashAttention2(         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (q_proj): Linear(in_features=4096,       â”‚ â”‚
â”‚ â”‚               out_features=4096, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (k_proj): Linear(in_features=4096,       â”‚ â”‚
â”‚ â”‚               out_features=1024, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (v_proj): Linear(in_features=4096,       â”‚ â”‚
â”‚ â”‚               out_features=1024, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (o_proj): Linear(in_features=4096,       â”‚ â”‚
â”‚ â”‚               out_features=4096, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (rotary_emb): LlamaRotaryEmbedding()     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   )                                          â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   (mlp): LlamaMLP(                           â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (gate_proj): Linear(in_features=4096,    â”‚ â”‚
â”‚ â”‚               out_features=14336, bias=False)                    â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (up_proj): Linear(in_features=4096,      â”‚ â”‚
â”‚ â”‚               out_features=14336, bias=False)                    â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (down_proj): Linear(in_features=14336,   â”‚ â”‚
â”‚ â”‚               out_features=4096, bias=False)                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     (act_fn): SiLU()                         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   )                                          â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   (input_layernorm): LlamaRMSNorm((4096,),   â”‚ â”‚
â”‚ â”‚               eps=1e-05)                                         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   (post_attention_layernorm):                â”‚ â”‚
â”‚ â”‚               LlamaRMSNorm((4096,), eps=1e-05)                   â”‚ â”‚
â”‚ â”‚               â”‚     )                                            â”‚ â”‚
â”‚ â”‚               â”‚   )                                              â”‚ â”‚
â”‚ â”‚               â”‚   (norm): LlamaRMSNorm((4096,), eps=1e-05)       â”‚ â”‚
â”‚ â”‚               â”‚   (rotary_emb): LlamaRotaryEmbedding()           â”‚ â”‚
â”‚ â”‚                 )                                                â”‚ â”‚
â”‚ â”‚                 (lm_head): Linear(in_features=4096,              â”‚ â”‚
â”‚ â”‚               out_features=128256, bias=False)                   â”‚ â”‚
â”‚ â”‚               ),                                                 â”‚ â”‚
â”‚ â”‚               â”‚   tensor([[128000,    198,   1556,   8671,       â”‚ â”‚
â”‚ â”‚               67,  27620,  57831,   5176,     17,                â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   320,  19838,  14060,     11,    220,   â”‚ â”‚
â”‚ â”‚               18,     13,    845,   9653,                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   337,      8,    323,    473,     19,   â”‚ â”‚
â”‚ â”‚               43,     16,  14260,     21,                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    13,     20,     39,     17,     46,   â”‚ â”‚
â”‚ â”‚               320,  11057,  14060,     11,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   220,     15,     13,   1544,   9653,   â”‚ â”‚
â”‚ â”‚               337,    705,   1051,  56767,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   304,    264,  21655,    315,  20804,   â”‚ â”‚
â”‚ â”‚               37,    320,   1313,     13,                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    20,  65170,      8,    323,   2206,   â”‚ â”‚
â”‚ â”‚               47861,    320,   1313,     13,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    20,  65170,      8,   4871,    264,   â”‚ â”‚
â”‚ â”‚               220,   1135,  65170,   9168,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   348,    532,  19584,    449,    264,   â”‚ â”‚
â”‚ â”‚               350,    830,  12490,  32393,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     22733,   2107,     13,    578,   6425,   â”‚ â”‚
â”‚ â”‚               574,  32813,    369,    220,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    23,   2919,    520,    220,   2031,   â”‚ â”‚
â”‚ â”‚               37386,     34,     13,  20902,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     65251,   1499,   3418,     81,    599,   â”‚ â”‚
â”‚ â”‚               543,    483,  88751,    988,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   315,   4661,   1933,   1752,  48473,   â”‚ â”‚
â”‚ â”‚               1051,  54568,   6041,    505,                      â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   279,   2132,   1938,    315,  24494,   â”‚ â”‚
â”‚ â”‚               13,    578,  39887,    266,                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   519,     11,   8649,    264,   9099,   â”‚ â”‚
â”‚ â”‚               3392,    315,   2536,  48689,                      â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   599,    543,    483,  16946,     11,   â”‚ â”‚
â”‚ â”‚               574,   1654,   7719,    323,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   279,  64568,    483,  36841,  20227,   â”‚ â”‚
â”‚ â”‚               574,   6288,  23217,   1139,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   264,   5124,   2963,     74,  61319,   â”‚ â”‚
â”‚ â”‚               449,    264,    282,   1018,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   369,    279,  17876,  28786,    323,   â”‚ â”‚
â”‚ â”‚               76038,   7677,   1234,  81073,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚      6962,     13,   4427,   9861,   2536,   â”‚ â”‚
â”‚ â”‚               48689,    599,    543,    483,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     14933,    953,  19020,   1051,  19180,   â”‚ â”‚
â”‚ â”‚               555,   9482,   2518,    279,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     64568,    483,  36841,  20227,   5361,   â”‚ â”‚
â”‚ â”‚               3115,    304,    264,    220,                      â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    16,     25,     16,  21655,    315,   â”‚ â”‚
â”‚ â”‚               20804,     37,     25,   7979,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     47861,    323,  18054,    279,  39887,   â”‚ â”‚
â”‚ â”‚               266,    519,    449,    264,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     58325,    324,  24547,   6672,   8272,   â”‚ â”‚
â”‚ â”‚               555,   2033,  97139,    287,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   315,    279,  16946,    449,    279,   â”‚ â”‚
â”‚ â”‚               1890,  69996,  21655,    323,                      â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     84878,  76038,     13,   3804,  72457,   â”‚ â”‚
â”‚ â”‚               25402,  46479,    304,   9467,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     24012,    320,    605,  34363,     17,   â”‚ â”‚
â”‚ â”‚               8611,     81,      8,    520,                      â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   436,    739,     13,  58487,    220,   â”‚ â”‚
â”‚ â”‚               13384,  14060,    320,   5313,                     â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚     4,   3196,    389,    473,     19,   â”‚ â”‚
â”‚ â”‚               43,     16,  14260,     21,                        â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    13,     20,     39,     17,     46,   â”‚ â”‚
â”‚ â”‚               8,    315,   4661,   1933,                         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚      1752,  64568,    483,   2027,     13,   â”‚ â”‚
â”‚ â”‚               720,   5207,   1121,    304,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   279,   2768,   4823,  11036,   3645,   â”‚ â”‚
â”‚ â”‚               512,   5018,   1337,    794,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   330,   1735,    498,    330,  13495,   â”‚ â”‚
â”‚ â”‚               794,   5324,    723,   3486,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   794,   5324,   1337,    794,    330,   â”‚ â”‚
â”‚ â”‚               928,  14682,    330,  39298,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   688,    794,   5324,   1337,    794,   â”‚ â”‚
â”‚ â”‚               330,    928,  14682,    330,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     35658,    794,   5324,   1337,    794,   â”‚ â”‚
â”‚ â”‚               330,   4174,  14682,    330,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     35658,  15176,    794,   5324,   1337,   â”‚ â”‚
â”‚ â”‚               794,    330,    928,  14682,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   330,   1712,    794,   5324,   1337,   â”‚ â”‚
â”‚ â”‚               794,    330,   4174,  14682,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚   330,   1712,  15176,    794,   5324,   â”‚ â”‚
â”‚ â”‚               1337,    794,    330,    928,                      â”‚ â”‚
â”‚ â”‚               â”‚   â”‚     32075,    534,   2122,     25,   5324,   â”‚ â”‚
â”‚ â”‚               723,   3486,    794,    330,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚      4155,    498,    330,  39298,    688,   â”‚ â”‚
â”‚ â”‚               794,    330,   8561,     37,                       â”‚ â”‚
â”‚ â”‚               â”‚   â”‚   â”‚    25,   7979,  47861,    498,    330,   â”‚ â”‚
â”‚ â”‚               35658,    794,    220]],                           â”‚ â”‚
â”‚ â”‚               â”‚      device='cuda:0')                            â”‚ â”‚
â”‚ â”‚               )                                                  â”‚ â”‚
â”‚ â”‚ ctx_factory = <bound method _DecoratorContextManager.clone of    â”‚ â”‚
â”‚ â”‚               <torch.autograd.grad_mode.no_grad object at        â”‚ â”‚
â”‚ â”‚               0x1484ad78f490>>                                   â”‚ â”‚
â”‚ â”‚        func = <function GenerationMixin.generate at              â”‚ â”‚
â”‚ â”‚               0x1484ad78bba0>                                    â”‚ â”‚
â”‚ â”‚      kwargs = {                                                  â”‚ â”‚
â”‚ â”‚               â”‚   'max_new_tokens': 6,                           â”‚ â”‚
â”‚ â”‚               â”‚   'num_return_sequences': 1,                     â”‚ â”‚
â”‚ â”‚               â”‚   'logits_processor': [                          â”‚ â”‚
â”‚ â”‚               â”‚   â”‚                                              â”‚ â”‚
â”‚ â”‚               <jsonformer.logits_processors.OutputNumbersTokens  â”‚ â”‚
â”‚ â”‚               object at 0x14849782a690>                          â”‚ â”‚
â”‚ â”‚               â”‚   ],                                             â”‚ â”‚
â”‚ â”‚               â”‚   'stopping_criteria': [                         â”‚ â”‚
â”‚ â”‚               â”‚   â”‚                                              â”‚ â”‚
â”‚ â”‚               <jsonformer.logits_processors.NumberStoppingCriteâ€¦ â”‚ â”‚
â”‚ â”‚               object at 0x148499999c10>                          â”‚ â”‚
â”‚ â”‚               â”‚   ],                                             â”‚ â”‚
â”‚ â”‚               â”‚   'temperature': 0.1,                            â”‚ â”‚
â”‚ â”‚               â”‚   'pad_token_id': 128009                         â”‚ â”‚
â”‚ â”‚               }                                                  â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/transformers/generation/utils.py:2252 in generate       â”‚
â”‚                                                                      â”‚
â”‚   2249 â”‚   â”‚   â”‚   )                                                 â”‚
â”‚   2250 â”‚   â”‚   â”‚                                                     â”‚
â”‚   2251 â”‚   â”‚   â”‚   # 12. run sample (it degenerates to greedy search â”‚
â”‚ â± 2252 â”‚   â”‚   â”‚   result = self._sample(                            â”‚
â”‚   2253 â”‚   â”‚   â”‚   â”‚   input_ids,                                    â”‚
â”‚   2254 â”‚   â”‚   â”‚   â”‚   logits_processor=prepared_logits_processor,   â”‚
â”‚   2255 â”‚   â”‚   â”‚   â”‚   stopping_criteria=prepared_stopping_criteria, â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚        accepts_attention_mask = True                             â”‚ â”‚
â”‚ â”‚               assistant_model = None                             â”‚ â”‚
â”‚ â”‚           assistant_tokenizer = None                             â”‚ â”‚
â”‚ â”‚                    batch_size = 1                                â”‚ â”‚
â”‚ â”‚                    cache_name = 'past_key_values'                â”‚ â”‚
â”‚ â”‚                        device = device(type='cuda', index=0)     â”‚ â”‚
â”‚ â”‚             generation_config = <repr-error 'Object of type      â”‚ â”‚
â”‚ â”‚                                 Tensor is not JSON               â”‚ â”‚
â”‚ â”‚                                 serializable'>                   â”‚ â”‚
â”‚ â”‚               generation_mode = <GenerationMode.SAMPLE:          â”‚ â”‚
â”‚ â”‚                                 'sample'>                        â”‚ â”‚
â”‚ â”‚        has_default_max_length = True                             â”‚ â”‚
â”‚ â”‚        has_default_min_length = True                             â”‚ â”‚
â”‚ â”‚                     input_ids = tensor([[128000,    198,   1556, â”‚ â”‚
â”‚ â”‚                                 8671,     67,  27620,  57831,    â”‚ â”‚
â”‚ â”‚                                 5176,     17,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   320,  19838,  14060, â”‚ â”‚
â”‚ â”‚                                 11,    220,     18,     13,      â”‚ â”‚
â”‚ â”‚                                 845,   9653,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   337,      8,    323, â”‚ â”‚
â”‚ â”‚                                 473,     19,     43,     16,     â”‚ â”‚
â”‚ â”‚                                 14260,     21,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    13,     20,     39, â”‚ â”‚
â”‚ â”‚                                 17,     46,    320,  11057,      â”‚ â”‚
â”‚ â”‚                                 14060,     11,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   220,     15,     13, â”‚ â”‚
â”‚ â”‚                                 1544,   9653,    337,    705,    â”‚ â”‚
â”‚ â”‚                                 1051,  56767,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   304,    264,  21655, â”‚ â”‚
â”‚ â”‚                                 315,  20804,     37,    320,     â”‚ â”‚
â”‚ â”‚                                 1313,     13,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    20,  65170,      8, â”‚ â”‚
â”‚ â”‚                                 323,   2206,  47861,    320,     â”‚ â”‚
â”‚ â”‚                                 1313,     13,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    20,  65170,      8, â”‚ â”‚
â”‚ â”‚                                 4871,    264,    220,   1135,    â”‚ â”‚
â”‚ â”‚                                 65170,   9168,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   348,    532,  19584, â”‚ â”‚
â”‚ â”‚                                 449,    264,    350,    830,     â”‚ â”‚
â”‚ â”‚                                 12490,  32393,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     22733,   2107,     13, â”‚ â”‚
â”‚ â”‚                                 578,   6425,    574,  32813,     â”‚ â”‚
â”‚ â”‚                                 369,    220,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    23,   2919,    520, â”‚ â”‚
â”‚ â”‚                                 220,   2031,  37386,     34,     â”‚ â”‚
â”‚ â”‚                                 13,  20902,                      â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     65251,   1499,   3418, â”‚ â”‚
â”‚ â”‚                                 81,    599,    543,    483,      â”‚ â”‚
â”‚ â”‚                                 88751,    988,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   315,   4661,   1933, â”‚ â”‚
â”‚ â”‚                                 1752,  48473,   1051,  54568,    â”‚ â”‚
â”‚ â”‚                                 6041,    505,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,   2132,   1938, â”‚ â”‚
â”‚ â”‚                                 315,  24494,     13,    578,     â”‚ â”‚
â”‚ â”‚                                 39887,    266,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   519,     11,   8649, â”‚ â”‚
â”‚ â”‚                                 264,   9099,   3392,    315,     â”‚ â”‚
â”‚ â”‚                                 2536,  48689,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   599,    543,    483, â”‚ â”‚
â”‚ â”‚                                 16946,     11,    574,   1654,   â”‚ â”‚
â”‚ â”‚                                 7719,    323,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,  64568,    483, â”‚ â”‚
â”‚ â”‚                                 36841,  20227,    574,   6288,   â”‚ â”‚
â”‚ â”‚                                 23217,   1139,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   264,   5124,   2963, â”‚ â”‚
â”‚ â”‚                                 74,  61319,    449,    264,      â”‚ â”‚
â”‚ â”‚                                 282,   1018,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   369,    279,  17876, â”‚ â”‚
â”‚ â”‚                                 28786,    323,  76038,   7677,   â”‚ â”‚
â”‚ â”‚                                 1234,  81073,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      6962,     13,   4427, â”‚ â”‚
â”‚ â”‚                                 9861,   2536,  48689,    599,    â”‚ â”‚
â”‚ â”‚                                 543,    483,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     14933,    953,  19020, â”‚ â”‚
â”‚ â”‚                                 1051,  19180,    555,   9482,    â”‚ â”‚
â”‚ â”‚                                 2518,    279,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     64568,    483,  36841, â”‚ â”‚
â”‚ â”‚                                 20227,   5361,   3115,    304,   â”‚ â”‚
â”‚ â”‚                                 264,    220,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    16,     25,     16, â”‚ â”‚
â”‚ â”‚                                 21655,    315,  20804,     37,   â”‚ â”‚
â”‚ â”‚                                 25,   7979,                      â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     47861,    323,  18054, â”‚ â”‚
â”‚ â”‚                                 279,  39887,    266,    519,     â”‚ â”‚
â”‚ â”‚                                 449,    264,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     58325,    324,  24547, â”‚ â”‚
â”‚ â”‚                                 6672,   8272,    555,   2033,    â”‚ â”‚
â”‚ â”‚                                 97139,    287,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   315,    279,  16946, â”‚ â”‚
â”‚ â”‚                                 449,    279,   1890,  69996,     â”‚ â”‚
â”‚ â”‚                                 21655,    323,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     84878,  76038,     13, â”‚ â”‚
â”‚ â”‚                                 3804,  72457,  25402,  46479,    â”‚ â”‚
â”‚ â”‚                                 304,   9467,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     24012,    320,    605, â”‚ â”‚
â”‚ â”‚                                 34363,     17,   8611,     81,   â”‚ â”‚
â”‚ â”‚                                 8,    520,                       â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   436,    739,     13, â”‚ â”‚
â”‚ â”‚                                 58487,    220,  13384,  14060,   â”‚ â”‚
â”‚ â”‚                                 320,   5313,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚     4,   3196,    389, â”‚ â”‚
â”‚ â”‚                                 473,     19,     43,     16,     â”‚ â”‚
â”‚ â”‚                                 14260,     21,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    13,     20,     39, â”‚ â”‚
â”‚ â”‚                                 17,     46,      8,    315,      â”‚ â”‚
â”‚ â”‚                                 4661,   1933,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      1752,  64568,    483, â”‚ â”‚
â”‚ â”‚                                 2027,     13,    720,   5207,    â”‚ â”‚
â”‚ â”‚                                 1121,    304,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,   2768,   4823, â”‚ â”‚
â”‚ â”‚                                 11036,   3645,    512,   5018,   â”‚ â”‚
â”‚ â”‚                                 1337,    794,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1735,    498, â”‚ â”‚
â”‚ â”‚                                 330,  13495,    794,   5324,     â”‚ â”‚
â”‚ â”‚                                 723,   3486,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   794,   5324,   1337, â”‚ â”‚
â”‚ â”‚                                 794,    330,    928,  14682,     â”‚ â”‚
â”‚ â”‚                                 330,  39298,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   688,    794,   5324, â”‚ â”‚
â”‚ â”‚                                 1337,    794,    330,    928,    â”‚ â”‚
â”‚ â”‚                                 14682,    330,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     35658,    794,   5324, â”‚ â”‚
â”‚ â”‚                                 1337,    794,    330,   4174,    â”‚ â”‚
â”‚ â”‚                                 14682,    330,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     35658,  15176,    794, â”‚ â”‚
â”‚ â”‚                                 5324,   1337,    794,    330,    â”‚ â”‚
â”‚ â”‚                                 928,  14682,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1712,    794, â”‚ â”‚
â”‚ â”‚                                 5324,   1337,    794,    330,    â”‚ â”‚
â”‚ â”‚                                 4174,  14682,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1712,  15176, â”‚ â”‚
â”‚ â”‚                                 794,   5324,   1337,    794,     â”‚ â”‚
â”‚ â”‚                                 330,    928,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     32075,    534,   2122, â”‚ â”‚
â”‚ â”‚                                 25,   5324,    723,   3486,      â”‚ â”‚
â”‚ â”‚                                 794,    330,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      4155,    498,    330, â”‚ â”‚
â”‚ â”‚                                 39298,    688,    794,    330,   â”‚ â”‚
â”‚ â”‚                                 8561,     37,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    25,   7979,  47861, â”‚ â”‚
â”‚ â”‚                                 498,    330,  35658,    794,     â”‚ â”‚
â”‚ â”‚                                 220]],                           â”‚ â”‚
â”‚ â”‚                                 â”‚      device='cuda:0')          â”‚ â”‚
â”‚ â”‚              input_ids_length = 386                              â”‚ â”‚
â”‚ â”‚                        inputs = tensor([[128000,    198,   1556, â”‚ â”‚
â”‚ â”‚                                 8671,     67,  27620,  57831,    â”‚ â”‚
â”‚ â”‚                                 5176,     17,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   320,  19838,  14060, â”‚ â”‚
â”‚ â”‚                                 11,    220,     18,     13,      â”‚ â”‚
â”‚ â”‚                                 845,   9653,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   337,      8,    323, â”‚ â”‚
â”‚ â”‚                                 473,     19,     43,     16,     â”‚ â”‚
â”‚ â”‚                                 14260,     21,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    13,     20,     39, â”‚ â”‚
â”‚ â”‚                                 17,     46,    320,  11057,      â”‚ â”‚
â”‚ â”‚                                 14060,     11,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   220,     15,     13, â”‚ â”‚
â”‚ â”‚                                 1544,   9653,    337,    705,    â”‚ â”‚
â”‚ â”‚                                 1051,  56767,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   304,    264,  21655, â”‚ â”‚
â”‚ â”‚                                 315,  20804,     37,    320,     â”‚ â”‚
â”‚ â”‚                                 1313,     13,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    20,  65170,      8, â”‚ â”‚
â”‚ â”‚                                 323,   2206,  47861,    320,     â”‚ â”‚
â”‚ â”‚                                 1313,     13,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    20,  65170,      8, â”‚ â”‚
â”‚ â”‚                                 4871,    264,    220,   1135,    â”‚ â”‚
â”‚ â”‚                                 65170,   9168,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   348,    532,  19584, â”‚ â”‚
â”‚ â”‚                                 449,    264,    350,    830,     â”‚ â”‚
â”‚ â”‚                                 12490,  32393,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     22733,   2107,     13, â”‚ â”‚
â”‚ â”‚                                 578,   6425,    574,  32813,     â”‚ â”‚
â”‚ â”‚                                 369,    220,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    23,   2919,    520, â”‚ â”‚
â”‚ â”‚                                 220,   2031,  37386,     34,     â”‚ â”‚
â”‚ â”‚                                 13,  20902,                      â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     65251,   1499,   3418, â”‚ â”‚
â”‚ â”‚                                 81,    599,    543,    483,      â”‚ â”‚
â”‚ â”‚                                 88751,    988,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   315,   4661,   1933, â”‚ â”‚
â”‚ â”‚                                 1752,  48473,   1051,  54568,    â”‚ â”‚
â”‚ â”‚                                 6041,    505,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,   2132,   1938, â”‚ â”‚
â”‚ â”‚                                 315,  24494,     13,    578,     â”‚ â”‚
â”‚ â”‚                                 39887,    266,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   519,     11,   8649, â”‚ â”‚
â”‚ â”‚                                 264,   9099,   3392,    315,     â”‚ â”‚
â”‚ â”‚                                 2536,  48689,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   599,    543,    483, â”‚ â”‚
â”‚ â”‚                                 16946,     11,    574,   1654,   â”‚ â”‚
â”‚ â”‚                                 7719,    323,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,  64568,    483, â”‚ â”‚
â”‚ â”‚                                 36841,  20227,    574,   6288,   â”‚ â”‚
â”‚ â”‚                                 23217,   1139,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   264,   5124,   2963, â”‚ â”‚
â”‚ â”‚                                 74,  61319,    449,    264,      â”‚ â”‚
â”‚ â”‚                                 282,   1018,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   369,    279,  17876, â”‚ â”‚
â”‚ â”‚                                 28786,    323,  76038,   7677,   â”‚ â”‚
â”‚ â”‚                                 1234,  81073,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      6962,     13,   4427, â”‚ â”‚
â”‚ â”‚                                 9861,   2536,  48689,    599,    â”‚ â”‚
â”‚ â”‚                                 543,    483,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     14933,    953,  19020, â”‚ â”‚
â”‚ â”‚                                 1051,  19180,    555,   9482,    â”‚ â”‚
â”‚ â”‚                                 2518,    279,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     64568,    483,  36841, â”‚ â”‚
â”‚ â”‚                                 20227,   5361,   3115,    304,   â”‚ â”‚
â”‚ â”‚                                 264,    220,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    16,     25,     16, â”‚ â”‚
â”‚ â”‚                                 21655,    315,  20804,     37,   â”‚ â”‚
â”‚ â”‚                                 25,   7979,                      â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     47861,    323,  18054, â”‚ â”‚
â”‚ â”‚                                 279,  39887,    266,    519,     â”‚ â”‚
â”‚ â”‚                                 449,    264,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     58325,    324,  24547, â”‚ â”‚
â”‚ â”‚                                 6672,   8272,    555,   2033,    â”‚ â”‚
â”‚ â”‚                                 97139,    287,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   315,    279,  16946, â”‚ â”‚
â”‚ â”‚                                 449,    279,   1890,  69996,     â”‚ â”‚
â”‚ â”‚                                 21655,    323,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     84878,  76038,     13, â”‚ â”‚
â”‚ â”‚                                 3804,  72457,  25402,  46479,    â”‚ â”‚
â”‚ â”‚                                 304,   9467,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     24012,    320,    605, â”‚ â”‚
â”‚ â”‚                                 34363,     17,   8611,     81,   â”‚ â”‚
â”‚ â”‚                                 8,    520,                       â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   436,    739,     13, â”‚ â”‚
â”‚ â”‚                                 58487,    220,  13384,  14060,   â”‚ â”‚
â”‚ â”‚                                 320,   5313,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚     4,   3196,    389, â”‚ â”‚
â”‚ â”‚                                 473,     19,     43,     16,     â”‚ â”‚
â”‚ â”‚                                 14260,     21,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    13,     20,     39, â”‚ â”‚
â”‚ â”‚                                 17,     46,      8,    315,      â”‚ â”‚
â”‚ â”‚                                 4661,   1933,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      1752,  64568,    483, â”‚ â”‚
â”‚ â”‚                                 2027,     13,    720,   5207,    â”‚ â”‚
â”‚ â”‚                                 1121,    304,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,   2768,   4823, â”‚ â”‚
â”‚ â”‚                                 11036,   3645,    512,   5018,   â”‚ â”‚
â”‚ â”‚                                 1337,    794,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1735,    498, â”‚ â”‚
â”‚ â”‚                                 330,  13495,    794,   5324,     â”‚ â”‚
â”‚ â”‚                                 723,   3486,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   794,   5324,   1337, â”‚ â”‚
â”‚ â”‚                                 794,    330,    928,  14682,     â”‚ â”‚
â”‚ â”‚                                 330,  39298,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   688,    794,   5324, â”‚ â”‚
â”‚ â”‚                                 1337,    794,    330,    928,    â”‚ â”‚
â”‚ â”‚                                 14682,    330,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     35658,    794,   5324, â”‚ â”‚
â”‚ â”‚                                 1337,    794,    330,   4174,    â”‚ â”‚
â”‚ â”‚                                 14682,    330,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     35658,  15176,    794, â”‚ â”‚
â”‚ â”‚                                 5324,   1337,    794,    330,    â”‚ â”‚
â”‚ â”‚                                 928,  14682,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1712,    794, â”‚ â”‚
â”‚ â”‚                                 5324,   1337,    794,    330,    â”‚ â”‚
â”‚ â”‚                                 4174,  14682,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1712,  15176, â”‚ â”‚
â”‚ â”‚                                 794,   5324,   1337,    794,     â”‚ â”‚
â”‚ â”‚                                 330,    928,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     32075,    534,   2122, â”‚ â”‚
â”‚ â”‚                                 25,   5324,    723,   3486,      â”‚ â”‚
â”‚ â”‚                                 794,    330,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      4155,    498,    330, â”‚ â”‚
â”‚ â”‚                                 39298,    688,    794,    330,   â”‚ â”‚
â”‚ â”‚                                 8561,     37,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    25,   7979,  47861, â”‚ â”‚
â”‚ â”‚                                 498,    330,  35658,    794,     â”‚ â”‚
â”‚ â”‚                                 220]],                           â”‚ â”‚
â”‚ â”‚                                 â”‚      device='cuda:0')          â”‚ â”‚
â”‚ â”‚                 inputs_tensor = tensor([[128000,    198,   1556, â”‚ â”‚
â”‚ â”‚                                 8671,     67,  27620,  57831,    â”‚ â”‚
â”‚ â”‚                                 5176,     17,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   320,  19838,  14060, â”‚ â”‚
â”‚ â”‚                                 11,    220,     18,     13,      â”‚ â”‚
â”‚ â”‚                                 845,   9653,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   337,      8,    323, â”‚ â”‚
â”‚ â”‚                                 473,     19,     43,     16,     â”‚ â”‚
â”‚ â”‚                                 14260,     21,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    13,     20,     39, â”‚ â”‚
â”‚ â”‚                                 17,     46,    320,  11057,      â”‚ â”‚
â”‚ â”‚                                 14060,     11,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   220,     15,     13, â”‚ â”‚
â”‚ â”‚                                 1544,   9653,    337,    705,    â”‚ â”‚
â”‚ â”‚                                 1051,  56767,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   304,    264,  21655, â”‚ â”‚
â”‚ â”‚                                 315,  20804,     37,    320,     â”‚ â”‚
â”‚ â”‚                                 1313,     13,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    20,  65170,      8, â”‚ â”‚
â”‚ â”‚                                 323,   2206,  47861,    320,     â”‚ â”‚
â”‚ â”‚                                 1313,     13,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    20,  65170,      8, â”‚ â”‚
â”‚ â”‚                                 4871,    264,    220,   1135,    â”‚ â”‚
â”‚ â”‚                                 65170,   9168,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   348,    532,  19584, â”‚ â”‚
â”‚ â”‚                                 449,    264,    350,    830,     â”‚ â”‚
â”‚ â”‚                                 12490,  32393,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     22733,   2107,     13, â”‚ â”‚
â”‚ â”‚                                 578,   6425,    574,  32813,     â”‚ â”‚
â”‚ â”‚                                 369,    220,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    23,   2919,    520, â”‚ â”‚
â”‚ â”‚                                 220,   2031,  37386,     34,     â”‚ â”‚
â”‚ â”‚                                 13,  20902,                      â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     65251,   1499,   3418, â”‚ â”‚
â”‚ â”‚                                 81,    599,    543,    483,      â”‚ â”‚
â”‚ â”‚                                 88751,    988,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   315,   4661,   1933, â”‚ â”‚
â”‚ â”‚                                 1752,  48473,   1051,  54568,    â”‚ â”‚
â”‚ â”‚                                 6041,    505,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,   2132,   1938, â”‚ â”‚
â”‚ â”‚                                 315,  24494,     13,    578,     â”‚ â”‚
â”‚ â”‚                                 39887,    266,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   519,     11,   8649, â”‚ â”‚
â”‚ â”‚                                 264,   9099,   3392,    315,     â”‚ â”‚
â”‚ â”‚                                 2536,  48689,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   599,    543,    483, â”‚ â”‚
â”‚ â”‚                                 16946,     11,    574,   1654,   â”‚ â”‚
â”‚ â”‚                                 7719,    323,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,  64568,    483, â”‚ â”‚
â”‚ â”‚                                 36841,  20227,    574,   6288,   â”‚ â”‚
â”‚ â”‚                                 23217,   1139,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   264,   5124,   2963, â”‚ â”‚
â”‚ â”‚                                 74,  61319,    449,    264,      â”‚ â”‚
â”‚ â”‚                                 282,   1018,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   369,    279,  17876, â”‚ â”‚
â”‚ â”‚                                 28786,    323,  76038,   7677,   â”‚ â”‚
â”‚ â”‚                                 1234,  81073,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      6962,     13,   4427, â”‚ â”‚
â”‚ â”‚                                 9861,   2536,  48689,    599,    â”‚ â”‚
â”‚ â”‚                                 543,    483,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     14933,    953,  19020, â”‚ â”‚
â”‚ â”‚                                 1051,  19180,    555,   9482,    â”‚ â”‚
â”‚ â”‚                                 2518,    279,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     64568,    483,  36841, â”‚ â”‚
â”‚ â”‚                                 20227,   5361,   3115,    304,   â”‚ â”‚
â”‚ â”‚                                 264,    220,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    16,     25,     16, â”‚ â”‚
â”‚ â”‚                                 21655,    315,  20804,     37,   â”‚ â”‚
â”‚ â”‚                                 25,   7979,                      â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     47861,    323,  18054, â”‚ â”‚
â”‚ â”‚                                 279,  39887,    266,    519,     â”‚ â”‚
â”‚ â”‚                                 449,    264,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     58325,    324,  24547, â”‚ â”‚
â”‚ â”‚                                 6672,   8272,    555,   2033,    â”‚ â”‚
â”‚ â”‚                                 97139,    287,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   315,    279,  16946, â”‚ â”‚
â”‚ â”‚                                 449,    279,   1890,  69996,     â”‚ â”‚
â”‚ â”‚                                 21655,    323,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     84878,  76038,     13, â”‚ â”‚
â”‚ â”‚                                 3804,  72457,  25402,  46479,    â”‚ â”‚
â”‚ â”‚                                 304,   9467,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     24012,    320,    605, â”‚ â”‚
â”‚ â”‚                                 34363,     17,   8611,     81,   â”‚ â”‚
â”‚ â”‚                                 8,    520,                       â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   436,    739,     13, â”‚ â”‚
â”‚ â”‚                                 58487,    220,  13384,  14060,   â”‚ â”‚
â”‚ â”‚                                 320,   5313,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚     4,   3196,    389, â”‚ â”‚
â”‚ â”‚                                 473,     19,     43,     16,     â”‚ â”‚
â”‚ â”‚                                 14260,     21,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    13,     20,     39, â”‚ â”‚
â”‚ â”‚                                 17,     46,      8,    315,      â”‚ â”‚
â”‚ â”‚                                 4661,   1933,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      1752,  64568,    483, â”‚ â”‚
â”‚ â”‚                                 2027,     13,    720,   5207,    â”‚ â”‚
â”‚ â”‚                                 1121,    304,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   279,   2768,   4823, â”‚ â”‚
â”‚ â”‚                                 11036,   3645,    512,   5018,   â”‚ â”‚
â”‚ â”‚                                 1337,    794,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1735,    498, â”‚ â”‚
â”‚ â”‚                                 330,  13495,    794,   5324,     â”‚ â”‚
â”‚ â”‚                                 723,   3486,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   794,   5324,   1337, â”‚ â”‚
â”‚ â”‚                                 794,    330,    928,  14682,     â”‚ â”‚
â”‚ â”‚                                 330,  39298,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   688,    794,   5324, â”‚ â”‚
â”‚ â”‚                                 1337,    794,    330,    928,    â”‚ â”‚
â”‚ â”‚                                 14682,    330,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     35658,    794,   5324, â”‚ â”‚
â”‚ â”‚                                 1337,    794,    330,   4174,    â”‚ â”‚
â”‚ â”‚                                 14682,    330,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     35658,  15176,    794, â”‚ â”‚
â”‚ â”‚                                 5324,   1337,    794,    330,    â”‚ â”‚
â”‚ â”‚                                 928,  14682,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1712,    794, â”‚ â”‚
â”‚ â”‚                                 5324,   1337,    794,    330,    â”‚ â”‚
â”‚ â”‚                                 4174,  14682,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚   330,   1712,  15176, â”‚ â”‚
â”‚ â”‚                                 794,   5324,   1337,    794,     â”‚ â”‚
â”‚ â”‚                                 330,    928,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     32075,    534,   2122, â”‚ â”‚
â”‚ â”‚                                 25,   5324,    723,   3486,      â”‚ â”‚
â”‚ â”‚                                 794,    330,                     â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚      4155,    498,    330, â”‚ â”‚
â”‚ â”‚                                 39298,    688,    794,    330,   â”‚ â”‚
â”‚ â”‚                                 8561,     37,                    â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   â”‚    25,   7979,  47861, â”‚ â”‚
â”‚ â”‚                                 498,    330,  35658,    794,     â”‚ â”‚
â”‚ â”‚                                 220]],                           â”‚ â”‚
â”‚ â”‚                                 â”‚      device='cuda:0')          â”‚ â”‚
â”‚ â”‚                        kwargs = {                                â”‚ â”‚
â”‚ â”‚                                 â”‚   'max_new_tokens': 6,         â”‚ â”‚
â”‚ â”‚                                 â”‚   'num_return_sequences': 1,   â”‚ â”‚
â”‚ â”‚                                 â”‚   'temperature': 0.1,          â”‚ â”‚
â”‚ â”‚                                 â”‚   'pad_token_id': 128009       â”‚ â”‚
â”‚ â”‚                                 }                                â”‚ â”‚
â”‚ â”‚     kwargs_has_attention_mask = False                            â”‚ â”‚
â”‚ â”‚              logits_processor = [                                â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <jsonformer.logits_processors.Oâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x14849782a690>        â”‚ â”‚
â”‚ â”‚                                 ]                                â”‚ â”‚
â”‚ â”‚              max_cache_length = 392                              â”‚ â”‚
â”‚ â”‚              model_input_name = 'input_ids'                      â”‚ â”‚
â”‚ â”‚                  model_kwargs = {                                â”‚ â”‚
â”‚ â”‚                                 â”‚   'attention_mask':            â”‚ â”‚
â”‚ â”‚                                 tensor([[1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, â”‚ â”‚
â”‚ â”‚                                 1, 1, 1, 1, 1,                   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚    1, 1]],                 â”‚ â”‚
â”‚ â”‚                                 device='cuda:0'),                â”‚ â”‚
â”‚ â”‚                                 â”‚   'num_logits_to_keep': 1,     â”‚ â”‚
â”‚ â”‚                                 â”‚   'past_key_values':           â”‚ â”‚
â”‚ â”‚                                 DynamicCache(),                  â”‚ â”‚
â”‚ â”‚                                 â”‚   'use_cache': True            â”‚ â”‚
â”‚ â”‚                                 }                                â”‚ â”‚
â”‚ â”‚  negative_prompt_attention_mask None                             â”‚ â”‚
â”‚ â”‚                               =                                  â”‚ â”‚
â”‚ â”‚           negative_prompt_ids = None                             â”‚ â”‚
â”‚ â”‚      prefix_allowed_tokens_fn = None                             â”‚ â”‚
â”‚ â”‚     prepared_logits_processor = [                                â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <jsonformer.logits_processors.Oâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x14849782a690>,       â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <transformers.generation.logitsâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x1484977c8f10>,       â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <transformers.generation.logitsâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x148497807bd0>,       â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <transformers.generation.logitsâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x148497876b90>        â”‚ â”‚
â”‚ â”‚                                 ]                                â”‚ â”‚
â”‚ â”‚    prepared_stopping_criteria = [                                â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <transformers.generation.stoppiâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x148497876e10>,       â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <transformers.generation.stoppiâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x14849782bdd0>,       â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <jsonformer.logits_processors.Nâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x148499999c10>        â”‚ â”‚
â”‚ â”‚                                 ]                                â”‚ â”‚
â”‚ â”‚       requires_attention_mask = True                             â”‚ â”‚
â”‚ â”‚                          self = LlamaForCausalLM(                â”‚ â”‚
â”‚ â”‚                                   (model): LlamaModel(           â”‚ â”‚
â”‚ â”‚                                 â”‚   (embed_tokens):              â”‚ â”‚
â”‚ â”‚                                 Embedding(128256, 4096)          â”‚ â”‚
â”‚ â”‚                                 â”‚   (layers): ModuleList(        â”‚ â”‚
â”‚ â”‚                                 â”‚     (0-31): 32 x               â”‚ â”‚
â”‚ â”‚                                 LlamaDecoderLayer(               â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   (self_attn):             â”‚ â”‚
â”‚ â”‚                                 LlamaFlashAttention2(            â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (q_proj):              â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=4096, bias=False)   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (k_proj):              â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=1024, bias=False)   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (v_proj):              â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=1024, bias=False)   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (o_proj):              â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=4096, bias=False)   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (rotary_emb):          â”‚ â”‚
â”‚ â”‚                                 LlamaRotaryEmbedding()           â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   )                        â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   (mlp): LlamaMLP(         â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (gate_proj):           â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=14336, bias=False)  â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (up_proj):             â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=14336, bias=False)  â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (down_proj):           â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=14336,        â”‚ â”‚
â”‚ â”‚                                 out_features=4096, bias=False)   â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚     (act_fn): SiLU()       â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   )                        â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚   (input_layernorm):       â”‚ â”‚
â”‚ â”‚                                 LlamaRMSNorm((4096,), eps=1e-05) â”‚ â”‚
â”‚ â”‚                                 â”‚   â”‚                            â”‚ â”‚
â”‚ â”‚                                 (post_attention_layernorm):      â”‚ â”‚
â”‚ â”‚                                 LlamaRMSNorm((4096,), eps=1e-05) â”‚ â”‚
â”‚ â”‚                                 â”‚     )                          â”‚ â”‚
â”‚ â”‚                                 â”‚   )                            â”‚ â”‚
â”‚ â”‚                                 â”‚   (norm):                      â”‚ â”‚
â”‚ â”‚                                 LlamaRMSNorm((4096,), eps=1e-05) â”‚ â”‚
â”‚ â”‚                                 â”‚   (rotary_emb):                â”‚ â”‚
â”‚ â”‚                                 LlamaRotaryEmbedding()           â”‚ â”‚
â”‚ â”‚                                   )                              â”‚ â”‚
â”‚ â”‚                                   (lm_head):                     â”‚ â”‚
â”‚ â”‚                                 Linear(in_features=4096,         â”‚ â”‚
â”‚ â”‚                                 out_features=128256, bias=False) â”‚ â”‚
â”‚ â”‚                                 )                                â”‚ â”‚
â”‚ â”‚             stopping_criteria = [                                â”‚ â”‚
â”‚ â”‚                                 â”‚                                â”‚ â”‚
â”‚ â”‚                                 <jsonformer.logits_processors.Nâ€¦ â”‚ â”‚
â”‚ â”‚                                 object at 0x148499999c10>        â”‚ â”‚
â”‚ â”‚                                 ]                                â”‚ â”‚
â”‚ â”‚                      streamer = None                             â”‚ â”‚
â”‚ â”‚                   synced_gpus = False                            â”‚ â”‚
â”‚ â”‚                     tokenizer = None                             â”‚ â”‚
â”‚ â”‚            user_defined_cache = None                             â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/transformers/generation/utils.py:3271 in _sample        â”‚
â”‚                                                                      â”‚
â”‚   3268 â”‚   â”‚   â”‚   next_token_logits = next_token_logits.to(input_id â”‚
â”‚   3269 â”‚   â”‚   â”‚                                                     â”‚
â”‚   3270 â”‚   â”‚   â”‚   # pre-process distribution                        â”‚
â”‚ â± 3271 â”‚   â”‚   â”‚   next_token_scores = logits_processor(input_ids, n â”‚
â”‚   3272 â”‚   â”‚   â”‚                                                     â”‚
â”‚   3273 â”‚   â”‚   â”‚   # Store scores, attentions and hidden_states when â”‚
â”‚   3274 â”‚   â”‚   â”‚   if return_dict_in_generate:                       â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚                batch_size = 1                                    â”‚ â”‚
â”‚ â”‚          cross_attentions = None                                 â”‚ â”‚
â”‚ â”‚                   cur_len = 386                                  â”‚ â”‚
â”‚ â”‚        decoder_attentions = None                                 â”‚ â”‚
â”‚ â”‚     decoder_hidden_states = None                                 â”‚ â”‚
â”‚ â”‚                 do_sample = True                                 â”‚ â”‚
â”‚ â”‚         generation_config = <repr-error 'Object of type Tensor   â”‚ â”‚
â”‚ â”‚                             is not JSON serializable'>           â”‚ â”‚
â”‚ â”‚ has_eos_stopping_criteria = True                                 â”‚ â”‚
â”‚ â”‚                 input_ids = tensor([[128000,    198,   1556,     â”‚ â”‚
â”‚ â”‚                             8671,     67,  27620,  57831,        â”‚ â”‚
â”‚ â”‚                             5176,     17,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   320,  19838,  14060,     â”‚ â”‚
â”‚ â”‚                             11,    220,     18,     13,    845,  â”‚ â”‚
â”‚ â”‚                             9653,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   337,      8,    323,     â”‚ â”‚
â”‚ â”‚                             473,     19,     43,     16,  14260, â”‚ â”‚
â”‚ â”‚                             21,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    13,     20,     39,     â”‚ â”‚
â”‚ â”‚                             17,     46,    320,  11057,  14060,  â”‚ â”‚
â”‚ â”‚                             11,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   220,     15,     13,     â”‚ â”‚
â”‚ â”‚                             1544,   9653,    337,    705,        â”‚ â”‚
â”‚ â”‚                             1051,  56767,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   304,    264,  21655,     â”‚ â”‚
â”‚ â”‚                             315,  20804,     37,    320,   1313, â”‚ â”‚
â”‚ â”‚                             13,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    20,  65170,      8,     â”‚ â”‚
â”‚ â”‚                             323,   2206,  47861,    320,   1313, â”‚ â”‚
â”‚ â”‚                             13,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    20,  65170,      8,     â”‚ â”‚
â”‚ â”‚                             4871,    264,    220,   1135,        â”‚ â”‚
â”‚ â”‚                             65170,   9168,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   348,    532,  19584,     â”‚ â”‚
â”‚ â”‚                             449,    264,    350,    830,  12490, â”‚ â”‚
â”‚ â”‚                             32393,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     22733,   2107,     13,     â”‚ â”‚
â”‚ â”‚                             578,   6425,    574,  32813,    369, â”‚ â”‚
â”‚ â”‚                             220,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    23,   2919,    520,     â”‚ â”‚
â”‚ â”‚                             220,   2031,  37386,     34,     13, â”‚ â”‚
â”‚ â”‚                             20902,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     65251,   1499,   3418,     â”‚ â”‚
â”‚ â”‚                             81,    599,    543,    483,  88751,  â”‚ â”‚
â”‚ â”‚                             988,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   315,   4661,   1933,     â”‚ â”‚
â”‚ â”‚                             1752,  48473,   1051,  54568,        â”‚ â”‚
â”‚ â”‚                             6041,    505,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   279,   2132,   1938,     â”‚ â”‚
â”‚ â”‚                             315,  24494,     13,    578,  39887, â”‚ â”‚
â”‚ â”‚                             266,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   519,     11,   8649,     â”‚ â”‚
â”‚ â”‚                             264,   9099,   3392,    315,   2536, â”‚ â”‚
â”‚ â”‚                             48689,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   599,    543,    483,     â”‚ â”‚
â”‚ â”‚                             16946,     11,    574,   1654,       â”‚ â”‚
â”‚ â”‚                             7719,    323,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   279,  64568,    483,     â”‚ â”‚
â”‚ â”‚                             36841,  20227,    574,   6288,       â”‚ â”‚
â”‚ â”‚                             23217,   1139,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   264,   5124,   2963,     â”‚ â”‚
â”‚ â”‚                             74,  61319,    449,    264,    282,  â”‚ â”‚
â”‚ â”‚                             1018,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   369,    279,  17876,     â”‚ â”‚
â”‚ â”‚                             28786,    323,  76038,   7677,       â”‚ â”‚
â”‚ â”‚                             1234,  81073,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚      6962,     13,   4427,     â”‚ â”‚
â”‚ â”‚                             9861,   2536,  48689,    599,        â”‚ â”‚
â”‚ â”‚                             543,    483,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     14933,    953,  19020,     â”‚ â”‚
â”‚ â”‚                             1051,  19180,    555,   9482,        â”‚ â”‚
â”‚ â”‚                             2518,    279,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     64568,    483,  36841,     â”‚ â”‚
â”‚ â”‚                             20227,   5361,   3115,    304,       â”‚ â”‚
â”‚ â”‚                             264,    220,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    16,     25,     16,     â”‚ â”‚
â”‚ â”‚                             21655,    315,  20804,     37,       â”‚ â”‚
â”‚ â”‚                             25,   7979,                          â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     47861,    323,  18054,     â”‚ â”‚
â”‚ â”‚                             279,  39887,    266,    519,    449, â”‚ â”‚
â”‚ â”‚                             264,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     58325,    324,  24547,     â”‚ â”‚
â”‚ â”‚                             6672,   8272,    555,   2033,        â”‚ â”‚
â”‚ â”‚                             97139,    287,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   315,    279,  16946,     â”‚ â”‚
â”‚ â”‚                             449,    279,   1890,  69996,  21655, â”‚ â”‚
â”‚ â”‚                             323,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     84878,  76038,     13,     â”‚ â”‚
â”‚ â”‚                             3804,  72457,  25402,  46479,        â”‚ â”‚
â”‚ â”‚                             304,   9467,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     24012,    320,    605,     â”‚ â”‚
â”‚ â”‚                             34363,     17,   8611,     81,       â”‚ â”‚
â”‚ â”‚                             8,    520,                           â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   436,    739,     13,     â”‚ â”‚
â”‚ â”‚                             58487,    220,  13384,  14060,       â”‚ â”‚
â”‚ â”‚                             320,   5313,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚     4,   3196,    389,     â”‚ â”‚
â”‚ â”‚                             473,     19,     43,     16,  14260, â”‚ â”‚
â”‚ â”‚                             21,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    13,     20,     39,     â”‚ â”‚
â”‚ â”‚                             17,     46,      8,    315,   4661,  â”‚ â”‚
â”‚ â”‚                             1933,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚      1752,  64568,    483,     â”‚ â”‚
â”‚ â”‚                             2027,     13,    720,   5207,        â”‚ â”‚
â”‚ â”‚                             1121,    304,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   279,   2768,   4823,     â”‚ â”‚
â”‚ â”‚                             11036,   3645,    512,   5018,       â”‚ â”‚
â”‚ â”‚                             1337,    794,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   330,   1735,    498,     â”‚ â”‚
â”‚ â”‚                             330,  13495,    794,   5324,    723, â”‚ â”‚
â”‚ â”‚                             3486,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   794,   5324,   1337,     â”‚ â”‚
â”‚ â”‚                             794,    330,    928,  14682,    330, â”‚ â”‚
â”‚ â”‚                             39298,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   688,    794,   5324,     â”‚ â”‚
â”‚ â”‚                             1337,    794,    330,    928,        â”‚ â”‚
â”‚ â”‚                             14682,    330,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     35658,    794,   5324,     â”‚ â”‚
â”‚ â”‚                             1337,    794,    330,   4174,        â”‚ â”‚
â”‚ â”‚                             14682,    330,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     35658,  15176,    794,     â”‚ â”‚
â”‚ â”‚                             5324,   1337,    794,    330,        â”‚ â”‚
â”‚ â”‚                             928,  14682,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   330,   1712,    794,     â”‚ â”‚
â”‚ â”‚                             5324,   1337,    794,    330,        â”‚ â”‚
â”‚ â”‚                             4174,  14682,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   330,   1712,  15176,     â”‚ â”‚
â”‚ â”‚                             794,   5324,   1337,    794,    330, â”‚ â”‚
â”‚ â”‚                             928,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     32075,    534,   2122,     â”‚ â”‚
â”‚ â”‚                             25,   5324,    723,   3486,    794,  â”‚ â”‚
â”‚ â”‚                             330,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚      4155,    498,    330,     â”‚ â”‚
â”‚ â”‚                             39298,    688,    794,    330,       â”‚ â”‚
â”‚ â”‚                             8561,     37,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    25,   7979,  47861,     â”‚ â”‚
â”‚ â”‚                             498,    330,  35658,    794,         â”‚ â”‚
â”‚ â”‚                             220]],                               â”‚ â”‚
â”‚ â”‚                             â”‚      device='cuda:0')              â”‚ â”‚
â”‚ â”‚                is_prefill = False                                â”‚ â”‚
â”‚ â”‚          logits_processor = [                                    â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <jsonformer.logits_processors.Outpuâ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x14849782a690>,           â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <transformers.generation.logits_proâ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x1484977c8f10>,           â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <transformers.generation.logits_proâ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x148497807bd0>,           â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <transformers.generation.logits_proâ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x148497876b90>            â”‚ â”‚
â”‚ â”‚                             ]                                    â”‚ â”‚
â”‚ â”‚                max_length = 392                                  â”‚ â”‚
â”‚ â”‚             model_forward = <bound method                        â”‚ â”‚
â”‚ â”‚                             Module._wrapped_call_impl of         â”‚ â”‚
â”‚ â”‚                             LlamaForCausalLM(                    â”‚ â”‚
â”‚ â”‚                               (model): LlamaModel(               â”‚ â”‚
â”‚ â”‚                             â”‚   (embed_tokens):                  â”‚ â”‚
â”‚ â”‚                             Embedding(128256, 4096)              â”‚ â”‚
â”‚ â”‚                             â”‚   (layers): ModuleList(            â”‚ â”‚
â”‚ â”‚                             â”‚     (0-31): 32 x                   â”‚ â”‚
â”‚ â”‚                             LlamaDecoderLayer(                   â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (self_attn):                 â”‚ â”‚
â”‚ â”‚                             LlamaFlashAttention2(                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (q_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=4096, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (k_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=1024, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (v_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=1024, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (o_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=4096, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (rotary_emb):              â”‚ â”‚
â”‚ â”‚                             LlamaRotaryEmbedding()               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   )                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (mlp): LlamaMLP(             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (gate_proj):               â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=14336, bias=False)      â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (up_proj):                 â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=14336, bias=False)      â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (down_proj):               â”‚ â”‚
â”‚ â”‚                             Linear(in_features=14336,            â”‚ â”‚
â”‚ â”‚                             out_features=4096, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (act_fn): SiLU()           â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   )                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (input_layernorm):           â”‚ â”‚
â”‚ â”‚                             LlamaRMSNorm((4096,), eps=1e-05)     â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (post_attention_layernorm):  â”‚ â”‚
â”‚ â”‚                             LlamaRMSNorm((4096,), eps=1e-05)     â”‚ â”‚
â”‚ â”‚                             â”‚     )                              â”‚ â”‚
â”‚ â”‚                             â”‚   )                                â”‚ â”‚
â”‚ â”‚                             â”‚   (norm): LlamaRMSNorm((4096,),    â”‚ â”‚
â”‚ â”‚                             eps=1e-05)                           â”‚ â”‚
â”‚ â”‚                             â”‚   (rotary_emb):                    â”‚ â”‚
â”‚ â”‚                             LlamaRotaryEmbedding()               â”‚ â”‚
â”‚ â”‚                               )                                  â”‚ â”‚
â”‚ â”‚                               (lm_head):                         â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=128256, bias=False)     â”‚ â”‚
â”‚ â”‚                             )>                                   â”‚ â”‚
â”‚ â”‚              model_inputs = {                                    â”‚ â”‚
â”‚ â”‚                             â”‚   'cache_position': tensor([  0,   â”‚ â”‚
â”‚ â”‚                             1,   2,   3,   4,   5,   6,   7,     â”‚ â”‚
â”‚ â”‚                             8,   9,  10,  11,  12,  13,          â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    14,  15,  16,  17,  18,     â”‚ â”‚
â”‚ â”‚                             19,  20,  21,  22,  23,  24,  25,    â”‚ â”‚
â”‚ â”‚                             26,  27,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    28,  29,  30,  31,  32,     â”‚ â”‚
â”‚ â”‚                             33,  34,  35,  36,  37,  38,  39,    â”‚ â”‚
â”‚ â”‚                             40,  41,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    42,  43,  44,  45,  46,     â”‚ â”‚
â”‚ â”‚                             47,  48,  49,  50,  51,  52,  53,    â”‚ â”‚
â”‚ â”‚                             54,  55,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    56,  57,  58,  59,  60,     â”‚ â”‚
â”‚ â”‚                             61,  62,  63,  64,  65,  66,  67,    â”‚ â”‚
â”‚ â”‚                             68,  69,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    70,  71,  72,  73,  74,     â”‚ â”‚
â”‚ â”‚                             75,  76,  77,  78,  79,  80,  81,    â”‚ â”‚
â”‚ â”‚                             82,  83,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    84,  85,  86,  87,  88,     â”‚ â”‚
â”‚ â”‚                             89,  90,  91,  92,  93,  94,  95,    â”‚ â”‚
â”‚ â”‚                             96,  97,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    98,  99, 100, 101, 102,     â”‚ â”‚
â”‚ â”‚                             103, 104, 105, 106, 107, 108, 109,   â”‚ â”‚
â”‚ â”‚                             110, 111,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   112, 113, 114, 115, 116,     â”‚ â”‚
â”‚ â”‚                             117, 118, 119, 120, 121, 122, 123,   â”‚ â”‚
â”‚ â”‚                             124, 125,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   126, 127, 128, 129, 130,     â”‚ â”‚
â”‚ â”‚                             131, 132, 133, 134, 135, 136, 137,   â”‚ â”‚
â”‚ â”‚                             138, 139,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   140, 141, 142, 143, 144,     â”‚ â”‚
â”‚ â”‚                             145, 146, 147, 148, 149, 150, 151,   â”‚ â”‚
â”‚ â”‚                             152, 153,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   154, 155, 156, 157, 158,     â”‚ â”‚
â”‚ â”‚                             159, 160, 161, 162, 163, 164, 165,   â”‚ â”‚
â”‚ â”‚                             166, 167,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   168, 169, 170, 171, 172,     â”‚ â”‚
â”‚ â”‚                             173, 174, 175, 176, 177, 178, 179,   â”‚ â”‚
â”‚ â”‚                             180, 181,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   182, 183, 184, 185, 186,     â”‚ â”‚
â”‚ â”‚                             187, 188, 189, 190, 191, 192, 193,   â”‚ â”‚
â”‚ â”‚                             194, 195,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   196, 197, 198, 199, 200,     â”‚ â”‚
â”‚ â”‚                             201, 202, 203, 204, 205, 206, 207,   â”‚ â”‚
â”‚ â”‚                             208, 209,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   210, 211, 212, 213, 214,     â”‚ â”‚
â”‚ â”‚                             215, 216, 217, 218, 219, 220, 221,   â”‚ â”‚
â”‚ â”‚                             222, 223,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   224, 225, 226, 227, 228,     â”‚ â”‚
â”‚ â”‚                             229, 230, 231, 232, 233, 234, 235,   â”‚ â”‚
â”‚ â”‚                             236, 237,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   238, 239, 240, 241, 242,     â”‚ â”‚
â”‚ â”‚                             243, 244, 245, 246, 247, 248, 249,   â”‚ â”‚
â”‚ â”‚                             250, 251,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   252, 253, 254, 255, 256,     â”‚ â”‚
â”‚ â”‚                             257, 258, 259, 260, 261, 262, 263,   â”‚ â”‚
â”‚ â”‚                             264, 265,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   266, 267, 268, 269, 270,     â”‚ â”‚
â”‚ â”‚                             271, 272, 273, 274, 275, 276, 277,   â”‚ â”‚
â”‚ â”‚                             278, 279,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   280, 281, 282, 283, 284,     â”‚ â”‚
â”‚ â”‚                             285, 286, 287, 288, 289, 290, 291,   â”‚ â”‚
â”‚ â”‚                             292, 293,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   294, 295, 296, 297, 298,     â”‚ â”‚
â”‚ â”‚                             299, 300, 301, 302, 303, 304, 305,   â”‚ â”‚
â”‚ â”‚                             306, 307,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   308, 309, 310, 311, 312,     â”‚ â”‚
â”‚ â”‚                             313, 314, 315, 316, 317, 318, 319,   â”‚ â”‚
â”‚ â”‚                             320, 321,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   322, 323, 324, 325, 326,     â”‚ â”‚
â”‚ â”‚                             327, 328, 329, 330, 331, 332, 333,   â”‚ â”‚
â”‚ â”‚                             334, 335,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   336, 337, 338, 339, 340,     â”‚ â”‚
â”‚ â”‚                             341, 342, 343, 344, 345, 346, 347,   â”‚ â”‚
â”‚ â”‚                             348, 349,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   350, 351, 352, 353, 354,     â”‚ â”‚
â”‚ â”‚                             355, 356, 357, 358, 359, 360, 361,   â”‚ â”‚
â”‚ â”‚                             362, 363,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   364, 365, 366, 367, 368,     â”‚ â”‚
â”‚ â”‚                             369, 370, 371, 372, 373, 374, 375,   â”‚ â”‚
â”‚ â”‚                             376, 377,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   378, 379, 380, 381, 382,     â”‚ â”‚
â”‚ â”‚                             383, 384, 385], device='cuda:0'),    â”‚ â”‚
â”‚ â”‚                             â”‚   'past_key_values':               â”‚ â”‚
â”‚ â”‚                             DynamicCache(),                      â”‚ â”‚
â”‚ â”‚                             â”‚   'input_ids': tensor([[128000,    â”‚ â”‚
â”‚ â”‚                             198,   1556,   8671,     67,  27620, â”‚ â”‚
â”‚ â”‚                             57831,   5176,     17,               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   320,  19838,  14060,     â”‚ â”‚
â”‚ â”‚                             11,    220,     18,     13,    845,  â”‚ â”‚
â”‚ â”‚                             9653,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   337,      8,    323,     â”‚ â”‚
â”‚ â”‚                             473,     19,     43,     16,  14260, â”‚ â”‚
â”‚ â”‚                             21,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    13,     20,     39,     â”‚ â”‚
â”‚ â”‚                             17,     46,    320,  11057,  14060,  â”‚ â”‚
â”‚ â”‚                             11,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   220,     15,     13,     â”‚ â”‚
â”‚ â”‚                             1544,   9653,    337,    705,        â”‚ â”‚
â”‚ â”‚                             1051,  56767,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   304,    264,  21655,     â”‚ â”‚
â”‚ â”‚                             315,  20804,     37,    320,   1313, â”‚ â”‚
â”‚ â”‚                             13,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    20,  65170,      8,     â”‚ â”‚
â”‚ â”‚                             323,   2206,  47861,    320,   1313, â”‚ â”‚
â”‚ â”‚                             13,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    20,  65170,      8,     â”‚ â”‚
â”‚ â”‚                             4871,    264,    220,   1135,        â”‚ â”‚
â”‚ â”‚                             65170,   9168,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   348,    532,  19584,     â”‚ â”‚
â”‚ â”‚                             449,    264,    350,    830,  12490, â”‚ â”‚
â”‚ â”‚                             32393,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     22733,   2107,     13,     â”‚ â”‚
â”‚ â”‚                             578,   6425,    574,  32813,    369, â”‚ â”‚
â”‚ â”‚                             220,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    23,   2919,    520,     â”‚ â”‚
â”‚ â”‚                             220,   2031,  37386,     34,     13, â”‚ â”‚
â”‚ â”‚                             20902,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     65251,   1499,   3418,     â”‚ â”‚
â”‚ â”‚                             81,    599,    543,    483,  88751,  â”‚ â”‚
â”‚ â”‚                             988,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   315,   4661,   1933,     â”‚ â”‚
â”‚ â”‚                             1752,  48473,   1051,  54568,        â”‚ â”‚
â”‚ â”‚                             6041,    505,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   279,   2132,   1938,     â”‚ â”‚
â”‚ â”‚                             315,  24494,     13,    578,  39887, â”‚ â”‚
â”‚ â”‚                             266,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   519,     11,   8649,     â”‚ â”‚
â”‚ â”‚                             264,   9099,   3392,    315,   2536, â”‚ â”‚
â”‚ â”‚                             48689,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   599,    543,    483,     â”‚ â”‚
â”‚ â”‚                             16946,     11,    574,   1654,       â”‚ â”‚
â”‚ â”‚                             7719,    323,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   279,  64568,    483,     â”‚ â”‚
â”‚ â”‚                             36841,  20227,    574,   6288,       â”‚ â”‚
â”‚ â”‚                             23217,   1139,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   264,   5124,   2963,     â”‚ â”‚
â”‚ â”‚                             74,  61319,    449,    264,    282,  â”‚ â”‚
â”‚ â”‚                             1018,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   369,    279,  17876,     â”‚ â”‚
â”‚ â”‚                             28786,    323,  76038,   7677,       â”‚ â”‚
â”‚ â”‚                             1234,  81073,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚      6962,     13,   4427,     â”‚ â”‚
â”‚ â”‚                             9861,   2536,  48689,    599,        â”‚ â”‚
â”‚ â”‚                             543,    483,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     14933,    953,  19020,     â”‚ â”‚
â”‚ â”‚                             1051,  19180,    555,   9482,        â”‚ â”‚
â”‚ â”‚                             2518,    279,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     64568,    483,  36841,     â”‚ â”‚
â”‚ â”‚                             20227,   5361,   3115,    304,       â”‚ â”‚
â”‚ â”‚                             264,    220,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    16,     25,     16,     â”‚ â”‚
â”‚ â”‚                             21655,    315,  20804,     37,       â”‚ â”‚
â”‚ â”‚                             25,   7979,                          â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     47861,    323,  18054,     â”‚ â”‚
â”‚ â”‚                             279,  39887,    266,    519,    449, â”‚ â”‚
â”‚ â”‚                             264,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     58325,    324,  24547,     â”‚ â”‚
â”‚ â”‚                             6672,   8272,    555,   2033,        â”‚ â”‚
â”‚ â”‚                             97139,    287,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   315,    279,  16946,     â”‚ â”‚
â”‚ â”‚                             449,    279,   1890,  69996,  21655, â”‚ â”‚
â”‚ â”‚                             323,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     84878,  76038,     13,     â”‚ â”‚
â”‚ â”‚                             3804,  72457,  25402,  46479,        â”‚ â”‚
â”‚ â”‚                             304,   9467,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     24012,    320,    605,     â”‚ â”‚
â”‚ â”‚                             34363,     17,   8611,     81,       â”‚ â”‚
â”‚ â”‚                             8,    520,                           â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   436,    739,     13,     â”‚ â”‚
â”‚ â”‚                             58487,    220,  13384,  14060,       â”‚ â”‚
â”‚ â”‚                             320,   5313,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚     4,   3196,    389,     â”‚ â”‚
â”‚ â”‚                             473,     19,     43,     16,  14260, â”‚ â”‚
â”‚ â”‚                             21,                                  â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    13,     20,     39,     â”‚ â”‚
â”‚ â”‚                             17,     46,      8,    315,   4661,  â”‚ â”‚
â”‚ â”‚                             1933,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚      1752,  64568,    483,     â”‚ â”‚
â”‚ â”‚                             2027,     13,    720,   5207,        â”‚ â”‚
â”‚ â”‚                             1121,    304,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   279,   2768,   4823,     â”‚ â”‚
â”‚ â”‚                             11036,   3645,    512,   5018,       â”‚ â”‚
â”‚ â”‚                             1337,    794,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   330,   1735,    498,     â”‚ â”‚
â”‚ â”‚                             330,  13495,    794,   5324,    723, â”‚ â”‚
â”‚ â”‚                             3486,                                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   794,   5324,   1337,     â”‚ â”‚
â”‚ â”‚                             794,    330,    928,  14682,    330, â”‚ â”‚
â”‚ â”‚                             39298,                               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   688,    794,   5324,     â”‚ â”‚
â”‚ â”‚                             1337,    794,    330,    928,        â”‚ â”‚
â”‚ â”‚                             14682,    330,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     35658,    794,   5324,     â”‚ â”‚
â”‚ â”‚                             1337,    794,    330,   4174,        â”‚ â”‚
â”‚ â”‚                             14682,    330,                       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     35658,  15176,    794,     â”‚ â”‚
â”‚ â”‚                             5324,   1337,    794,    330,        â”‚ â”‚
â”‚ â”‚                             928,  14682,                         â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   330,   1712,    794,     â”‚ â”‚
â”‚ â”‚                             5324,   1337,    794,    330,        â”‚ â”‚
â”‚ â”‚                             4174,  14682,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚   330,   1712,  15176,     â”‚ â”‚
â”‚ â”‚                             794,   5324,   1337,    794,    330, â”‚ â”‚
â”‚ â”‚                             928,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     32075,    534,   2122,     â”‚ â”‚
â”‚ â”‚                             25,   5324,    723,   3486,    794,  â”‚ â”‚
â”‚ â”‚                             330,                                 â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚      4155,    498,    330,     â”‚ â”‚
â”‚ â”‚                             39298,    688,    794,    330,       â”‚ â”‚
â”‚ â”‚                             8561,     37,                        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   â”‚    25,   7979,  47861,     â”‚ â”‚
â”‚ â”‚                             498,    330,  35658,    794,         â”‚ â”‚
â”‚ â”‚                             220]],                               â”‚ â”‚
â”‚ â”‚                             â”‚      device='cuda:0'),             â”‚ â”‚
â”‚ â”‚                             â”‚   'inputs_embeds': None,           â”‚ â”‚
â”‚ â”‚                             â”‚   'position_ids': tensor([[  0,    â”‚ â”‚
â”‚ â”‚                             1,   2,   3,   4,   5,   6,   7,     â”‚ â”‚
â”‚ â”‚                             8,   9,  10,  11,  12,  13,          â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     14,  15,  16,  17,  18,    â”‚ â”‚
â”‚ â”‚                             19,  20,  21,  22,  23,  24,  25,    â”‚ â”‚
â”‚ â”‚                             26,  27,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     28,  29,  30,  31,  32,    â”‚ â”‚
â”‚ â”‚                             33,  34,  35,  36,  37,  38,  39,    â”‚ â”‚
â”‚ â”‚                             40,  41,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     42,  43,  44,  45,  46,    â”‚ â”‚
â”‚ â”‚                             47,  48,  49,  50,  51,  52,  53,    â”‚ â”‚
â”‚ â”‚                             54,  55,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     56,  57,  58,  59,  60,    â”‚ â”‚
â”‚ â”‚                             61,  62,  63,  64,  65,  66,  67,    â”‚ â”‚
â”‚ â”‚                             68,  69,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     70,  71,  72,  73,  74,    â”‚ â”‚
â”‚ â”‚                             75,  76,  77,  78,  79,  80,  81,    â”‚ â”‚
â”‚ â”‚                             82,  83,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     84,  85,  86,  87,  88,    â”‚ â”‚
â”‚ â”‚                             89,  90,  91,  92,  93,  94,  95,    â”‚ â”‚
â”‚ â”‚                             96,  97,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     98,  99, 100, 101, 102,    â”‚ â”‚
â”‚ â”‚                             103, 104, 105, 106, 107, 108, 109,   â”‚ â”‚
â”‚ â”‚                             110, 111,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    112, 113, 114, 115, 116,    â”‚ â”‚
â”‚ â”‚                             117, 118, 119, 120, 121, 122, 123,   â”‚ â”‚
â”‚ â”‚                             124, 125,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    126, 127, 128, 129, 130,    â”‚ â”‚
â”‚ â”‚                             131, 132, 133, 134, 135, 136, 137,   â”‚ â”‚
â”‚ â”‚                             138, 139,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    140, 141, 142, 143, 144,    â”‚ â”‚
â”‚ â”‚                             145, 146, 147, 148, 149, 150, 151,   â”‚ â”‚
â”‚ â”‚                             152, 153,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    154, 155, 156, 157, 158,    â”‚ â”‚
â”‚ â”‚                             159, 160, 161, 162, 163, 164, 165,   â”‚ â”‚
â”‚ â”‚                             166, 167,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    168, 169, 170, 171, 172,    â”‚ â”‚
â”‚ â”‚                             173, 174, 175, 176, 177, 178, 179,   â”‚ â”‚
â”‚ â”‚                             180, 181,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    182, 183, 184, 185, 186,    â”‚ â”‚
â”‚ â”‚                             187, 188, 189, 190, 191, 192, 193,   â”‚ â”‚
â”‚ â”‚                             194, 195,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    196, 197, 198, 199, 200,    â”‚ â”‚
â”‚ â”‚                             201, 202, 203, 204, 205, 206, 207,   â”‚ â”‚
â”‚ â”‚                             208, 209,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    210, 211, 212, 213, 214,    â”‚ â”‚
â”‚ â”‚                             215, 216, 217, 218, 219, 220, 221,   â”‚ â”‚
â”‚ â”‚                             222, 223,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    224, 225, 226, 227, 228,    â”‚ â”‚
â”‚ â”‚                             229, 230, 231, 232, 233, 234, 235,   â”‚ â”‚
â”‚ â”‚                             236, 237,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    238, 239, 240, 241, 242,    â”‚ â”‚
â”‚ â”‚                             243, 244, 245, 246, 247, 248, 249,   â”‚ â”‚
â”‚ â”‚                             250, 251,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    252, 253, 254, 255, 256,    â”‚ â”‚
â”‚ â”‚                             257, 258, 259, 260, 261, 262, 263,   â”‚ â”‚
â”‚ â”‚                             264, 265,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    266, 267, 268, 269, 270,    â”‚ â”‚
â”‚ â”‚                             271, 272, 273, 274, 275, 276, 277,   â”‚ â”‚
â”‚ â”‚                             278, 279,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    280, 281, 282, 283, 284,    â”‚ â”‚
â”‚ â”‚                             285, 286, 287, 288, 289, 290, 291,   â”‚ â”‚
â”‚ â”‚                             292, 293,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    294, 295, 296, 297, 298,    â”‚ â”‚
â”‚ â”‚                             299, 300, 301, 302, 303, 304, 305,   â”‚ â”‚
â”‚ â”‚                             306, 307,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    308, 309, 310, 311, 312,    â”‚ â”‚
â”‚ â”‚                             313, 314, 315, 316, 317, 318, 319,   â”‚ â”‚
â”‚ â”‚                             320, 321,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    322, 323, 324, 325, 326,    â”‚ â”‚
â”‚ â”‚                             327, 328, 329, 330, 331, 332, 333,   â”‚ â”‚
â”‚ â”‚                             334, 335,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    336, 337, 338, 339, 340,    â”‚ â”‚
â”‚ â”‚                             341, 342, 343, 344, 345, 346, 347,   â”‚ â”‚
â”‚ â”‚                             348, 349,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    350, 351, 352, 353, 354,    â”‚ â”‚
â”‚ â”‚                             355, 356, 357, 358, 359, 360, 361,   â”‚ â”‚
â”‚ â”‚                             362, 363,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    364, 365, 366, 367, 368,    â”‚ â”‚
â”‚ â”‚                             369, 370, 371, 372, 373, 374, 375,   â”‚ â”‚
â”‚ â”‚                             376, 377,                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    378, 379, 380, 381, 382,    â”‚ â”‚
â”‚ â”‚                             383, 384, 385]], device='cuda:0'),   â”‚ â”‚
â”‚ â”‚                             â”‚   'attention_mask': tensor([[1, 1, â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1]], device='cuda:0'),   â”‚ â”‚
â”‚ â”‚                             â”‚   'num_logits_to_keep': 1,         â”‚ â”‚
â”‚ â”‚                             â”‚   'use_cache': True                â”‚ â”‚
â”‚ â”‚                             }                                    â”‚ â”‚
â”‚ â”‚              model_kwargs = {                                    â”‚ â”‚
â”‚ â”‚                             â”‚   'attention_mask': tensor([[1, 1, â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1,        â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  â”‚ â”‚
â”‚ â”‚                             1, 1, 1,                             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚    1, 1, 1]],                  â”‚ â”‚
â”‚ â”‚                             device='cuda:0'),                    â”‚ â”‚
â”‚ â”‚                             â”‚   'num_logits_to_keep': 1,         â”‚ â”‚
â”‚ â”‚                             â”‚   'past_key_values':               â”‚ â”‚
â”‚ â”‚                             DynamicCache(),                      â”‚ â”‚
â”‚ â”‚                             â”‚   'use_cache': True,               â”‚ â”‚
â”‚ â”‚                             â”‚   'cache_position': tensor([386],  â”‚ â”‚
â”‚ â”‚                             device='cuda:0')                     â”‚ â”‚
â”‚ â”‚                             }                                    â”‚ â”‚
â”‚ â”‚         next_token_logits = tensor([[ 7.1914,  7.8555,  8.5234,  â”‚ â”‚
â”‚ â”‚                             ..., -2.7402, -2.7402, -2.7402]],    â”‚ â”‚
â”‚ â”‚                             â”‚      device='cuda:0')              â”‚ â”‚
â”‚ â”‚         output_attentions = False                                â”‚ â”‚
â”‚ â”‚      output_hidden_states = False                                â”‚ â”‚
â”‚ â”‚             output_logits = None                                 â”‚ â”‚
â”‚ â”‚             output_scores = False                                â”‚ â”‚
â”‚ â”‚                   outputs = CausalLMOutputWithPast(              â”‚ â”‚
â”‚ â”‚                             â”‚   loss=None,                       â”‚ â”‚
â”‚ â”‚                             â”‚   logits=tensor([[[ 7.1914,        â”‚ â”‚
â”‚ â”‚                             7.8555,  8.5234,  ..., -2.7402,      â”‚ â”‚
â”‚ â”‚                             -2.7402, -2.7402]]],                 â”‚ â”‚
â”‚ â”‚                             â”‚      device='cuda:0',              â”‚ â”‚
â”‚ â”‚                             dtype=torch.float16),                â”‚ â”‚
â”‚ â”‚                             â”‚   past_key_values=DynamicCache(),  â”‚ â”‚
â”‚ â”‚                             â”‚   hidden_states=None,              â”‚ â”‚
â”‚ â”‚                             â”‚   attentions=None                  â”‚ â”‚
â”‚ â”‚                             )                                    â”‚ â”‚
â”‚ â”‚              pad_token_id = tensor(128009, device='cuda:0')      â”‚ â”‚
â”‚ â”‚                raw_logits = None                                 â”‚ â”‚
â”‚ â”‚   return_dict_in_generate = False                                â”‚ â”‚
â”‚ â”‚                    scores = None                                 â”‚ â”‚
â”‚ â”‚                      self = LlamaForCausalLM(                    â”‚ â”‚
â”‚ â”‚                               (model): LlamaModel(               â”‚ â”‚
â”‚ â”‚                             â”‚   (embed_tokens):                  â”‚ â”‚
â”‚ â”‚                             Embedding(128256, 4096)              â”‚ â”‚
â”‚ â”‚                             â”‚   (layers): ModuleList(            â”‚ â”‚
â”‚ â”‚                             â”‚     (0-31): 32 x                   â”‚ â”‚
â”‚ â”‚                             LlamaDecoderLayer(                   â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (self_attn):                 â”‚ â”‚
â”‚ â”‚                             LlamaFlashAttention2(                â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (q_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=4096, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (k_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=1024, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (v_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=1024, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (o_proj):                  â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=4096, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (rotary_emb):              â”‚ â”‚
â”‚ â”‚                             LlamaRotaryEmbedding()               â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   )                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (mlp): LlamaMLP(             â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (gate_proj):               â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=14336, bias=False)      â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (up_proj):                 â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=14336, bias=False)      â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (down_proj):               â”‚ â”‚
â”‚ â”‚                             Linear(in_features=14336,            â”‚ â”‚
â”‚ â”‚                             out_features=4096, bias=False)       â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚     (act_fn): SiLU()           â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   )                            â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (input_layernorm):           â”‚ â”‚
â”‚ â”‚                             LlamaRMSNorm((4096,), eps=1e-05)     â”‚ â”‚
â”‚ â”‚                             â”‚   â”‚   (post_attention_layernorm):  â”‚ â”‚
â”‚ â”‚                             LlamaRMSNorm((4096,), eps=1e-05)     â”‚ â”‚
â”‚ â”‚                             â”‚     )                              â”‚ â”‚
â”‚ â”‚                             â”‚   )                                â”‚ â”‚
â”‚ â”‚                             â”‚   (norm): LlamaRMSNorm((4096,),    â”‚ â”‚
â”‚ â”‚                             eps=1e-05)                           â”‚ â”‚
â”‚ â”‚                             â”‚   (rotary_emb):                    â”‚ â”‚
â”‚ â”‚                             LlamaRotaryEmbedding()               â”‚ â”‚
â”‚ â”‚                               )                                  â”‚ â”‚
â”‚ â”‚                               (lm_head):                         â”‚ â”‚
â”‚ â”‚                             Linear(in_features=4096,             â”‚ â”‚
â”‚ â”‚                             out_features=128256, bias=False)     â”‚ â”‚
â”‚ â”‚                             )                                    â”‚ â”‚
â”‚ â”‚         stopping_criteria = [                                    â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <transformers.generation.stopping_câ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x148497876e10>,           â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <transformers.generation.stopping_câ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x14849782bdd0>,           â”‚ â”‚
â”‚ â”‚                             â”‚                                    â”‚ â”‚
â”‚ â”‚                             <jsonformer.logits_processors.Numbeâ€¦ â”‚ â”‚
â”‚ â”‚                             object at 0x148499999c10>            â”‚ â”‚
â”‚ â”‚                             ]                                    â”‚ â”‚
â”‚ â”‚                  streamer = None                                 â”‚ â”‚
â”‚ â”‚               synced_gpus = False                                â”‚ â”‚
â”‚ â”‚        this_peer_finished = False                                â”‚ â”‚
â”‚ â”‚      unfinished_sequences = tensor([1], device='cuda:0')         â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/transformers/generation/logits_process.py:104 in        â”‚
â”‚ __call__                                                             â”‚
â”‚                                                                      â”‚
â”‚    101 â”‚   â”‚   â”‚   â”‚   â”‚   )                                         â”‚
â”‚    102 â”‚   â”‚   â”‚   â”‚   scores = processor(input_ids, scores, **kwarg â”‚
â”‚    103 â”‚   â”‚   â”‚   else:                                             â”‚
â”‚ â±  104 â”‚   â”‚   â”‚   â”‚   scores = processor(input_ids, scores)         â”‚
â”‚    105 â”‚   â”‚                                                         â”‚
â”‚    106 â”‚   â”‚   return scores                                         â”‚
â”‚    107                                                               â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚ function_args = mappingproxy({                                   â”‚ â”‚
â”‚ â”‚                 â”‚   '_': <Parameter "_">,                        â”‚ â”‚
â”‚ â”‚                 â”‚   'scores': <Parameter "scores">               â”‚ â”‚
â”‚ â”‚                 })                                               â”‚ â”‚
â”‚ â”‚     input_ids = tensor([[128000,    198,   1556,   8671,     67, â”‚ â”‚
â”‚ â”‚                 27620,  57831,   5176,     17,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   320,  19838,  14060,     11,    220, â”‚ â”‚
â”‚ â”‚                 18,     13,    845,   9653,                      â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   337,      8,    323,    473,     19, â”‚ â”‚
â”‚ â”‚                 43,     16,  14260,     21,                      â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    13,     20,     39,     17,     46, â”‚ â”‚
â”‚ â”‚                 320,  11057,  14060,     11,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   220,     15,     13,   1544,   9653, â”‚ â”‚
â”‚ â”‚                 337,    705,   1051,  56767,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   304,    264,  21655,    315,  20804, â”‚ â”‚
â”‚ â”‚                 37,    320,   1313,     13,                      â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    20,  65170,      8,    323,   2206, â”‚ â”‚
â”‚ â”‚                 47861,    320,   1313,     13,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    20,  65170,      8,   4871,    264, â”‚ â”‚
â”‚ â”‚                 220,   1135,  65170,   9168,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   348,    532,  19584,    449,    264, â”‚ â”‚
â”‚ â”‚                 350,    830,  12490,  32393,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     22733,   2107,     13,    578,   6425, â”‚ â”‚
â”‚ â”‚                 574,  32813,    369,    220,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    23,   2919,    520,    220,   2031, â”‚ â”‚
â”‚ â”‚                 37386,     34,     13,  20902,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     65251,   1499,   3418,     81,    599, â”‚ â”‚
â”‚ â”‚                 543,    483,  88751,    988,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   315,   4661,   1933,   1752,  48473, â”‚ â”‚
â”‚ â”‚                 1051,  54568,   6041,    505,                    â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   279,   2132,   1938,    315,  24494, â”‚ â”‚
â”‚ â”‚                 13,    578,  39887,    266,                      â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   519,     11,   8649,    264,   9099, â”‚ â”‚
â”‚ â”‚                 3392,    315,   2536,  48689,                    â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   599,    543,    483,  16946,     11, â”‚ â”‚
â”‚ â”‚                 574,   1654,   7719,    323,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   279,  64568,    483,  36841,  20227, â”‚ â”‚
â”‚ â”‚                 574,   6288,  23217,   1139,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   264,   5124,   2963,     74,  61319, â”‚ â”‚
â”‚ â”‚                 449,    264,    282,   1018,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   369,    279,  17876,  28786,    323, â”‚ â”‚
â”‚ â”‚                 76038,   7677,   1234,  81073,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚      6962,     13,   4427,   9861,   2536, â”‚ â”‚
â”‚ â”‚                 48689,    599,    543,    483,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     14933,    953,  19020,   1051,  19180, â”‚ â”‚
â”‚ â”‚                 555,   9482,   2518,    279,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     64568,    483,  36841,  20227,   5361, â”‚ â”‚
â”‚ â”‚                 3115,    304,    264,    220,                    â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    16,     25,     16,  21655,    315, â”‚ â”‚
â”‚ â”‚                 20804,     37,     25,   7979,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     47861,    323,  18054,    279,  39887, â”‚ â”‚
â”‚ â”‚                 266,    519,    449,    264,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     58325,    324,  24547,   6672,   8272, â”‚ â”‚
â”‚ â”‚                 555,   2033,  97139,    287,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   315,    279,  16946,    449,    279, â”‚ â”‚
â”‚ â”‚                 1890,  69996,  21655,    323,                    â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     84878,  76038,     13,   3804,  72457, â”‚ â”‚
â”‚ â”‚                 25402,  46479,    304,   9467,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     24012,    320,    605,  34363,     17, â”‚ â”‚
â”‚ â”‚                 8611,     81,      8,    520,                    â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   436,    739,     13,  58487,    220, â”‚ â”‚
â”‚ â”‚                 13384,  14060,    320,   5313,                   â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚     4,   3196,    389,    473,     19, â”‚ â”‚
â”‚ â”‚                 43,     16,  14260,     21,                      â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    13,     20,     39,     17,     46, â”‚ â”‚
â”‚ â”‚                 8,    315,   4661,   1933,                       â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚      1752,  64568,    483,   2027,     13, â”‚ â”‚
â”‚ â”‚                 720,   5207,   1121,    304,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   279,   2768,   4823,  11036,   3645, â”‚ â”‚
â”‚ â”‚                 512,   5018,   1337,    794,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   330,   1735,    498,    330,  13495, â”‚ â”‚
â”‚ â”‚                 794,   5324,    723,   3486,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   794,   5324,   1337,    794,    330, â”‚ â”‚
â”‚ â”‚                 928,  14682,    330,  39298,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   688,    794,   5324,   1337,    794, â”‚ â”‚
â”‚ â”‚                 330,    928,  14682,    330,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     35658,    794,   5324,   1337,    794, â”‚ â”‚
â”‚ â”‚                 330,   4174,  14682,    330,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     35658,  15176,    794,   5324,   1337, â”‚ â”‚
â”‚ â”‚                 794,    330,    928,  14682,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   330,   1712,    794,   5324,   1337, â”‚ â”‚
â”‚ â”‚                 794,    330,   4174,  14682,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚   330,   1712,  15176,    794,   5324, â”‚ â”‚
â”‚ â”‚                 1337,    794,    330,    928,                    â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚     32075,    534,   2122,     25,   5324, â”‚ â”‚
â”‚ â”‚                 723,   3486,    794,    330,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚      4155,    498,    330,  39298,    688, â”‚ â”‚
â”‚ â”‚                 794,    330,   8561,     37,                     â”‚ â”‚
â”‚ â”‚                 â”‚   â”‚   â”‚    25,   7979,  47861,    498,    330, â”‚ â”‚
â”‚ â”‚                 35658,    794,    220]],                         â”‚ â”‚
â”‚ â”‚                 â”‚      device='cuda:0')                          â”‚ â”‚
â”‚ â”‚        kwargs = {}                                               â”‚ â”‚
â”‚ â”‚     processor = <jsonformer.logits_processors.OutputNumbersTokeâ€¦ â”‚ â”‚
â”‚ â”‚                 object at 0x14849782a690>                        â”‚ â”‚
â”‚ â”‚        scores = tensor([[ 7.1914,  7.8555,  8.5234,  ...,        â”‚ â”‚
â”‚ â”‚                 -2.7402, -2.7402, -2.7402]],                     â”‚ â”‚
â”‚ â”‚                 â”‚      device='cuda:0')                          â”‚ â”‚
â”‚ â”‚          self = [                                                â”‚ â”‚
â”‚ â”‚                 â”‚                                                â”‚ â”‚
â”‚ â”‚                 <jsonformer.logits_processors.OutputNumbersTokeâ€¦ â”‚ â”‚
â”‚ â”‚                 object at 0x14849782a690>,                       â”‚ â”‚
â”‚ â”‚                 â”‚                                                â”‚ â”‚
â”‚ â”‚                 <transformers.generation.logits_process.Temperaâ€¦ â”‚ â”‚
â”‚ â”‚                 object at 0x1484977c8f10>,                       â”‚ â”‚
â”‚ â”‚                 â”‚                                                â”‚ â”‚
â”‚ â”‚                 <transformers.generation.logits_process.TopKLogâ€¦ â”‚ â”‚
â”‚ â”‚                 object at 0x148497807bd0>,                       â”‚ â”‚
â”‚ â”‚                 â”‚                                                â”‚ â”‚
â”‚ â”‚                 <transformers.generation.logits_process.TopPLogâ€¦ â”‚ â”‚
â”‚ â”‚                 object at 0x148497876b90>                        â”‚ â”‚
â”‚ â”‚                 ]                                                â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â”‚                                                                      â”‚
â”‚ /home/iti/zn2950/miniconda3/envs/llm-extraction-phi/lib/python3.11/s â”‚
â”‚ ite-packages/jsonformer/logits_processors.py:81 in __call__          â”‚
â”‚                                                                      â”‚
â”‚   78 â”‚   â”‚   â”‚   â”‚   self.allowed_mask[token_id] = True              â”‚
â”‚   79 â”‚                                                               â”‚
â”‚   80 â”‚   def __call__(self, _, scores):                              â”‚
â”‚ â± 81 â”‚   â”‚   mask = self.allowed_mask.expand_as(scores)              â”‚
â”‚   82 â”‚   â”‚   scores[~mask] = -float("inf")                           â”‚
â”‚   83 â”‚   â”‚                                                           â”‚
â”‚   84 â”‚   â”‚   return scores                                           â”‚
â”‚                                                                      â”‚
â”‚ â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ locals â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•® â”‚
â”‚ â”‚      _ = tensor([[128000,    198,   1556,   8671,     67,        â”‚ â”‚
â”‚ â”‚          27620,  57831,   5176,     17,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   320,  19838,  14060,     11,    220,        â”‚ â”‚
â”‚ â”‚          18,     13,    845,   9653,                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   337,      8,    323,    473,     19,        â”‚ â”‚
â”‚ â”‚          43,     16,  14260,     21,                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    13,     20,     39,     17,     46,        â”‚ â”‚
â”‚ â”‚          320,  11057,  14060,     11,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   220,     15,     13,   1544,   9653,        â”‚ â”‚
â”‚ â”‚          337,    705,   1051,  56767,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   304,    264,  21655,    315,  20804,        â”‚ â”‚
â”‚ â”‚          37,    320,   1313,     13,                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    20,  65170,      8,    323,   2206,        â”‚ â”‚
â”‚ â”‚          47861,    320,   1313,     13,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    20,  65170,      8,   4871,    264,        â”‚ â”‚
â”‚ â”‚          220,   1135,  65170,   9168,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   348,    532,  19584,    449,    264,        â”‚ â”‚
â”‚ â”‚          350,    830,  12490,  32393,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     22733,   2107,     13,    578,   6425,        â”‚ â”‚
â”‚ â”‚          574,  32813,    369,    220,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    23,   2919,    520,    220,   2031,        â”‚ â”‚
â”‚ â”‚          37386,     34,     13,  20902,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     65251,   1499,   3418,     81,    599,        â”‚ â”‚
â”‚ â”‚          543,    483,  88751,    988,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   315,   4661,   1933,   1752,  48473,        â”‚ â”‚
â”‚ â”‚          1051,  54568,   6041,    505,                           â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   279,   2132,   1938,    315,  24494,        â”‚ â”‚
â”‚ â”‚          13,    578,  39887,    266,                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   519,     11,   8649,    264,   9099,        â”‚ â”‚
â”‚ â”‚          3392,    315,   2536,  48689,                           â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   599,    543,    483,  16946,     11,        â”‚ â”‚
â”‚ â”‚          574,   1654,   7719,    323,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   279,  64568,    483,  36841,  20227,        â”‚ â”‚
â”‚ â”‚          574,   6288,  23217,   1139,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   264,   5124,   2963,     74,  61319,        â”‚ â”‚
â”‚ â”‚          449,    264,    282,   1018,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   369,    279,  17876,  28786,    323,        â”‚ â”‚
â”‚ â”‚          76038,   7677,   1234,  81073,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚      6962,     13,   4427,   9861,   2536,        â”‚ â”‚
â”‚ â”‚          48689,    599,    543,    483,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     14933,    953,  19020,   1051,  19180,        â”‚ â”‚
â”‚ â”‚          555,   9482,   2518,    279,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     64568,    483,  36841,  20227,   5361,        â”‚ â”‚
â”‚ â”‚          3115,    304,    264,    220,                           â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    16,     25,     16,  21655,    315,        â”‚ â”‚
â”‚ â”‚          20804,     37,     25,   7979,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     47861,    323,  18054,    279,  39887,        â”‚ â”‚
â”‚ â”‚          266,    519,    449,    264,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     58325,    324,  24547,   6672,   8272,        â”‚ â”‚
â”‚ â”‚          555,   2033,  97139,    287,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   315,    279,  16946,    449,    279,        â”‚ â”‚
â”‚ â”‚          1890,  69996,  21655,    323,                           â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     84878,  76038,     13,   3804,  72457,        â”‚ â”‚
â”‚ â”‚          25402,  46479,    304,   9467,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     24012,    320,    605,  34363,     17,        â”‚ â”‚
â”‚ â”‚          8611,     81,      8,    520,                           â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   436,    739,     13,  58487,    220,        â”‚ â”‚
â”‚ â”‚          13384,  14060,    320,   5313,                          â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚     4,   3196,    389,    473,     19,        â”‚ â”‚
â”‚ â”‚          43,     16,  14260,     21,                             â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    13,     20,     39,     17,     46,        â”‚ â”‚
â”‚ â”‚          8,    315,   4661,   1933,                              â”‚ â”‚
â”‚ â”‚          â”‚   â”‚      1752,  64568,    483,   2027,     13,        â”‚ â”‚
â”‚ â”‚          720,   5207,   1121,    304,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   279,   2768,   4823,  11036,   3645,        â”‚ â”‚
â”‚ â”‚          512,   5018,   1337,    794,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   330,   1735,    498,    330,  13495,        â”‚ â”‚
â”‚ â”‚          794,   5324,    723,   3486,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   794,   5324,   1337,    794,    330,        â”‚ â”‚
â”‚ â”‚          928,  14682,    330,  39298,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   688,    794,   5324,   1337,    794,        â”‚ â”‚
â”‚ â”‚          330,    928,  14682,    330,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     35658,    794,   5324,   1337,    794,        â”‚ â”‚
â”‚ â”‚          330,   4174,  14682,    330,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     35658,  15176,    794,   5324,   1337,        â”‚ â”‚
â”‚ â”‚          794,    330,    928,  14682,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   330,   1712,    794,   5324,   1337,        â”‚ â”‚
â”‚ â”‚          794,    330,   4174,  14682,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚   330,   1712,  15176,    794,   5324,        â”‚ â”‚
â”‚ â”‚          1337,    794,    330,    928,                           â”‚ â”‚
â”‚ â”‚          â”‚   â”‚     32075,    534,   2122,     25,   5324,        â”‚ â”‚
â”‚ â”‚          723,   3486,    794,    330,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚      4155,    498,    330,  39298,    688,        â”‚ â”‚
â”‚ â”‚          794,    330,   8561,     37,                            â”‚ â”‚
â”‚ â”‚          â”‚   â”‚   â”‚    25,   7979,  47861,    498,    330,        â”‚ â”‚
â”‚ â”‚          35658,    794,    220]],                                â”‚ â”‚
â”‚ â”‚          â”‚      device='cuda:0')                                 â”‚ â”‚
â”‚ â”‚ scores = tensor([[ 7.1914,  7.8555,  8.5234,  ..., -2.7402,      â”‚ â”‚
â”‚ â”‚          -2.7402, -2.7402]],                                     â”‚ â”‚
â”‚ â”‚          â”‚      device='cuda:0')                                 â”‚ â”‚
â”‚ â”‚   self = <jsonformer.logits_processors.OutputNumbersTokens       â”‚ â”‚
â”‚ â”‚          object at 0x14849782a690>                               â”‚ â”‚
â”‚ â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯ â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
RuntimeError: The expanded size of the tensor (128256) must match the 
existing size (128309) at non-singleton dimension 1.  Target sizes: [1, 
128256].  Tensor sizes: [128309]
======================128309=======================================
