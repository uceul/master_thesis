2025-01-06 20:14:17,419 INFO     Loading settings and stats
2025-01-06 20:14:17,420 INFO     Using prompt: Extract these synthesis conditions from the following Metal-Organic Framework (MOF) synthesis description: temperature (highest reaction temp, use 25Â°C if not specified), time (longest duration at highest temp), choose one main solvent (no mixtures or ratios), choose one chemical additive (write 'None' if no additive present). Never use JSON formatting like curly braces, escape characters, or newlines in your responses. Answers should be plain text values only., temperature: 0.0
2025-01-06 20:14:17,422 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-06 20:14:17,423 WARNING  [Errno 2] No such file or directory: 'stats_llama_31_instruction_full_ft_with_prompt_epoch_2_20250106_201413.yml'
2025-01-06 20:14:17,423 WARNING  [Errno 2] No such file or directory: 'stats_llama_31_instruction_full_ft_with_prompt_epoch_2_20250106_201413.yaml'
2025-01-06 20:14:17,423 WARNING  Could not load 'stats_llama_31_instruction_full_ft_with_prompt_epoch_2_20250106_201413.yml', creating it (empty)
2025-01-06 20:14:17,426 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-06 20:14:17,431 INFO     Found 778 paragraphs with labels
2025-01-06 20:14:17,431 DEBUG    First few valid IDs: ['VIDJID_clean', 'XASNUD_clean', 'NOQLOV01_charged', 'ZOFZUR_clean', 'ONODOM_clean']
2025-01-06 20:14:17,431 INFO     Loading evaluation set instead of regular dataset.
2025-01-06 20:14:17,431 DEBUG    Already evaluated: 0 items
2025-01-06 20:14:17,431 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs
2025-01-06 20:14:17,432 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs'
2025-01-06 20:14:17,435 INFO     Dataset loaded with 77 items
2025-01-06 20:14:17,435 INFO     Found 77 paragraphs that have labels in dataset
2025-01-06 20:14:17,436 DEBUG    First few overlapping IDs: ['XASPEP_clean', 'OHIHET_clean', 'LOMMIL_clean', 'GEVYEN_clean', 'LUMZEA_clean']
2025-01-06 20:14:17,436 INFO     Processing model: LLaMa 3.1 8B
2025-01-06 20:14:17,436 INFO     Skipping model [LLaMa 3.1 8B]
2025-01-06 20:14:17,436 INFO     Processing model: LLaMa 3.1 8B Instruct
2025-01-06 20:14:17,437 INFO       0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.1 8B Instruct]
2025-01-06 20:14:17,437 INFO     Loading Model [LLaMa 3.1 8B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama31_8b/instruction_full_ft/run_1_1736114112/epoch_2
2025-01-06 20:14:18,093 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
  0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.1 8B Instruct]

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][A
Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.14s/it][A
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:08<00:08,  4.13s/it][A
Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:12<00:04,  4.08s/it][A
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  2.88s/it][ALoading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.33s/it]
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-06 20:14:33,783 INFO     Saving progress to `stats_llama_31_instruction_full_ft_with_prompt_epoch_2_20250106_201413.yml`
model kwargs: {'torch_dtype': torch.float16, 'load_in_8bit': False, 'trust_remote_code': True, 'device_map': 'auto', 'do_sample': False}
EOS_TOKEN: 128009
PAD_TOKEN: None
[128000, 9906, 11, 856, 5679, 374, 19369]
