2025-01-07 18:13:52,949 INFO     Loading settings and stats
2025-01-07 18:13:52,949 INFO     Using prompt: Read the following Metal-Organic Framework (MOF) synthesis description and extract this information: temperature (highest reaction temp, use 25°C if not specified), time (longest duration at highest temp), one main solvent (no mixtures or ratios), one chemical additive ('None' if no additive present). Important: Do not use JSON or curly braces in your output, they are already provided for you and you do not need to generate them. Only output the extracted information and terminate strings with ", temperature: 0.0
2025-01-07 18:13:52,952 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:13:52,952 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_2_20250107_181348.yml'
2025-01-07 18:13:52,952 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_2_20250107_181348.yaml'
2025-01-07 18:13:52,952 WARNING  Could not load 'stats_llama_32_lora_run_1_epoch_2_20250107_181348.yml', creating it (empty)
2025-01-07 18:13:52,953 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:13:52,958 INFO     Found 778 paragraphs with labels
2025-01-07 18:13:52,958 DEBUG    First few valid IDs: ['AJAMEF_clean', 'CUTBIE_clean', 'MICDEJ_clean', 'CAYQIE_clean', 'KAKBUW_clean']
2025-01-07 18:13:52,958 INFO     Loading evaluation set instead of regular dataset.
2025-01-07 18:13:52,958 DEBUG    Already evaluated: 0 items
2025-01-07 18:13:52,958 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs
2025-01-07 18:13:52,958 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs'
2025-01-07 18:13:52,963 INFO     Dataset loaded with 77 items
2025-01-07 18:13:52,963 INFO     Found 77 paragraphs that have labels in dataset
2025-01-07 18:13:52,964 DEBUG    First few overlapping IDs: ['IVARUT_clean', 'SODQOT_clean', 'CAYQIE_clean', 'MAZSUD_clean', 'AVETEC_clean']
2025-01-07 18:13:52,964 INFO     Processing model: LLaMa 3.2 1B Instruct
2025-01-07 18:13:52,965 INFO       0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
2025-01-07 18:13:52,965 INFO     Loading Model [LLaMa 3.2 1B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama32_1b/instruction_lora/run_1_1736269137/epoch_2
2025-01-07 18:13:53,558 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-07 18:13:59,504 INFO       1%|▏         | 1/77 [00:06<08:16,  6.54s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:00,734 INFO       3%|▎         | 2/77 [00:07<04:16,  3.42s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:01,957 INFO       4%|▍         | 3/77 [00:08<02:58,  2.41s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:03,156 INFO       5%|▌         | 4/77 [00:10<02:21,  1.93s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:04,376 INFO       6%|▋         | 5/77 [00:11<02:00,  1.68s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:05,594 INFO       8%|▊         | 6/77 [00:12<01:47,  1.52s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:06,817 INFO       9%|▉         | 7/77 [00:13<01:39,  1.42s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:08,062 INFO      10%|█         | 8/77 [00:15<01:34,  1.37s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:09,273 INFO      12%|█▏        | 9/77 [00:16<01:29,  1.32s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:10,488 INFO      13%|█▎        | 10/77 [00:17<01:26,  1.29s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:11,682 INFO      14%|█▍        | 11/77 [00:18<01:23,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:12,882 INFO      16%|█▌        | 12/77 [00:19<01:20,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:14,198 INFO      17%|█▋        | 13/77 [00:21<01:20,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:15,417 INFO      18%|█▊        | 14/77 [00:22<01:18,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:16,638 INFO      19%|█▉        | 15/77 [00:23<01:16,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:17,871 INFO      21%|██        | 16/77 [00:24<01:15,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:19,100 INFO      22%|██▏       | 17/77 [00:26<01:14,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:20,315 INFO      23%|██▎       | 18/77 [00:27<01:12,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:21,580 INFO      25%|██▍       | 19/77 [00:28<01:11,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:22,893 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_2_20250107_181348.yml`
2025-01-07 18:14:22,904 INFO      26%|██▌       | 20/77 [00:29<01:12,  1.27s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:24,151 INFO      27%|██▋       | 21/77 [00:31<01:10,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:25,349 INFO      29%|██▊       | 22/77 [00:32<01:08,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:26,583 INFO      30%|██▉       | 23/77 [00:33<01:06,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:27,803 INFO      31%|███       | 24/77 [00:34<01:05,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:29,034 INFO      32%|███▏      | 25/77 [00:36<01:04,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:30,274 INFO      34%|███▍      | 26/77 [00:37<01:02,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:31,501 INFO      35%|███▌      | 27/77 [00:38<01:01,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:32,758 INFO      36%|███▋      | 28/77 [00:39<01:00,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:34,001 INFO      38%|███▊      | 29/77 [00:41<00:59,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:35,232 INFO      39%|███▉      | 30/77 [00:42<00:58,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:36,469 INFO      40%|████      | 31/77 [00:43<00:56,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:37,731 INFO      42%|████▏     | 32/77 [00:44<00:56,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:38,943 INFO      43%|████▎     | 33/77 [00:45<00:54,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:40,192 INFO      44%|████▍     | 34/77 [00:47<00:53,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:41,402 INFO      45%|████▌     | 35/77 [00:48<00:51,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:42,642 INFO      47%|████▋     | 36/77 [00:49<00:50,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:43,858 INFO      48%|████▊     | 37/77 [00:50<00:49,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:45,060 INFO      49%|████▉     | 38/77 [00:52<00:47,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:46,302 INFO      51%|█████     | 39/77 [00:53<00:46,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:47,528 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_2_20250107_181348.yml`
2025-01-07 18:14:47,560 INFO      52%|█████▏    | 40/77 [00:54<00:45,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:48,771 INFO      53%|█████▎    | 41/77 [00:55<00:44,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:50,031 INFO      55%|█████▍    | 42/77 [00:57<00:43,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:51,272 INFO      56%|█████▌    | 43/77 [00:58<00:42,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:52,525 INFO      57%|█████▋    | 44/77 [00:59<00:41,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:53,762 INFO      58%|█████▊    | 45/77 [01:00<00:39,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:54,972 INFO      60%|█████▉    | 46/77 [01:02<00:38,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:56,218 INFO      61%|██████    | 47/77 [01:03<00:37,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:57,479 INFO      62%|██████▏   | 48/77 [01:04<00:36,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:58,669 INFO      64%|██████▎   | 49/77 [01:05<00:34,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:14:59,866 INFO      65%|██████▍   | 50/77 [01:06<00:32,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:01,097 INFO      66%|██████▌   | 51/77 [01:08<00:31,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:02,322 INFO      68%|██████▊   | 52/77 [01:09<00:30,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:03,528 INFO      69%|██████▉   | 53/77 [01:10<00:29,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:04,745 INFO      70%|███████   | 54/77 [01:11<00:28,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:05,980 INFO      71%|███████▏  | 55/77 [01:13<00:26,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:07,192 INFO      73%|███████▎  | 56/77 [01:14<00:25,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:08,390 INFO      74%|███████▍  | 57/77 [01:15<00:24,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:09,636 INFO      75%|███████▌  | 58/77 [01:16<00:23,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:10,839 INFO      77%|███████▋  | 59/77 [01:17<00:21,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:12,098 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_2_20250107_181348.yml`
2025-01-07 18:15:12,135 INFO      78%|███████▊  | 60/77 [01:19<00:21,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:13,368 INFO      79%|███████▉  | 61/77 [01:20<00:19,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:14,590 INFO      81%|████████  | 62/77 [01:21<00:18,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:15,804 INFO      82%|████████▏ | 63/77 [01:22<00:17,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:17,046 INFO      83%|████████▎ | 64/77 [01:24<00:16,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:18,289 INFO      84%|████████▍ | 65/77 [01:25<00:14,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:19,507 INFO      86%|████████▌ | 66/77 [01:26<00:13,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:20,835 INFO      87%|████████▋ | 67/77 [01:27<00:12,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:22,070 INFO      88%|████████▊ | 68/77 [01:29<00:11,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:23,283 INFO      90%|████████▉ | 69/77 [01:30<00:09,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:24,519 INFO      91%|█████████ | 70/77 [01:31<00:08,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:25,727 INFO      92%|█████████▏| 71/77 [01:32<00:07,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:26,975 INFO      94%|█████████▎| 72/77 [01:34<00:06,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:28,178 INFO      95%|█████████▍| 73/77 [01:35<00:04,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:29,382 INFO      96%|█████████▌| 74/77 [01:36<00:03,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:30,586 INFO      97%|█████████▋| 75/77 [01:37<00:02,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:31,819 INFO      99%|█████████▊| 76/77 [01:38<00:01,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:33,044 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_2_20250107_181348.yml`
2025-01-07 18:15:33,086 INFO     Processing model: LLaMa 3.2 3B Instruct
2025-01-07 18:15:33,086 INFO     Skipping model [LLaMa 3.2 3B Instruct]
  0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
model kwargs: {'torch_dtype': torch.float16, 'load_in_8bit': False, 'trust_remote_code': True, 'device_map': 'auto', 'do_sample': False}
  1%|▏         | 1/77 [00:06<08:16,  6.54s/it, LLaMa 3.2 1B Instruct]
  3%|▎         | 2/77 [00:07<04:16,  3.42s/it, LLaMa 3.2 1B Instruct]
  4%|▍         | 3/77 [00:08<02:58,  2.41s/it, LLaMa 3.2 1B Instruct]
  5%|▌         | 4/77 [00:10<02:21,  1.93s/it, LLaMa 3.2 1B Instruct]
  6%|▋         | 5/77 [00:11<02:00,  1.68s/it, LLaMa 3.2 1B Instruct]
  8%|▊         | 6/77 [00:12<01:47,  1.52s/it, LLaMa 3.2 1B Instruct]
  9%|▉         | 7/77 [00:13<01:39,  1.42s/it, LLaMa 3.2 1B Instruct]
 10%|█         | 8/77 [00:15<01:34,  1.37s/it, LLaMa 3.2 1B Instruct]
 12%|█▏        | 9/77 [00:16<01:29,  1.32s/it, LLaMa 3.2 1B Instruct]
 13%|█▎        | 10/77 [00:17<01:26,  1.29s/it, LLaMa 3.2 1B Instruct]
 14%|█▍        | 11/77 [00:18<01:23,  1.26s/it, LLaMa 3.2 1B Instruct]
 16%|█▌        | 12/77 [00:19<01:20,  1.24s/it, LLaMa 3.2 1B Instruct]
 17%|█▋        | 13/77 [00:21<01:20,  1.26s/it, LLaMa 3.2 1B Instruct]
 18%|█▊        | 14/77 [00:22<01:18,  1.25s/it, LLaMa 3.2 1B Instruct]
 19%|█▉        | 15/77 [00:23<01:16,  1.24s/it, LLaMa 3.2 1B Instruct]
 21%|██        | 16/77 [00:24<01:15,  1.24s/it, LLaMa 3.2 1B Instruct]
 22%|██▏       | 17/77 [00:26<01:14,  1.24s/it, LLaMa 3.2 1B Instruct]
 23%|██▎       | 18/77 [00:27<01:12,  1.23s/it, LLaMa 3.2 1B Instruct]
 25%|██▍       | 19/77 [00:28<01:11,  1.24s/it, LLaMa 3.2 1B Instruct]
 26%|██▌       | 20/77 [00:29<01:12,  1.27s/it, LLaMa 3.2 1B Instruct]
 27%|██▋       | 21/77 [00:31<01:10,  1.26s/it, LLaMa 3.2 1B Instruct]
 29%|██▊       | 22/77 [00:32<01:08,  1.24s/it, LLaMa 3.2 1B Instruct]
 30%|██▉       | 23/77 [00:33<01:06,  1.24s/it, LLaMa 3.2 1B Instruct]
 31%|███       | 24/77 [00:34<01:05,  1.23s/it, LLaMa 3.2 1B Instruct]
 32%|███▏      | 25/77 [00:36<01:04,  1.23s/it, LLaMa 3.2 1B Instruct]
 34%|███▍      | 26/77 [00:37<01:02,  1.23s/it, LLaMa 3.2 1B Instruct]
 35%|███▌      | 27/77 [00:38<01:01,  1.23s/it, LLaMa 3.2 1B Instruct]
 36%|███▋      | 28/77 [00:39<01:00,  1.24s/it, LLaMa 3.2 1B Instruct]
 38%|███▊      | 29/77 [00:41<00:59,  1.24s/it, LLaMa 3.2 1B Instruct]
 39%|███▉      | 30/77 [00:42<00:58,  1.24s/it, LLaMa 3.2 1B Instruct]
 40%|████      | 31/77 [00:43<00:56,  1.24s/it, LLaMa 3.2 1B Instruct]
 42%|████▏     | 32/77 [00:44<00:56,  1.24s/it, LLaMa 3.2 1B Instruct]
 43%|████▎     | 33/77 [00:45<00:54,  1.23s/it, LLaMa 3.2 1B Instruct]
 44%|████▍     | 34/77 [00:47<00:53,  1.24s/it, LLaMa 3.2 1B Instruct]
 45%|████▌     | 35/77 [00:48<00:51,  1.23s/it, LLaMa 3.2 1B Instruct]
 47%|████▋     | 36/77 [00:49<00:50,  1.23s/it, LLaMa 3.2 1B Instruct]
 48%|████▊     | 37/77 [00:50<00:49,  1.23s/it, LLaMa 3.2 1B Instruct]
 49%|████▉     | 38/77 [00:52<00:47,  1.22s/it, LLaMa 3.2 1B Instruct]
 51%|█████     | 39/77 [00:53<00:46,  1.23s/it, LLaMa 3.2 1B Instruct]
 52%|█████▏    | 40/77 [00:54<00:45,  1.24s/it, LLaMa 3.2 1B Instruct]
 53%|█████▎    | 41/77 [00:55<00:44,  1.23s/it, LLaMa 3.2 1B Instruct]
 55%|█████▍    | 42/77 [00:57<00:43,  1.24s/it, LLaMa 3.2 1B Instruct]
 56%|█████▌    | 43/77 [00:58<00:42,  1.24s/it, LLaMa 3.2 1B Instruct]
 57%|█████▋    | 44/77 [00:59<00:41,  1.24s/it, LLaMa 3.2 1B Instruct]
 58%|█████▊    | 45/77 [01:00<00:39,  1.24s/it, LLaMa 3.2 1B Instruct]
 60%|█████▉    | 46/77 [01:02<00:38,  1.23s/it, LLaMa 3.2 1B Instruct]
 61%|██████    | 47/77 [01:03<00:37,  1.24s/it, LLaMa 3.2 1B Instruct]
 62%|██████▏   | 48/77 [01:04<00:36,  1.24s/it, LLaMa 3.2 1B Instruct]
 64%|██████▎   | 49/77 [01:05<00:34,  1.23s/it, LLaMa 3.2 1B Instruct]
 65%|██████▍   | 50/77 [01:06<00:32,  1.22s/it, LLaMa 3.2 1B Instruct]
 66%|██████▌   | 51/77 [01:08<00:31,  1.22s/it, LLaMa 3.2 1B Instruct]
 68%|██████▊   | 52/77 [01:09<00:30,  1.22s/it, LLaMa 3.2 1B Instruct]
 69%|██████▉   | 53/77 [01:10<00:29,  1.22s/it, LLaMa 3.2 1B Instruct]
 70%|███████   | 54/77 [01:11<00:28,  1.22s/it, LLaMa 3.2 1B Instruct]
 71%|███████▏  | 55/77 [01:13<00:26,  1.22s/it, LLaMa 3.2 1B Instruct]
 73%|███████▎  | 56/77 [01:14<00:25,  1.22s/it, LLaMa 3.2 1B Instruct]
 74%|███████▍  | 57/77 [01:15<00:24,  1.21s/it, LLaMa 3.2 1B Instruct]
 75%|███████▌  | 58/77 [01:16<00:23,  1.22s/it, LLaMa 3.2 1B Instruct]
 77%|███████▋  | 59/77 [01:17<00:21,  1.22s/it, LLaMa 3.2 1B Instruct]
 78%|███████▊  | 60/77 [01:19<00:21,  1.24s/it, LLaMa 3.2 1B Instruct]
 79%|███████▉  | 61/77 [01:20<00:19,  1.24s/it, LLaMa 3.2 1B Instruct]
 81%|████████  | 62/77 [01:21<00:18,  1.23s/it, LLaMa 3.2 1B Instruct]
 82%|████████▏ | 63/77 [01:22<00:17,  1.23s/it, LLaMa 3.2 1B Instruct]
 83%|████████▎ | 64/77 [01:24<00:16,  1.23s/it, LLaMa 3.2 1B Instruct]
 84%|████████▍ | 65/77 [01:25<00:14,  1.24s/it, LLaMa 3.2 1B Instruct]
 86%|████████▌ | 66/77 [01:26<00:13,  1.23s/it, LLaMa 3.2 1B Instruct]
 87%|████████▋ | 67/77 [01:27<00:12,  1.26s/it, LLaMa 3.2 1B Instruct]
 88%|████████▊ | 68/77 [01:29<00:11,  1.25s/it, LLaMa 3.2 1B Instruct]
 90%|████████▉ | 69/77 [01:30<00:09,  1.24s/it, LLaMa 3.2 1B Instruct]
 91%|█████████ | 70/77 [01:31<00:08,  1.24s/it, LLaMa 3.2 1B Instruct]
 92%|█████████▏| 71/77 [01:32<00:07,  1.23s/it, LLaMa 3.2 1B Instruct]
 94%|█████████▎| 72/77 [01:34<00:06,  1.24s/it, LLaMa 3.2 1B Instruct]
 95%|█████████▍| 73/77 [01:35<00:04,  1.23s/it, LLaMa 3.2 1B Instruct]
 96%|█████████▌| 74/77 [01:36<00:03,  1.22s/it, LLaMa 3.2 1B Instruct]
 97%|█████████▋| 75/77 [01:37<00:02,  1.21s/it, LLaMa 3.2 1B Instruct]
 99%|█████████▊| 76/77 [01:38<00:01,  1.22s/it, LLaMa 3.2 1B Instruct]
