2025-01-07 18:10:10,207 INFO     Loading settings and stats
2025-01-07 18:10:10,207 INFO     Using prompt: Read the following Metal-Organic Framework (MOF) synthesis description and extract this information: temperature (highest reaction temp, use 25°C if not specified), time (longest duration at highest temp), one main solvent (no mixtures or ratios), one chemical additive ('None' if no additive present). Important: Do not use JSON or curly braces in your output, they are already provided for you and you do not need to generate them. Only output the extracted information and terminate strings with ", temperature: 0.0
2025-01-07 18:10:10,210 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:10:10,210 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_0_20250107_181004.yml'
2025-01-07 18:10:10,210 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_0_20250107_181004.yaml'
2025-01-07 18:10:10,210 WARNING  Could not load 'stats_llama_32_lora_run_1_epoch_0_20250107_181004.yml', creating it (empty)
2025-01-07 18:10:10,211 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:10:10,216 INFO     Found 778 paragraphs with labels
2025-01-07 18:10:10,216 DEBUG    First few valid IDs: ['VEZWEE_clean', 'COWXOC_clean', 'FIGQUJ_clean', 'SOBZEQ_clean', 'AZAVOO_clean']
2025-01-07 18:10:10,216 INFO     Loading evaluation set instead of regular dataset.
2025-01-07 18:10:10,216 DEBUG    Already evaluated: 0 items
2025-01-07 18:10:10,216 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs
2025-01-07 18:10:10,216 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs'
2025-01-07 18:10:10,221 INFO     Dataset loaded with 77 items
2025-01-07 18:10:10,221 INFO     Found 77 paragraphs that have labels in dataset
2025-01-07 18:10:10,221 DEBUG    First few overlapping IDs: ['RAHPAT_clean', 'XASPEP_clean', 'HOZFIN_clean', 'NAFGOR_clean', 'ELOZIQ_clean']
2025-01-07 18:10:10,221 INFO     Processing model: LLaMa 3.2 1B Instruct
2025-01-07 18:10:10,223 INFO       0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:10,223 INFO     Loading Model [LLaMa 3.2 1B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama32_1b/instruction_lora/run_1_1736269137/epoch_0
2025-01-07 18:10:10,853 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-01-07 18:10:16,807 INFO       1%|▏         | 1/77 [00:06<08:20,  6.58s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:18,043 INFO       3%|▎         | 2/77 [00:07<04:17,  3.44s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:19,300 INFO       4%|▍         | 3/77 [00:09<03:00,  2.44s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:20,500 INFO       5%|▌         | 4/77 [00:10<02:22,  1.95s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:21,706 INFO       6%|▋         | 5/77 [00:11<02:01,  1.68s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:22,902 INFO       8%|▊         | 6/77 [00:12<01:47,  1.52s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:24,131 INFO       9%|▉         | 7/77 [00:13<01:39,  1.42s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:25,338 INFO      10%|█         | 8/77 [00:15<01:33,  1.35s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:26,543 INFO      12%|█▏        | 9/77 [00:16<01:28,  1.31s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:27,726 INFO      13%|█▎        | 10/77 [00:17<01:25,  1.27s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:28,914 INFO      14%|█▍        | 11/77 [00:18<01:22,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:30,108 INFO      16%|█▌        | 12/77 [00:19<01:19,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:31,334 INFO      17%|█▋        | 13/77 [00:21<01:18,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:32,577 INFO      18%|█▊        | 14/77 [00:22<01:17,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:33,778 INFO      19%|█▉        | 15/77 [00:23<01:15,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:35,018 INFO      21%|██        | 16/77 [00:24<01:14,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:36,233 INFO      22%|██▏       | 17/77 [00:26<01:13,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:37,439 INFO      23%|██▎       | 18/77 [00:27<01:11,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:38,666 INFO      25%|██▍       | 19/77 [00:28<01:10,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:39,939 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_0_20250107_181004.yml`
2025-01-07 18:10:39,959 INFO      26%|██▌       | 20/77 [00:29<01:10,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:41,177 INFO      27%|██▋       | 21/77 [00:30<01:09,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:42,406 INFO      29%|██▊       | 22/77 [00:32<01:07,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:43,650 INFO      30%|██▉       | 23/77 [00:33<01:06,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:44,859 INFO      31%|███       | 24/77 [00:34<01:05,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:46,066 INFO      32%|███▏      | 25/77 [00:35<01:03,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:47,311 INFO      34%|███▍      | 26/77 [00:37<01:02,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:48,534 INFO      35%|███▌      | 27/77 [00:38<01:01,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:49,781 INFO      36%|███▋      | 28/77 [00:39<01:00,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:50,982 INFO      38%|███▊      | 29/77 [00:40<00:58,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:52,200 INFO      39%|███▉      | 30/77 [00:41<00:57,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:53,411 INFO      40%|████      | 31/77 [00:43<00:56,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:54,632 INFO      42%|████▏     | 32/77 [00:44<00:54,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:55,856 INFO      43%|████▎     | 33/77 [00:45<00:53,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:57,095 INFO      44%|████▍     | 34/77 [00:46<00:52,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:58,313 INFO      45%|████▌     | 35/77 [00:48<00:51,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:10:59,573 INFO      47%|████▋     | 36/77 [00:49<00:50,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:00,775 INFO      48%|████▊     | 37/77 [00:50<00:48,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:01,966 INFO      49%|████▉     | 38/77 [00:51<00:47,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:03,224 INFO      51%|█████     | 39/77 [00:53<00:46,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:04,448 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_0_20250107_181004.yml`
2025-01-07 18:11:04,470 INFO      52%|█████▏    | 40/77 [00:54<00:45,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:05,694 INFO      53%|█████▎    | 41/77 [00:55<00:44,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:06,950 INFO      55%|█████▍    | 42/77 [00:56<00:43,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:08,150 INFO      56%|█████▌    | 43/77 [00:57<00:41,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:09,359 INFO      57%|█████▋    | 44/77 [00:59<00:40,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:10,568 INFO      58%|█████▊    | 45/77 [01:00<00:38,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:11,776 INFO      60%|█████▉    | 46/77 [01:01<00:37,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:12,973 INFO      61%|██████    | 47/77 [01:02<00:36,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:14,229 INFO      62%|██████▏   | 48/77 [01:04<00:35,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:15,421 INFO      64%|██████▎   | 49/77 [01:05<00:33,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:16,628 INFO      65%|██████▍   | 50/77 [01:06<00:32,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:17,833 INFO      66%|██████▌   | 51/77 [01:07<00:31,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:19,097 INFO      68%|██████▊   | 52/77 [01:08<00:30,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:20,301 INFO      69%|██████▉   | 53/77 [01:10<00:29,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:21,494 INFO      70%|███████   | 54/77 [01:11<00:27,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:22,723 INFO      71%|███████▏  | 55/77 [01:12<00:26,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:23,963 INFO      73%|███████▎  | 56/77 [01:13<00:25,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:25,143 INFO      74%|███████▍  | 57/77 [01:14<00:24,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:26,391 INFO      75%|███████▌  | 58/77 [01:16<00:23,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:27,598 INFO      77%|███████▋  | 59/77 [01:17<00:21,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:28,817 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_0_20250107_181004.yml`
2025-01-07 18:11:28,851 INFO      78%|███████▊  | 60/77 [01:18<00:20,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:30,090 INFO      79%|███████▉  | 61/77 [01:19<00:19,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:31,352 INFO      81%|████████  | 62/77 [01:21<00:18,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:32,580 INFO      82%|████████▏ | 63/77 [01:22<00:17,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:33,807 INFO      83%|████████▎ | 64/77 [01:23<00:16,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:35,057 INFO      84%|████████▍ | 65/77 [01:24<00:14,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:36,297 INFO      86%|████████▌ | 66/77 [01:26<00:13,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:37,534 INFO      87%|████████▋ | 67/77 [01:27<00:12,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:38,760 INFO      88%|████████▊ | 68/77 [01:28<00:11,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:39,961 INFO      90%|████████▉ | 69/77 [01:29<00:09,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:41,180 INFO      91%|█████████ | 70/77 [01:30<00:08,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:42,389 INFO      92%|█████████▏| 71/77 [01:32<00:07,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:43,636 INFO      94%|█████████▎| 72/77 [01:33<00:06,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:44,840 INFO      95%|█████████▍| 73/77 [01:34<00:04,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:46,069 INFO      96%|█████████▌| 74/77 [01:35<00:03,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:47,271 INFO      97%|█████████▋| 75/77 [01:37<00:02,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:48,503 INFO      99%|█████████▊| 76/77 [01:38<00:01,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:11:49,756 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_0_20250107_181004.yml`
2025-01-07 18:11:49,798 INFO     Processing model: LLaMa 3.2 3B Instruct
2025-01-07 18:11:49,798 INFO     Skipping model [LLaMa 3.2 3B Instruct]
