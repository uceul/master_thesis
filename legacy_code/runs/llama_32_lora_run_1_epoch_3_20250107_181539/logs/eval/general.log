2025-01-07 18:15:43,469 INFO     Loading settings and stats
2025-01-07 18:15:43,469 INFO     Using prompt: Read the following Metal-Organic Framework (MOF) synthesis description and extract this information: temperature (highest reaction temp, use 25°C if not specified), time (longest duration at highest temp), one main solvent (no mixtures or ratios), one chemical additive ('None' if no additive present). Important: Do not use JSON or curly braces in your output, they are already provided for you and you do not need to generate them. Only output the extracted information and terminate strings with ", temperature: 0.0
2025-01-07 18:15:43,471 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:15:43,472 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_3_20250107_181539.yml'
2025-01-07 18:15:43,472 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_3_20250107_181539.yaml'
2025-01-07 18:15:43,472 WARNING  Could not load 'stats_llama_32_lora_run_1_epoch_3_20250107_181539.yml', creating it (empty)
2025-01-07 18:15:43,472 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:15:43,477 INFO     Found 778 paragraphs with labels
2025-01-07 18:15:43,478 DEBUG    First few valid IDs: ['KITCAT_clean', 'LIVLUZ_clean', 'XOTSIK_clean', 'PODXEN_clean', 'IYAJOJ_clean']
2025-01-07 18:15:43,478 INFO     Loading evaluation set instead of regular dataset.
2025-01-07 18:15:43,478 DEBUG    Already evaluated: 0 items
2025-01-07 18:15:43,478 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs
2025-01-07 18:15:43,478 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs'
2025-01-07 18:15:43,483 INFO     Dataset loaded with 77 items
2025-01-07 18:15:43,483 INFO     Found 77 paragraphs that have labels in dataset
2025-01-07 18:15:43,483 DEBUG    First few overlapping IDs: ['GEVYEN_clean', 'XOTSIK_clean', 'NEKMUN_clean', 'ELOZIQ_clean', 'TEVHEJ_clean']
2025-01-07 18:15:43,483 INFO     Processing model: LLaMa 3.2 1B Instruct
2025-01-07 18:15:43,484 INFO       0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:43,484 INFO     Loading Model [LLaMa 3.2 1B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama32_1b/instruction_lora/run_1_1736269137/epoch_3
2025-01-07 18:15:44,073 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-01-07 18:15:50,129 INFO       1%|▏         | 1/77 [00:06<08:24,  6.64s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:51,333 INFO       3%|▎         | 2/77 [00:07<04:18,  3.44s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:52,596 INFO       4%|▍         | 3/77 [00:09<03:01,  2.45s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:53,795 INFO       5%|▌         | 4/77 [00:10<02:22,  1.95s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:54,990 INFO       6%|▋         | 5/77 [00:11<02:01,  1.68s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:56,211 INFO       8%|▊         | 6/77 [00:12<01:48,  1.52s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:57,446 INFO       9%|▉         | 7/77 [00:13<01:40,  1.43s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:58,676 INFO      10%|█         | 8/77 [00:15<01:34,  1.37s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:15:59,892 INFO      12%|█▏        | 9/77 [00:16<01:29,  1.32s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:01,078 INFO      13%|█▎        | 10/77 [00:17<01:25,  1.28s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:02,289 INFO      14%|█▍        | 11/77 [00:18<01:23,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:03,477 INFO      16%|█▌        | 12/77 [00:19<01:20,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:04,753 INFO      17%|█▋        | 13/77 [00:21<01:19,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:05,986 INFO      18%|█▊        | 14/77 [00:22<01:18,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:07,205 INFO      19%|█▉        | 15/77 [00:23<01:16,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:08,431 INFO      21%|██        | 16/77 [00:24<01:15,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:09,664 INFO      22%|██▏       | 17/77 [00:26<01:13,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:10,880 INFO      23%|██▎       | 18/77 [00:27<01:12,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:12,092 INFO      25%|██▍       | 19/77 [00:28<01:10,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:13,406 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_3_20250107_181539.yml`
2025-01-07 18:16:13,424 INFO      26%|██▌       | 20/77 [00:29<01:11,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:14,679 INFO      27%|██▋       | 21/77 [00:31<01:10,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:15,889 INFO      29%|██▊       | 22/77 [00:32<01:08,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:17,113 INFO      30%|██▉       | 23/77 [00:33<01:06,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:18,352 INFO      31%|███       | 24/77 [00:34<01:05,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:19,573 INFO      32%|███▏      | 25/77 [00:36<01:04,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:20,800 INFO      34%|███▍      | 26/77 [00:37<01:02,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:22,025 INFO      35%|███▌      | 27/77 [00:38<01:01,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:23,280 INFO      36%|███▋      | 28/77 [00:39<01:00,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:24,483 INFO      38%|███▊      | 29/77 [00:40<00:58,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:25,723 INFO      39%|███▉      | 30/77 [00:42<00:57,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:26,979 INFO      40%|████      | 31/77 [00:43<00:56,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:28,219 INFO      42%|████▏     | 32/77 [00:44<00:55,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:29,443 INFO      43%|████▎     | 33/77 [00:45<00:54,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:30,700 INFO      44%|████▍     | 34/77 [00:47<00:53,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:31,906 INFO      45%|████▌     | 35/77 [00:48<00:51,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:33,143 INFO      47%|████▋     | 36/77 [00:49<00:50,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:34,383 INFO      48%|████▊     | 37/77 [00:50<00:49,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:35,639 INFO      49%|████▉     | 38/77 [00:52<00:48,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:36,879 INFO      51%|█████     | 39/77 [00:53<00:47,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:38,106 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_3_20250107_181539.yml`
2025-01-07 18:16:38,128 INFO      52%|█████▏    | 40/77 [00:54<00:46,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:39,357 INFO      53%|█████▎    | 41/77 [00:55<00:44,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:40,621 INFO      55%|█████▍    | 42/77 [00:57<00:43,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:41,860 INFO      56%|█████▌    | 43/77 [00:58<00:42,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:43,123 INFO      57%|█████▋    | 44/77 [00:59<00:41,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:44,354 INFO      58%|█████▊    | 45/77 [01:00<00:39,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:45,571 INFO      60%|█████▉    | 46/77 [01:02<00:38,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:46,802 INFO      61%|██████    | 47/77 [01:03<00:37,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:48,021 INFO      62%|██████▏   | 48/77 [01:04<00:35,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:49,281 INFO      64%|██████▎   | 49/77 [01:05<00:34,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:50,556 INFO      65%|██████▍   | 50/77 [01:07<00:33,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:51,788 INFO      66%|██████▌   | 51/77 [01:08<00:32,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:53,021 INFO      68%|██████▊   | 52/77 [01:09<00:31,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:54,257 INFO      69%|██████▉   | 53/77 [01:10<00:29,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:55,512 INFO      70%|███████   | 54/77 [01:12<00:28,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:56,746 INFO      71%|███████▏  | 55/77 [01:13<00:27,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:57,987 INFO      73%|███████▎  | 56/77 [01:14<00:26,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:16:59,202 INFO      74%|███████▍  | 57/77 [01:15<00:24,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:00,455 INFO      75%|███████▌  | 58/77 [01:16<00:23,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:01,662 INFO      77%|███████▋  | 59/77 [01:18<00:22,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:02,914 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_3_20250107_181539.yml`
2025-01-07 18:17:02,961 INFO      78%|███████▊  | 60/77 [01:19<00:21,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:04,211 INFO      79%|███████▉  | 61/77 [01:20<00:20,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:05,455 INFO      81%|████████  | 62/77 [01:21<00:18,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:06,714 INFO      82%|████████▏ | 63/77 [01:23<00:17,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:07,948 INFO      83%|████████▎ | 64/77 [01:24<00:16,  1.25s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:09,185 INFO      84%|████████▍ | 65/77 [01:25<00:14,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:10,402 INFO      86%|████████▌ | 66/77 [01:26<00:13,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:11,611 INFO      87%|████████▋ | 67/77 [01:28<00:12,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:12,844 INFO      88%|████████▊ | 68/77 [01:29<00:11,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:14,035 INFO      90%|████████▉ | 69/77 [01:30<00:09,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:15,261 INFO      91%|█████████ | 70/77 [01:31<00:08,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:16,468 INFO      92%|█████████▏| 71/77 [01:32<00:07,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:17,709 INFO      94%|█████████▎| 72/77 [01:34<00:06,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:18,888 INFO      95%|█████████▍| 73/77 [01:35<00:04,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:20,100 INFO      96%|█████████▌| 74/77 [01:36<00:03,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:21,298 INFO      97%|█████████▋| 75/77 [01:37<00:02,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:22,518 INFO      99%|█████████▊| 76/77 [01:39<00:01,  1.21s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:23,737 INFO     Saving progress to `stats_llama_32_lora_run_1_epoch_3_20250107_181539.yml`
2025-01-07 18:17:23,779 INFO     Processing model: LLaMa 3.2 3B Instruct
2025-01-07 18:17:23,779 INFO     Skipping model [LLaMa 3.2 3B Instruct]
