2025-01-07 18:17:33,818 INFO     Loading settings and stats
2025-01-07 18:17:33,819 INFO     Using prompt: Read the following Metal-Organic Framework (MOF) synthesis description and extract this information: temperature (highest reaction temp, use 25°C if not specified), time (longest duration at highest temp), one main solvent (no mixtures or ratios), one chemical additive ('None' if no additive present). Important: Do not use JSON or curly braces in your output, they are already provided for you and you do not need to generate them. Only output the extracted information and terminate strings with ", temperature: 0.0
2025-01-07 18:17:33,821 DEBUG    Settings loaded. CSV path: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:17:33,821 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_4_20250107_181729.yml'
2025-01-07 18:17:33,821 WARNING  [Errno 2] No such file or directory: 'stats_llama_32_lora_run_1_epoch_4_20250107_181729.yaml'
2025-01-07 18:17:33,821 WARNING  Could not load 'stats_llama_32_lora_run_1_epoch_4_20250107_181729.yml', creating it (empty)
2025-01-07 18:17:33,825 INFO     Loading labels from CSV file: /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/tobias/SynMOF_M_out.csv
2025-01-07 18:17:33,830 INFO     Found 778 paragraphs with labels
2025-01-07 18:17:33,830 DEBUG    First few valid IDs: ['LAQZOV_clean', 'VEZWEE_clean', 'UYAMIS_clean', 'DEJCIF_clean', 'EJOQUR_clean']
2025-01-07 18:17:33,830 INFO     Loading evaluation set instead of regular dataset.
2025-01-07 18:17:33,830 DEBUG    Already evaluated: 0 items
2025-01-07 18:17:33,830 INFO     Loading Dataset from /gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs
2025-01-07 18:17:33,830 INFO     Loading MOFDataset from '/gfse/data/LSDF/lsdf01/lsdf/kit/iti/zn2950/ws/data/eval_paragraphs'
2025-01-07 18:17:33,835 INFO     Dataset loaded with 77 items
2025-01-07 18:17:33,836 INFO     Found 77 paragraphs that have labels in dataset
2025-01-07 18:17:33,836 DEBUG    First few overlapping IDs: ['KUKQAK_clean', 'CAHQAG_clean', 'NORJEL_clean', 'IVARUT_clean', 'MATPUV_clean']
2025-01-07 18:17:33,836 INFO     Processing model: LLaMa 3.2 1B Instruct
2025-01-07 18:17:33,837 INFO       0%|          | 0/77 [00:00<?, ?it/s, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:33,837 INFO     Loading Model [LLaMa 3.2 1B Instruct] from /home/iti/zn2950/home/haicore_ws/tunes/llama32_1b/instruction_lora/run_1_1736269137/epoch_4
2025-01-07 18:17:34,421 INFO     We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
2025-01-07 18:17:40,458 INFO       1%|▏         | 1/77 [00:06<08:23,  6.62s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:41,687 INFO       3%|▎         | 2/77 [00:07<04:18,  3.45s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:42,957 INFO       4%|▍         | 3/77 [00:09<03:01,  2.45s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:44,153 INFO       5%|▌         | 4/77 [00:10<02:22,  1.96s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:45,339 INFO       6%|▋         | 5/77 [00:11<02:00,  1.68s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:46,585 INFO       8%|▊         | 6/77 [00:12<01:48,  1.53s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:47,827 INFO       9%|▉         | 7/77 [00:13<01:40,  1.44s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:49,020 INFO      10%|█         | 8/77 [00:15<01:33,  1.36s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:50,204 INFO      12%|█▏        | 9/77 [00:16<01:28,  1.30s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:51,428 INFO      13%|█▎        | 10/77 [00:17<01:25,  1.28s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:52,631 INFO      14%|█▍        | 11/77 [00:18<01:22,  1.26s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:53,833 INFO      16%|█▌        | 12/77 [00:19<01:20,  1.24s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:55,053 INFO      17%|█▋        | 13/77 [00:21<01:18,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:56,280 INFO      18%|█▊        | 14/77 [00:22<01:17,  1.23s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:57,477 INFO      19%|█▉        | 15/77 [00:23<01:15,  1.22s/it, LLaMa 3.2 1B Instruct]
2025-01-07 18:17:58,721 INFO      21%|██        | 16/77 [00:24<01:14,  1.23s/it, LLaMa 3.2 1B Instruct]
